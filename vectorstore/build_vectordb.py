#!/usr/bin/env python3
"""Vector DB êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸ (ì¦ë¶„ ì—…ë°ì´íŠ¸ ì§€ì›)"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from config.settings import *
from models.embeddings.factory import get_embedder
from models.vision.factory import get_vision_model
from core.data_collector import NotionDataSourceCollector
from core.chunker import process_page_data
from core.vector_store import (
    init_qdrant,
    store_to_qdrant,
    check_qdrant_data,
    delete_page_from_qdrant
)
from services.incremental_sync import (
    check_existing_data,
    collect_missing_pages,
    update_changed_pages
)
from utils.file_utils import save_json, load_json
from utils.langfuse_utils import get_langfuse_client, trace_operation
from qdrant_client import QdrantClient

def main(force_recreate: bool = False, check_updates: bool = True, limit: int = None):
    """
    Vector DB êµ¬ì¶• ë©”ì¸ í•¨ìˆ˜

    Args:
        force_recreate: Trueë©´ ì „ì²´ ì¬ìƒì„±
        check_updates: Trueë©´ ìˆ˜ì •ëœ í˜ì´ì§€ë„ í™•ì¸
        limit: ì²˜ë¦¬í•  í˜ì´ì§€ ìˆ˜ ì œí•œ (Noneì´ë©´ ì „ì²´)
    """
    print("=" * 60)
    print("ğŸš€ Vector DB êµ¬ì¶• ì‹œì‘")
    if limit:
        print(f"ğŸ“Š ì œí•œ: {limit}ê°œ í˜ì´ì§€ë§Œ ì²˜ë¦¬")
    print("=" * 60)

    # Langfuse ì´ˆê¸°í™”
    get_langfuse_client()

    # ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ Langfuseë¡œ íŠ¸ë ˆì´ì‹±
    with trace_operation(
        name="vectordb_build",
        metadata={
            "force_recreate": force_recreate,
            "check_updates": check_updates,
            "limit": limit,
            "db_name": DB_NAME
        }
    ) as trace:

        data_file = DATA_DIR / "notion_data.json"

        # 1. ëª¨ë¸ ì´ˆê¸°í™”
        print("\nğŸ“¦ ëª¨ë¸ ë¡œë”©...")
        if trace:
            model_span = trace.span(name="model_initialization")

        embedder = get_embedder()
        vision_model = get_vision_model()
        qdrant_client = QdrantClient(path=QDRANT_PATH)

        if trace:
            model_span.end()

        # 2. ë°ì´í„° ìˆ˜ì§‘ (ì¦ë¶„)
        if trace:
            collection_span = trace.span(
                name="data_collection",
                metadata={"mode": "force_recreate" if force_recreate else "incremental"}
            )

        if force_recreate:
            print("\nâ™»ï¸ ì „ì²´ ì¬ìˆ˜ì§‘ ëª¨ë“œ")
            if data_file.exists():
                from datetime import datetime
                backup = data_file.with_suffix(f".backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
                data_file.rename(backup)
                print(f"   ë°±ì—…: {backup}")

            collector = NotionDataSourceCollector(NOTION_TOKEN, DATA_SOURCE_ID)
            all_data = collector.collect_all(limit=limit)
            save_json(all_data, str(data_file))
            pages_to_index = all_data

        else:
            # ê¸°ì¡´ ë°ì´í„° í™•ì¸
            existing_info = check_existing_data(str(data_file))

            if not existing_info["exists"]:
                print("\nğŸ“¥ ì´ˆê¸° ìˆ˜ì§‘ ì‹œì‘...")
                collector = NotionDataSourceCollector(NOTION_TOKEN, DATA_SOURCE_ID)
                all_data = collector.collect_all(limit=limit)
                save_json(all_data, str(data_file))
                pages_to_index = all_data

            else:
                # ìƒˆ í˜ì´ì§€ ìˆ˜ì§‘
                collector = NotionDataSourceCollector(NOTION_TOKEN, DATA_SOURCE_ID)
                new_data = collect_missing_pages(
                    collector,
                    existing_info["page_ids"],
                    str(data_file),
                    limit=limit
                )

                # ìˆ˜ì •ëœ í˜ì´ì§€ ì—…ë°ì´íŠ¸
                updated_page_ids = set()
                if check_updates:
                    all_data = load_json(str(data_file))
                    old_data = all_data.copy()
                    all_data = update_changed_pages(collector, all_data, str(data_file))

                    # ì–´ë–¤ í˜ì´ì§€ê°€ ì—…ë°ì´íŠ¸ëëŠ”ì§€ ì¶”ì 
                    for old, new in zip(old_data, all_data):
                        if old.get("last_edited_time") != new.get("last_edited_time"):
                            updated_page_ids.add(new["page_id"])
                else:
                    all_data = load_json(str(data_file))

                # ì¸ë±ì‹±í•  í˜ì´ì§€ ê²°ì •
                pages_to_index = [
                    p for p in all_data
                    if p["page_id"] in updated_page_ids or
                       p in new_data
                ]

        if trace:
            collection_span.end(metadata={
                "total_pages_to_index": len(pages_to_index)
            })

        if not pages_to_index:
            print("\nâœ… ì¸ë±ì‹±í•  í˜ì´ì§€ ì—†ìŒ (ëª¨ë‘ ìµœì‹  ìƒíƒœ)")
            return

        # limit ì ìš©
        if limit and len(pages_to_index) > limit:
            print(f"\nâš ï¸  {len(pages_to_index)}ê°œ í˜ì´ì§€ ì¤‘ {limit}ê°œë§Œ ì²˜ë¦¬")
            pages_to_index = pages_to_index[:limit]

        print(f"\nğŸ“ {len(pages_to_index)}ê°œ í˜ì´ì§€ ì¸ë±ì‹±...")

        # 3. Qdrant ì´ˆê¸°í™”
        qdrant_info = check_qdrant_data(qdrant_client)

        if force_recreate or not qdrant_info["exists"]:
            # ì „ì²´ ì¬ìƒì„±
            if trace:
                chunking_span = trace.span(name="chunking")

            all_chunks = []
            for page in pages_to_index:
                chunks = process_page_data(page, embedder, vision_model)
                all_chunks.extend(chunks)
                print(f"  {page.get('title', 'Untitled')}: {len(chunks)}ê°œ ì²­í¬")

            if trace:
                chunking_span.end(metadata={"total_chunks": len(all_chunks)})

            if all_chunks:
                # ì„ë² ë”© ìƒì„±
                if trace:
                    embedding_span = trace.span(name="embedding_generation")

                print(f"\nğŸ”¢ ì„ë² ë”© ìƒì„± ì¤‘...")
                texts = [c.combined_text for c in all_chunks]
                embeddings = embedder.embed_texts(texts)

                if trace:
                    embedding_span.end(metadata={
                        "num_embeddings": len(embeddings),
                        "embedding_dimension": len(embeddings[0])
                    })

                # Qdrant ì €ì¥
                if trace:
                    storage_span = trace.span(name="qdrant_storage")

                print(f"\nğŸ’¾ Qdrant ì €ì¥ ì¤‘...")
                init_qdrant(qdrant_client, dimension=len(embeddings[0]), recreate=force_recreate)
                store_to_qdrant(all_chunks, embeddings, qdrant_client)

                if trace:
                    storage_span.end()
        else:
            # ì¦ë¶„ ì—…ë°ì´íŠ¸: ë³€ê²½ëœ í˜ì´ì§€ë§Œ ì¬ì¸ë±ì‹±
            print("\nğŸ”„ ë³€ê²½ëœ í˜ì´ì§€ ì¬ì¸ë±ì‹±...")

            if trace:
                incremental_span = trace.span(name="incremental_update")

            total_chunks = 0
            for page in pages_to_index:
                page_id = page["page_id"]

                # ê¸°ì¡´ ì²­í¬ ì‚­ì œ
                delete_page_from_qdrant(qdrant_client, page_id)

                # ìƒˆ ì²­í¬ ìƒì„±
                chunks = process_page_data(page, embedder, vision_model)

                if chunks:
                    texts = [c.combined_text for c in chunks]
                    embeddings = embedder.embed_texts(texts)
                    store_to_qdrant(chunks, embeddings, qdrant_client)
                    total_chunks += len(chunks)

                    print(f"  âœ… {page.get('title', 'Untitled')}: {len(chunks)}ê°œ ì²­í¬ ì—…ë°ì´íŠ¸")

            if trace:
                incremental_span.end(metadata={
                    "pages_updated": len(pages_to_index),
                    "total_chunks": total_chunks
                })

        print("\n" + "=" * 60)
        print("ğŸ‰ Vector DB êµ¬ì¶• ì™„ë£Œ!")
        print("=" * 60)

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--force", action="store_true", help="ì „ì²´ ì¬ìƒì„±")
    parser.add_argument("--no-updates", action="store_true", help="ìˆ˜ì • ì²´í¬ ì•ˆ í•¨")
    parser.add_argument("--limit", type=int, default=None, help="ì²˜ë¦¬í•  í˜ì´ì§€ ìˆ˜ ì œí•œ")
    args = parser.parse_args()

    main(force_recreate=args.force, check_updates=not args.no_updates, limit=args.limit)
