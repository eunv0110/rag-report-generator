#!/usr/bin/env python3
"""Vector DB êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸ (LangChain í˜¸í™˜, ì¦ë¶„ ì—…ë°ì´íŠ¸ ì§€ì›)"""

import sys
from pathlib import Path
from typing import List

sys.path.insert(0, str(Path(__file__).parent.parent))

from config.settings import *
from models.embeddings.factory import get_embedder
from models.vision.factory import get_vision_model
from core.data_collector import NotionDataSourceCollector
from core.chunker import process_page_data
from services.incremental_sync import (
    check_existing_data,
    collect_missing_pages,
    update_changed_pages
)
from utils.file_utils import save_json, load_json
from utils.langfuse_utils import get_langfuse_client, trace_operation
from utils.embedding_cache import CachedEmbedder

# LangChain imports
from langchain_community.vectorstores import Qdrant
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams


def get_langchain_embeddings(embedder) -> Embeddings:
    """
    ê¸°ì¡´ embedderë¥¼ LangChain Embeddingsë¡œ ë˜í•‘

    Note: OpenAIEmbedderëŠ” ì´ë¯¸ Embeddingsë¥¼ ìƒì†ë°›ìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë°˜í™˜
    """
    # embedderê°€ ì´ë¯¸ Embeddings ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•˜ê³  ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if isinstance(embedder, Embeddings):
        return embedder

    # ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ wrapper ìƒì„±
    class CustomEmbeddings(Embeddings):
        def __init__(self, embedder):
            self.embedder = embedder

        def embed_documents(self, texts: List[str]) -> List[List[float]]:
            """ë¬¸ì„œ ì„ë² ë”©"""
            if hasattr(self.embedder, 'embed_documents'):
                return self.embedder.embed_documents(texts)
            elif hasattr(self.embedder, 'embed_texts'):
                return self.embedder.embed_texts(texts)
            else:
                raise AttributeError("embedderì— embed_documents ë˜ëŠ” embed_texts ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤")

        def embed_query(self, text: str) -> List[float]:
            """ì¿¼ë¦¬ ì„ë² ë”©"""
            if hasattr(self.embedder, 'embed_query'):
                return self.embedder.embed_query(text)
            elif hasattr(self.embedder, 'embed_texts'):
                return self.embedder.embed_texts([text])[0]
            else:
                raise AttributeError("embedderì— embed_query ë˜ëŠ” embed_texts ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤")

    return CustomEmbeddings(embedder)


def chunks_to_documents(chunks) -> List[Document]:
    """ì²­í¬ë¥¼ LangChain Documentë¡œ ë³€í™˜"""
    documents = []
    for chunk in chunks:
        # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
        metadata = {
            "chunk_id": chunk.chunk_id,
            "page_id": chunk.page_id,
            "page_title": chunk.page_title,
            "section_title": chunk.section_title,
            "section_path": chunk.section_path,
            "has_image": chunk.has_image,
            "image_paths": chunk.image_paths,
            "image_descriptions": chunk.image_descriptions,
        }

        # propertiesê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if hasattr(chunk, 'properties') and chunk.properties:
            props = {k: (v if isinstance(v, (str, int, float, bool, list)) else str(v))
                    for k, v in chunk.properties.items()}
            metadata["properties"] = props

        # Document ìƒì„±
        doc = Document(
            page_content=chunk.combined_text,
            metadata=metadata
        )
        documents.append(doc)

    return documents


def check_qdrant_collection(client: QdrantClient) -> dict:
    """Qdrant ì»¬ë ‰ì…˜ì˜ ë°ì´í„° í™•ì¸"""
    try:
        collections = client.get_collections().collections
        exists = any(c.name == QDRANT_COLLECTION for c in collections)

        if not exists:
            print(f"âŒ Qdrant ì»¬ë ‰ì…˜ ì—†ìŒ: {QDRANT_COLLECTION}")
            return {"exists": False, "count": 0, "page_ids": set()}

        info = client.get_collection(QDRANT_COLLECTION)
        count = info.points_count

        # ëª¨ë“  page_id ì¶”ì¶œ
        scroll_result = client.scroll(
            collection_name=QDRANT_COLLECTION,
            limit=10000,
            with_payload=["page_id"]
        )

        page_ids = {point.payload.get("page_id") for point in scroll_result[0] if point.payload.get("page_id")}

        print(f"âœ… Qdrant ë°ì´í„°: {count}ê°œ ì²­í¬, {len(page_ids)}ê°œ í˜ì´ì§€")

        return {
            "exists": True,
            "count": count,
            "page_ids": page_ids
        }
    except Exception as e:
        print(f"âš ï¸ Qdrant í™•ì¸ ì‹¤íŒ¨: {e}")
        return {"exists": False, "count": 0, "page_ids": set()}


def delete_page_from_vectorstore(vectorstore: Qdrant, page_id: str):
    """LangChain Qdrantì—ì„œ íŠ¹ì • í˜ì´ì§€ì˜ ëª¨ë“  ì²­í¬ ì‚­ì œ"""
    from qdrant_client.models import Filter, FieldCondition, MatchValue

    vectorstore.client.delete(
        collection_name=QDRANT_COLLECTION,
        points_selector=Filter(
            must=[
                FieldCondition(
                    key="page_id",
                    match=MatchValue(value=page_id)
                )
            ]
        )
    )
    print(f"  ğŸ—‘ï¸ í˜ì´ì§€ ì‚­ì œ: {page_id}")


def main(force_recreate: bool = False, check_updates: bool = True, limit: int = None, input_file: str = None):
    """
    Vector DB êµ¬ì¶• ë©”ì¸ í•¨ìˆ˜

    Args:
        force_recreate: Trueë©´ ì „ì²´ ì¬ìƒì„±
        check_updates: Trueë©´ ìˆ˜ì •ëœ í˜ì´ì§€ë„ í™•ì¸
        limit: ì²˜ë¦¬í•  í˜ì´ì§€ ìˆ˜ ì œí•œ (Noneì´ë©´ ì „ì²´)
        input_file: ì‚¬ìš©í•  ì…ë ¥ JSON íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ íŒŒì¼ ì‚¬ìš©)
    """
    print("=" * 60)
    print("ğŸš€ Vector DB êµ¬ì¶• ì‹œì‘")
    if limit:
        print(f"ğŸ“Š ì œí•œ: {limit}ê°œ í˜ì´ì§€ë§Œ ì²˜ë¦¬")
    if input_file:
        print(f"ğŸ“ ì…ë ¥ íŒŒì¼: {input_file}")
    print("=" * 60)

    # Langfuse ì´ˆê¸°í™”
    get_langfuse_client()

    # ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ Langfuseë¡œ íŠ¸ë ˆì´ì‹±
    with trace_operation(
        name="vectordb_build",
        metadata={
            "force_recreate": force_recreate,
            "check_updates": check_updates,
            "limit": limit,
            "db_name": DB_NAME
        }
    ) as trace:

        # ì…ë ¥ íŒŒì¼ ê²°ì •
        if input_file:
            data_file = Path(input_file)
            if not data_file.exists():
                raise FileNotFoundError(f"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
            print(f"âœ… ê¸°ì¡´ JSON íŒŒì¼ ì‚¬ìš©: {data_file}")
        else:
            data_file = DATA_DIR / "notion_data.json"

        # 1. ëª¨ë¸ ì´ˆê¸°í™”
        print("\nğŸ“¦ ëª¨ë¸ ë¡œë”©...")
        if trace:
            model_span = trace.span(name="model_initialization")

        base_embedder = get_embedder()
        # ìºì‹œëœ ì„ë² ë”ë¡œ ë˜í•‘
        cached_embedder = CachedEmbedder(
            embedder=base_embedder,
            model_name=DB_NAME
        )
        embedder = cached_embedder.cached_embedder  # LangChain Embeddings ì¸í„°í˜ì´ìŠ¤
        langchain_embeddings = get_langchain_embeddings(embedder)
        vision_model = get_vision_model()

        # Qdrant clientëŠ” í•„ìš”í•  ë•Œë§Œ ìƒì„± (ì¤‘ë³µ ìƒì„± ë°©ì§€)
        qdrant_client = None

        if trace:
            model_span.end()

        # 2. ë°ì´í„° ìˆ˜ì§‘ (ì¦ë¶„)
        if trace:
            collection_span = trace.span(
                name="data_collection",
                metadata={"mode": "force_recreate" if force_recreate else "incremental"}
            )

        # input_fileì´ ì§€ì •ëœ ê²½ìš° í•´ë‹¹ íŒŒì¼ì—ì„œ ì§ì ‘ ë¡œë“œ
        if input_file:
            print(f"\nğŸ“‚ ê¸°ì¡´ JSON íŒŒì¼ ë¡œë”©: {data_file}")
            all_data = load_json(str(data_file))
            pages_to_index = all_data
            print(f"âœ… {len(all_data)}ê°œ í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")

        elif force_recreate:
            print("\nâ™»ï¸ ì „ì²´ ì¬ìˆ˜ì§‘ ëª¨ë“œ")
            if data_file.exists():
                from datetime import datetime
                backup = data_file.with_suffix(f".backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
                data_file.rename(backup)
                print(f"   ë°±ì—…: {backup}")

            collector = NotionDataSourceCollector(NOTION_TOKEN, DATA_SOURCE_ID)
            all_data = collector.collect_all(limit=limit)
            save_json(all_data, str(data_file))
            pages_to_index = all_data

        else:
            # ê¸°ì¡´ ë°ì´í„° í™•ì¸
            existing_info = check_existing_data(str(data_file))

            if not existing_info["exists"]:
                print("\nğŸ“¥ ì´ˆê¸° ìˆ˜ì§‘ ì‹œì‘...")
                collector = NotionDataSourceCollector(NOTION_TOKEN, DATA_SOURCE_ID)
                all_data = collector.collect_all(limit=limit)
                save_json(all_data, str(data_file))
                pages_to_index = all_data

            else:
                # ìƒˆ í˜ì´ì§€ ìˆ˜ì§‘
                collector = NotionDataSourceCollector(NOTION_TOKEN, DATA_SOURCE_ID)
                new_data = collect_missing_pages(
                    collector,
                    existing_info["page_ids"],
                    str(data_file),
                    limit=limit
                )

                # ìˆ˜ì •ëœ í˜ì´ì§€ ì—…ë°ì´íŠ¸
                updated_page_ids = set()
                if check_updates:
                    all_data = load_json(str(data_file))
                    old_data = all_data.copy()
                    all_data = update_changed_pages(collector, all_data, str(data_file))

                    # ì–´ë–¤ í˜ì´ì§€ê°€ ì—…ë°ì´íŠ¸ëëŠ”ì§€ ì¶”ì 
                    for old, new in zip(old_data, all_data):
                        if old.get("last_edited_time") != new.get("last_edited_time"):
                            updated_page_ids.add(new["page_id"])
                else:
                    all_data = load_json(str(data_file))

                # ì¸ë±ì‹±í•  í˜ì´ì§€ ê²°ì •
                pages_to_index = [
                    p for p in all_data
                    if p["page_id"] in updated_page_ids or
                       p in new_data
                ]

        if trace:
            collection_span.end(metadata={
                "total_pages_to_index": len(pages_to_index)
            })

        if not pages_to_index:
            print("\nâœ… ì¸ë±ì‹±í•  í˜ì´ì§€ ì—†ìŒ (ëª¨ë‘ ìµœì‹  ìƒíƒœ)")
            return

        # limit ì ìš©
        if limit and len(pages_to_index) > limit:
            print(f"\nâš ï¸  {len(pages_to_index)}ê°œ í˜ì´ì§€ ì¤‘ {limit}ê°œë§Œ ì²˜ë¦¬")
            pages_to_index = pages_to_index[:limit]

        print(f"\nğŸ“ {len(pages_to_index)}ê°œ í˜ì´ì§€ ì¸ë±ì‹±...")

        # 3. Qdrant ì´ˆê¸°í™” (ì²´í¬ìš© client ìƒì„±)
        if qdrant_client is None:
            qdrant_client = QdrantClient(path=QDRANT_PATH)
        qdrant_info = check_qdrant_collection(qdrant_client)

        if force_recreate or not qdrant_info["exists"]:
            # ì „ì²´ ì¬ìƒì„±
            if trace:
                chunking_span = trace.span(name="chunking")

            all_chunks = []
            for page in pages_to_index:
                chunks = process_page_data(page, embedder, vision_model)
                all_chunks.extend(chunks)
                print(f"  {page.get('title', 'Untitled')}: {len(chunks)}ê°œ ì²­í¬")

            if trace:
                chunking_span.end(metadata={"total_chunks": len(all_chunks)})

            if all_chunks:
                # LangChain Documentsë¡œ ë³€í™˜
                if trace:
                    conversion_span = trace.span(name="document_conversion")

                print(f"\nğŸ“„ LangChain Documentsë¡œ ë³€í™˜ ì¤‘...")
                documents = chunks_to_documents(all_chunks)

                if trace:
                    conversion_span.end(metadata={"num_documents": len(documents)})

                # LangChain Qdrant vectorstore ìƒì„± ë° ì €ì¥
                if trace:
                    storage_span = trace.span(name="qdrant_storage")

                print(f"\nğŸ’¾ LangChain Qdrant ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...")

                # ê¸°ì¡´ client ë‹«ê¸° (ì¤‘ë³µ ì ‘ê·¼ ë°©ì§€)
                if qdrant_client:
                    del qdrant_client
                    qdrant_client = None

                # ìƒˆë¡œìš´ client ìƒì„± ë° ì»¬ë ‰ì…˜ ì„¤ì •
                new_client = QdrantClient(path=QDRANT_PATH)

                # ì»¬ë ‰ì…˜ì´ ìˆìœ¼ë©´ ì‚­ì œ
                try:
                    new_client.delete_collection(QDRANT_COLLECTION)
                    print(f"  ğŸ—‘ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ: {QDRANT_COLLECTION}")
                except Exception:
                    pass

                # ë²¡í„° ì°¨ì› í™•ì¸ (ì²« ë²ˆì§¸ ë¬¸ì„œë¡œ)
                sample_embedding = langchain_embeddings.embed_query(documents[0].page_content)
                vector_dim = len(sample_embedding)
                print(f"  ğŸ“ ë²¡í„° ì°¨ì›: {vector_dim}")

                # ì»¬ë ‰ì…˜ ìƒì„±
                from qdrant_client.models import Distance, VectorParams
                new_client.create_collection(
                    collection_name=QDRANT_COLLECTION,
                    vectors_config=VectorParams(size=vector_dim, distance=Distance.COSINE),
                )
                print(f"  âœ… ì»¬ë ‰ì…˜ ìƒì„±: {QDRANT_COLLECTION}")

                # vectorstore ìƒì„± ë° ë¬¸ì„œ ì¶”ê°€
                vectorstore = Qdrant(
                    client=new_client,
                    collection_name=QDRANT_COLLECTION,
                    embeddings=langchain_embeddings,
                )

                # ë¬¸ì„œ ì¶”ê°€
                vectorstore.add_documents(documents)
                print(f"âœ… {len(documents)}ê°œ ë¬¸ì„œ ì €ì¥ ì™„ë£Œ")

                if trace:
                    storage_span.end()
        else:
            # ì¦ë¶„ ì—…ë°ì´íŠ¸: ë³€ê²½ëœ í˜ì´ì§€ë§Œ ì¬ì¸ë±ì‹±
            print("\nğŸ”„ ë³€ê²½ëœ í˜ì´ì§€ ì¬ì¸ë±ì‹±...")

            if trace:
                incremental_span = trace.span(name="incremental_update")

            # ê¸°ì¡´ vectorstore ë¡œë“œ
            vectorstore = Qdrant(
                client=qdrant_client,
                collection_name=QDRANT_COLLECTION,
                embeddings=langchain_embeddings,
            )

            total_chunks = 0
            for page in pages_to_index:
                page_id = page["page_id"]

                # ê¸°ì¡´ ì²­í¬ ì‚­ì œ
                delete_page_from_vectorstore(vectorstore, page_id)

                # ìƒˆ ì²­í¬ ìƒì„±
                chunks = process_page_data(page, embedder, vision_model)

                if chunks:
                    # LangChain Documentsë¡œ ë³€í™˜ ë° ì¶”ê°€
                    documents = chunks_to_documents(chunks)
                    vectorstore.add_documents(documents)
                    total_chunks += len(chunks)

                    print(f"  âœ… {page.get('title', 'Untitled')}: {len(chunks)}ê°œ ì²­í¬ ì—…ë°ì´íŠ¸")

            if trace:
                incremental_span.end(metadata={
                    "pages_updated": len(pages_to_index),
                    "total_chunks": total_chunks
                })

        # ìºì‹œ í†µê³„ ì¶œë ¥
        print("\nğŸ“Š ì„ë² ë”© ìºì‹œ í†µê³„:")
        cached_embedder.print_stats()

        print("\n" + "=" * 60)
        print("ğŸ‰ Vector DB êµ¬ì¶• ì™„ë£Œ!")
        print("=" * 60)

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--force", action="store_true", help="ì „ì²´ ì¬ìƒì„±")
    parser.add_argument("--no-updates", action="store_true", help="ìˆ˜ì • ì²´í¬ ì•ˆ í•¨")
    parser.add_argument("--limit", type=int, default=None, help="ì²˜ë¦¬í•  í˜ì´ì§€ ìˆ˜ ì œí•œ")
    parser.add_argument("--input-file", type=str, default=None, help="ì‚¬ìš©í•  JSON íŒŒì¼ ê²½ë¡œ")
    args = parser.parse_args()

    main(force_recreate=args.force, check_updates=not args.no_updates, limit=args.limit, input_file=args.input_file)
