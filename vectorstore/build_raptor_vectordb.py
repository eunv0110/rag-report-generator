#!/usr/bin/env python3
"""RAPTOR Vector DB êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from config.settings import *
from models.embeddings.factory import get_embedder
from models.vision.factory import get_vision_model
from core.data_collector import NotionDataSourceCollector
from core.chunker import process_page_data
from retrievers.raptor_tree import RaptorTreeBuilder
from utils.file_utils import save_json, load_json
from utils.langfuse_utils import get_langfuse_client, trace_operation
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
import uuid
from tqdm import tqdm


def init_raptor_collection(
    client: QdrantClient,
    collection_name: str,
    dimension: int,
    recreate: bool = False
):
    """
    RAPTORìš© Qdrant ì»¬ë ‰ì…˜ ì´ˆê¸°í™”

    Args:
        client: Qdrant í´ë¼ì´ì–¸íŠ¸
        collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
        dimension: ë²¡í„° ì°¨ì›
        recreate: ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ í›„ ì¬ìƒì„± ì—¬ë¶€
    """
    if recreate:
        try:
            client.delete_collection(collection_name)
            print(f"  âœ“ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ: {collection_name}")
        except:
            pass

    # ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    collections = [col.name for col in client.get_collections().collections]

    if collection_name not in collections:
        client.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(
                size=dimension,
                distance=Distance.COSINE
            )
        )
        print(f"  âœ“ ì»¬ë ‰ì…˜ ìƒì„±: {collection_name}")
    else:
        print(f"  âœ“ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚¬ìš©: {collection_name}")


def store_raptor_nodes_to_qdrant(
    nodes,
    client: QdrantClient,
    collection_name: str
):
    """
    RAPTOR ë…¸ë“œë“¤ì„ Qdrantì— ì €ì¥

    Args:
        nodes: RaptorNode ë¦¬ìŠ¤íŠ¸
        client: Qdrant í´ë¼ì´ì–¸íŠ¸
        collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
    """
    points = []

    for node in nodes:
        # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
        payload = {
            "chunk_id": node.node_id,
            "page_id": node.metadata.get("page_id", ""),
            "text": node.text,
            "combined_text": node.text,
            "page_title": node.metadata.get("page_title", ""),
            "section_title": node.metadata.get("section_title"),
            "section_path": node.metadata.get("section_path"),
            "has_image": node.metadata.get("has_image", False),
            "image_descriptions": node.metadata.get("image_descriptions", []),
            "level": node.level,
            "children_ids": node.children_ids,
            "parent_id": node.parent_id,
            "properties": {
                "level": node.level,
                "children_ids": node.children_ids,
                "parent_id": node.parent_id,
                **node.metadata
            }
        }

        # Point ìƒì„±
        point = PointStruct(
            id=str(uuid.uuid4()),
            vector=node.embedding.tolist(),
            payload=payload
        )
        points.append(point)

    # ë°°ì¹˜ë¡œ ì €ì¥
    batch_size = 100
    for i in range(0, len(points), batch_size):
        batch = points[i:i + batch_size]
        client.upsert(
            collection_name=collection_name,
            points=batch
        )

    print(f"  âœ“ {len(points)}ê°œ ë…¸ë“œ ì €ì¥ ì™„ë£Œ")


def main(
    force_recreate: bool = False,
    limit: int = None,
    collection_name: str = "notion_raptor",
    max_clusters: int = 10,
    max_tree_depth: int = 3,
    summarizer_type: str = "map_reduce"
):
    """
    RAPTOR Vector DB êµ¬ì¶• ë©”ì¸ í•¨ìˆ˜

    Args:
        force_recreate: Trueë©´ ì „ì²´ ì¬ìƒì„±
        limit: ì²˜ë¦¬í•  í˜ì´ì§€ ìˆ˜ ì œí•œ (Noneì´ë©´ ì „ì²´)
        collection_name: ì €ì¥í•  ì»¬ë ‰ì…˜ ì´ë¦„
        max_clusters: í´ëŸ¬ìŠ¤í„°ë§ ì‹œ ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜
        max_tree_depth: RAPTOR íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´
        summarizer_type: ìš”ì•½ ë°©ì‹ ("map_reduce" ë˜ëŠ” "refine")
    """
    print("=" * 60)
    print("ğŸŒ³ RAPTOR Vector DB êµ¬ì¶• ì‹œì‘")
    if limit:
        print(f"ğŸ“Š ì œí•œ: {limit}ê°œ í˜ì´ì§€ë§Œ ì²˜ë¦¬")
    print(f"ğŸ“¦ ì»¬ë ‰ì…˜: {collection_name}")
    print(f"ğŸ“ ìš”ì•½ ë°©ì‹: {summarizer_type}")
    print("=" * 60)

    # Langfuse ì´ˆê¸°í™”
    get_langfuse_client()

    # ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ Langfuseë¡œ íŠ¸ë ˆì´ì‹±
    with trace_operation(
        name="raptor_vectordb_build",
        metadata={
            "force_recreate": force_recreate,
            "limit": limit,
            "collection_name": collection_name,
            "max_clusters": max_clusters,
            "max_tree_depth": max_tree_depth
        }
    ) as trace:

        data_file = DATA_DIR / "notion_data.json"

        # 1. ëª¨ë¸ ì´ˆê¸°í™”
        print("\nğŸ“¦ ëª¨ë¸ ë¡œë”©...")
        if trace:
            model_span = trace.span(name="model_initialization")

        embedder = get_embedder()
        vision_model = get_vision_model()
        qdrant_client = QdrantClient(path=QDRANT_PATH)

        # RAPTOR Tree Builder ì´ˆê¸°í™”
        tree_builder = RaptorTreeBuilder(
            embedder=embedder,
            max_clusters=max_clusters,
            max_tree_depth=max_tree_depth,
            summarizer_type=summarizer_type
        )

        if trace:
            model_span.end()

        # 2. ë°ì´í„° ë¡œë“œ
        print("\nğŸ“‚ ë°ì´í„° ë¡œë”©...")
        if not data_file.exists():
            print("âŒ notion_data.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            print("ë¨¼ì € build_vectordb.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”.")
            return

        all_data = load_json(str(data_file))
        pages_to_process = all_data[:limit] if limit else all_data
        print(f"  âœ“ {len(pages_to_process)}ê°œ í˜ì´ì§€ ë¡œë“œ")

        # 3. í˜ì´ì§€ë³„ë¡œ ì²­í¬ ìƒì„±
        print("\nâœ‚ï¸ ì²­í‚¹ ë° íŠ¸ë¦¬ êµ¬ì¶• ì¤€ë¹„...")
        if trace:
            chunking_span = trace.span(name="chunking")

        all_chunks = []
        chunk_metadata_list = []

        for page in tqdm(pages_to_process, desc="ì²­í‚¹"):
            chunks = process_page_data(page, embedder, vision_model)

            for chunk in chunks:
                all_chunks.append(chunk.combined_text)
                chunk_metadata_list.append({
                    "page_id": chunk.page_id,
                    "page_title": chunk.page_title,
                    "section_title": chunk.section_title,
                    "section_path": chunk.section_path,
                    "has_image": chunk.has_image,
                    "image_descriptions": chunk.image_descriptions
                })

        if trace:
            chunking_span.end(metadata={"total_chunks": len(all_chunks)})

        print(f"  âœ“ ì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„±")

        if not all_chunks:
            print("âŒ ì²­í¬ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            return

        # 4. RAPTOR íŠ¸ë¦¬ êµ¬ì¶•
        print("\nğŸŒ³ RAPTOR íŠ¸ë¦¬ êµ¬ì¶• ì¤‘...")
        if trace:
            tree_span = trace.span(name="raptor_tree_build")

        raptor_nodes = tree_builder.build_tree(
            texts=all_chunks,
            metadatas=chunk_metadata_list
        )

        if trace:
            tree_span.end(metadata={
                "total_nodes": len(raptor_nodes),
                "levels": max([node.level for node in raptor_nodes]) + 1
            })

        # ë ˆë²¨ë³„ í†µê³„
        level_counts = {}
        for node in raptor_nodes:
            level_counts[node.level] = level_counts.get(node.level, 0) + 1

        print("\nğŸ“Š ë ˆë²¨ë³„ ë…¸ë“œ ìˆ˜:")
        for level in sorted(level_counts.keys()):
            print(f"  ë ˆë²¨ {level}: {level_counts[level]}ê°œ")

        # 5. Qdrantì— ì €ì¥
        print(f"\nğŸ’¾ Qdrant ì €ì¥ ì¤‘ (ì»¬ë ‰ì…˜: {collection_name})...")
        if trace:
            storage_span = trace.span(name="qdrant_storage")

        # ì„ë² ë”© ì°¨ì› í™•ì¸
        embedding_dimension = len(raptor_nodes[0].embedding)

        # ì»¬ë ‰ì…˜ ì´ˆê¸°í™”
        init_raptor_collection(
            qdrant_client,
            collection_name,
            embedding_dimension,
            recreate=force_recreate
        )

        # ë…¸ë“œ ì €ì¥
        store_raptor_nodes_to_qdrant(
            raptor_nodes,
            qdrant_client,
            collection_name
        )

        if trace:
            storage_span.end()

        # 6. ê²€ì¦
        print("\nğŸ” ì €ì¥ ê²€ì¦...")
        collection_info = qdrant_client.get_collection(collection_name)
        print(f"  âœ“ ì €ì¥ëœ í¬ì¸íŠ¸ ìˆ˜: {collection_info.points_count}")

        print("\n" + "=" * 60)
        print("ğŸ‰ RAPTOR Vector DB êµ¬ì¶• ì™„ë£Œ!")
        print(f"ğŸ“¦ ì»¬ë ‰ì…˜: {collection_name}")
        print(f"ğŸ“Š ì´ ë…¸ë“œ ìˆ˜: {len(raptor_nodes)}")
        print(f"ğŸŒ² íŠ¸ë¦¬ ë ˆë²¨: {len(level_counts)}")
        print("=" * 60)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="RAPTOR Vector DB êµ¬ì¶•")
    parser.add_argument("--force", action="store_true", help="ì „ì²´ ì¬ìƒì„±")
    parser.add_argument("--limit", type=int, default=None, help="ì²˜ë¦¬í•  í˜ì´ì§€ ìˆ˜ ì œí•œ")
    parser.add_argument("--collection", type=str, default="notion_raptor", help="ì»¬ë ‰ì…˜ ì´ë¦„")
    parser.add_argument("--max-clusters", type=int, default=10, help="ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ê°œìˆ˜")
    parser.add_argument("--max-depth", type=int, default=3, help="ìµœëŒ€ íŠ¸ë¦¬ ê¹Šì´")
    parser.add_argument("--summarizer", type=str, default="map_reduce", choices=["map_reduce", "refine"], help="ìš”ì•½ ë°©ì‹ (ê¸°ë³¸ê°’: map_reduce)")

    args = parser.parse_args()

    main(
        force_recreate=args.force,
        limit=args.limit,
        collection_name=args.collection,
        max_clusters=args.max_clusters,
        max_tree_depth=args.max_depth,
        summarizer_type=args.summarizer
    )
