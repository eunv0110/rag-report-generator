"""BM25 ê¸°ë°˜ ê²€ìƒ‰ ë¦¬íŠ¸ë¦¬ë²„"""

from typing import List, Dict, Any, Optional
from rank_bm25 import BM25Okapi
from qdrant_client import QdrantClient
from config.settings import QDRANT_COLLECTION
from .base_retriever import BaseRetriever, SearchResult
import jieba


class BM25Retriever(BaseRetriever):
    """BM25 ê¸°ë°˜ ê²€ìƒ‰ ì—”ì§„"""

    def __init__(self, qdrant_client: QdrantClient, use_korean_tokenizer: bool = True):
        """
        Args:
            qdrant_client: Qdrant í´ë¼ì´ì–¸íŠ¸
            use_korean_tokenizer: í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì‚¬ìš© ì—¬ë¶€ (jieba ì‚¬ìš©)
        """
        self.client = qdrant_client
        self.use_korean_tokenizer = use_korean_tokenizer
        self.corpus = []
        self.tokenized_corpus = []
        self.bm25 = None
        self.metadata = []

        self._load_corpus()

    def _tokenize(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•"""
        if self.use_korean_tokenizer:
            # jiebaë¥¼ ì‚¬ìš©í•œ í•œêµ­ì–´/ì¤‘êµ­ì–´ í† í¬ë‚˜ì´ì§•
            return list(jieba.cut(text.lower()))
        else:
            # ê¸°ë³¸ ê³µë°± ê¸°ë°˜ í† í¬ë‚˜ì´ì§•
            return text.lower().split()

    def _load_corpus(self):
        """Qdrantì—ì„œ ì „ì²´ ë¬¸ì„œ ë¡œë“œ ë° BM25 ì¸ë±ìŠ¤ êµ¬ì¶•"""
        print("ðŸ“š BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")

        # Qdrantì—ì„œ ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        scroll_result = self.client.scroll(
            collection_name=QDRANT_COLLECTION,
            limit=10000,
            with_payload=True,
            with_vectors=False  # ë²¡í„°ëŠ” í•„ìš” ì—†ìŒ
        )

        points = scroll_result[0]

        for point in points:
            payload = point.payload

            # combined_textë¥¼ corpusë¡œ ì‚¬ìš©
            text = payload.get("combined_text", "")
            self.corpus.append(text)
            self.tokenized_corpus.append(self._tokenize(text))

            # ë©”íƒ€ë°ì´í„° ì €ìž¥
            self.metadata.append({
                "chunk_id": payload.get("chunk_id"),
                "page_id": payload.get("page_id"),
                "text": payload.get("text"),
                "combined_text": text,
                "page_title": payload.get("page_title"),
                "section_title": payload.get("section_title"),
                "section_path": payload.get("section_path"),
                "has_image": payload.get("has_image", False),
                "image_descriptions": payload.get("image_descriptions", []),
                "properties": payload.get("properties", {})
            })

        # BM25 ì¸ë±ìŠ¤ êµ¬ì¶•
        self.bm25 = BM25Okapi(self.tokenized_corpus)

        print(f"âœ… BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(self.corpus)}ê°œ ë¬¸ì„œ")

    def search(self, query: str, top_k: int = 5) -> List[SearchResult]:
        """
        BM25ë¡œ ë¬¸ì„œ ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ê°œìˆ˜

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if not self.bm25:
            raise ValueError("BM25 ì¸ë±ìŠ¤ê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        # ì¿¼ë¦¬ í† í¬ë‚˜ì´ì§•
        tokenized_query = self._tokenize(query)

        # BM25 ìŠ¤ì½”ì–´ ê³„ì‚°
        scores = self.bm25.get_scores(tokenized_query)

        # ìƒìœ„ kê°œ ì„ íƒ
        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]

        # ê²°ê³¼ êµ¬ì„±
        results = []
        for idx in top_indices:
            meta = self.metadata[idx]
            results.append(SearchResult(
                chunk_id=meta["chunk_id"],
                page_id=meta["page_id"],
                text=meta["text"],
                combined_text=meta["combined_text"],
                page_title=meta["page_title"],
                section_title=meta["section_title"],
                section_path=meta["section_path"],
                score=float(scores[idx]),
                has_image=meta["has_image"],
                image_descriptions=meta["image_descriptions"],
                properties=meta["properties"]
            ))

        return results

    def batch_search(self, queries: List[str], top_k: int = 5) -> List[List[SearchResult]]:
        """
        ì—¬ëŸ¬ ì¿¼ë¦¬ë¥¼ ë°°ì¹˜ë¡œ ê²€ìƒ‰

        Args:
            queries: ê²€ìƒ‰ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
            top_k: ê° ì¿¼ë¦¬ë‹¹ ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ê°œìˆ˜

        Returns:
            ê° ì¿¼ë¦¬ë³„ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        return [self.search(query, top_k) for query in queries]

    @property
    def name(self) -> str:
        """ë¦¬íŠ¸ë¦¬ë²„ ì´ë¦„ ë°˜í™˜"""
        tokenizer_name = "Korean" if self.use_korean_tokenizer else "Basic"
        return f"BM25_{tokenizer_name}"

    def get_info(self) -> Dict[str, Any]:
        """ë¦¬íŠ¸ë¦¬ë²„ ì •ë³´ ë°˜í™˜"""
        info = super().get_info()
        info.update({
            "use_korean_tokenizer": self.use_korean_tokenizer,
            "corpus_size": len(self.corpus)
        })
        return info
