# 평가 데이터셋 품질 분석 및 개선 방안

## 현재 데이터셋 문제점

### 1. 프롬프트와 실제 데이터의 불일치

**프롬프트 의도** (qa_generation_prompt_simple.txt):
- 보고서 작성을 요청하는 사용자 질문
- 5~15단어
- 구어체
- 예시: "[주제] 보고서 만들어줘"

**실제 생성된 데이터**:
- 평균 질문 길이: 93.9자 (기대치의 약 3배)
- 긴 질문(100자 이상) 비율: 54.4%
- 문체: 시험 문제/논술형 (구어체 아님)
- 예시: "이 문서에서 테니스 장비 데이터(장비 정의·비교·교체 시기)를 수집하려는 주요 이유는 각각 무엇이며, 이를 위해 제시된 구체적인 데이터 수집 경로는 어떻게 구분되어 있는지 설명하시오."

### 2. 심각한 품질 문제

#### 질문이 너무 어렵고 구체적함
```
질문 복잡도 분포:
  짧음 (<50자): 40개 (44.4%)
  보통 (50-100자): 1개 (1.1%)
  김 (>=100자): 49개 (54.4%)
```

- 다중 요구사항 포함 ("각각 무엇이며", "어떻게 구분되어 있는지")
- 문서 구조를 명시적으로 언급 ("이 문서에서")
- 실제 사용자가 하지 않을 법한 질문

#### 평가 지표와의 미스매치
- **Context Precision/Recall**: 질문이 문서 구조를 명시적으로 언급 ("이 문서에서")
  - 실제 사용자는 "이 문서에서"라고 하지 않음
  - 검색기가 부자연스럽게 유리해짐
- **Answer Relevancy**: 질문이 너무 복잡해서 답변 관련성 판단 어려움
- **질문의 모호성 부족**: 실제 환경에서는 모호한 질문도 많지만, 현재는 모두 명확하고 구체적

#### 정답(ground_truth) 길이는 적절
```
정답 길이 통계:
  평균: 910.1자 ✅ (오히려 좋음)
  최소: 205자
  최대: 2,056자
```

**재평가**: 정답이 긴 것은 문제가 아닙니다!
- RAGAS에서 ground_truth는 Context Recall 계산에 사용
- 정답이 상세할수록 검색기가 충분한 문서를 찾았는지 평가 가능
- 긴 정답 자체는 오히려 평가에 유리

### 3. 실제 사용자 질문과의 괴리

**현재 데이터셋**:
```
"테니스 장비 데이터를 수집하려는 주요 이유는 각각 무엇이며,
이를 위해 제시된 구체적인 데이터 수집 경로는 어떻게 구분되어 있는지 설명하시오."
```

**실제 사용자가 할 법한 질문**:
```
"테니스 장비 데이터 수집 계획 보고서 만들어줘"
"테니스 장비 선택 가이드 만들어줘"
"장비 교체 시기 분석 보고서 만들어줘"
```

## 개선 방안

### 1. 프롬프트 수정

**현재 프롬프트**는 5~15단어를 요구하지만, LLM이 무시하고 복잡한 질문 생성

**개선된 프롬프트** 예시:
```
[질문 생성 규칙]
- 보고서 작성을 요청하는 자연스러운 질문
- 반드시 "~보고서 만들어줘" 또는 "~분석해줘" 형태
- 10단어 이내 (엄격)
- 구체적인 세부사항 요구 금지
- "이 문서에서", "설명하시오" 등 학술적 표현 금지

좋은 예시:
  ✅ "테니스 장비 선택 가이드 만들어줘"
  ✅ "모멘텀 계산 결과 보고서 만들어줘"
  ✅ "데이터 수집 기준 정리해줘"

나쁜 예시:
  ❌ "이 문서에서 테니스 장비 데이터를 수집하려는 주요 이유는 각각 무엇이며..."
  ❌ "~을 종합적으로 설명하시오"
  ❌ 100자 이상의 긴 질문
```

### 2. 정답(ground_truth) 길이는 유지 ✅

**현재**: 평균 910자

**판단**: 정답이 긴 것은 문제가 아닙니다!
- RAGAS의 Context Recall: 정답이 상세할수록 검색 품질을 정확히 평가 가능
- Answer Correctness: 생성된 답변과 의미적 유사도 비교에 유리
- 보고서 생성 시스템이므로 상세한 정답이 적합

**유지**:
```
[답변 생성 규칙]
- 문서 내용에만 근거
- 핵심 내용·결과·시사점을 상세히 포함 (200-1000자)
- 평가용 기준 정답으로 사용 가능하게 명확하게 작성
- 문서에 없는 정보는 추가하지 말 것
```

### 3. Few-shot 예시 추가

프롬프트에 실제 예시 3-5개 추가:

```json
[예시 1]
입력 문서: "테니스 RAG 챗봇 - 데이터 기획 초안"
{
  "question": "테니스 장비 선택 가이드 만들어줘",
  "ground_truth": "테니스 장비 선택을 위한 데이터는 세 가지 영역으로 구분됩니다. 첫째, 장비 정의 영역에서는 라켓, 스트링, 신발, 의류, 액세서리에 대한 기본 정보를 위키피디아에서 수집합니다. 둘째, 장비 비교 영역에서는 브랜드별 라켓 비교표, 가격대별 성능 분석, 사용자 레벨별 추천 장비, 프로 선수 사용 장비 정보를 네이버·티스토리 블로그, 동호회 카페, 유튜브 리뷰에서 수집하여 중복 정보를 정제합니다. 셋째, 장비 교체 시기 영역에서는 라켓 교체 주기 및 판단 기준, 스트링 교체 타이밍, 신발 마모도 체크 포인트를 관련 블로그와 테니스서브아카데미 등의 유튜브 채널에서 수집하여 마모된 장비로 인한 부상을 예방할 수 있도록 합니다."
}

[예시 2]
입력 문서: "테니스 모멘텀 계산 및 시각화 결과"
{
  "question": "모멘텀 계산 결과 보고서 만들어줘",
  "ground_truth": "테니스 모멘텀 프로젝트에서는 HMM(Hidden Markov Model)으로 숨겨진 모멘텀 상태를 추정하고, EMA(Exponential Moving Average)로 값을 부드럽게 처리하여 최종 모멘텀 범위를 -2에서 +2로 달성했습니다. 전체 6,605개 경기 데이터에 적용한 히트맵 분석에서는 평균화 과정에서 개별 패턴이 소실되어 약한 상관관계만 나타났으나, 개별 경기 단위 분석으로 전환하면 모멘텀과 승리 확률 간 강한 상관관계를 관찰할 수 있어, 개별 경기 분석이 더 의미 있다고 결론지었습니다."
}
```

### 4. 후처리 검증 추가

생성된 QA를 자동 검증:

```python
def validate_qa(question: str, ground_truth: str) -> bool:
    """생성된 QA 품질 검증"""

    # 질문 검증
    if len(question) > 50:  # 50자 초과
        return False

    if "설명하시오" in question or "무엇이며" in question:
        return False

    if "이 문서에서" in question or "문서에서" in question:
        return False

    if "보고서" not in question and "분석" not in question and "정리" not in question and "가이드" not in question:
        return False

    # 정답 검증 (긴 것은 OK!)
    if len(ground_truth) < 100:  # 너무 짧으면 평가에 부적합
        return False

    if len(ground_truth) > 2000:  # 너무 길면 토큰 낭비
        return False

    return True
```

### 5. 난이도 다양화

현재는 모든 질문이 "medium"으로 고정

**개선**:
```python
difficulty_criteria = {
    "easy": {
        "question_type": "단일 주제 보고서 요청",
        "example": "테니스 장비 정의 정리해줘",
        "ground_truth_length": "200-400자",
        "coverage": "문서의 한 섹션만 다룸"
    },
    "medium": {
        "question_type": "비교/분석 보고서 요청",
        "example": "장비 비교 분석 보고서 만들어줘",
        "ground_truth_length": "400-800자",
        "coverage": "문서의 여러 섹션을 연결"
    },
    "hard": {
        "question_type": "다중 요소 통합 보고서 요청",
        "example": "테니스 장비 종합 가이드 만들어줘",
        "ground_truth_length": "800-1500자",
        "coverage": "문서 전체를 통합 분석"
    }
}
```

## 다음 단계

### 즉시 조치 필요
1. ✅ 현재 데이터셋 품질 문제 확인 완료
2. ⏭️ 프롬프트 수정 (qa_generation_prompt_simple.txt)
3. ⏭️ Few-shot 예시 3-5개 추가
4. ⏭️ 후처리 검증 로직 추가

### 재생성 필요
- 현재 90개 질문 중 대부분이 프롬프트 의도와 불일치
- 개선된 프롬프트로 데이터셋 재생성 권장

### 평가 전략 재검토
- 현재 데이터셋으로는 검색기 성능을 정확히 평가할 수 없음
- 간단하고 자연스러운 질문으로 재구성 필요
