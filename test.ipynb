{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6da42cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ í…Œë‹ˆìŠ¤ ê¸°ë³¸ ê·œì¹™ ë° ëª¨ë©˜í…€ ì˜ˆì¸¡ ë…¼ë¬¸ ë¦¬ë·°\n",
      "ğŸ–¼ï¸ ì´ë¯¸ì§€ ê°œìˆ˜: 2\n",
      "ğŸ“ ì›ë³¸ í…ìŠ¤íŠ¸: ## HMM (Hidden Markov Model)\n",
      "â€¢ HMM = ì‹œê°„ì— ë”°ë¥¸ ìˆ¨ê²¨ì§„ ìƒíƒœ ì „ì´ + ê´€ì¸¡ê°’ ë°œìƒ\n",
      "â€¢ ìƒíƒœëŠ” ì‹œê°„ ìˆœì„œì— ë”°ë¼ ì´ì „ ìƒíƒœ â†’ í˜„ì¬ ìƒíƒœë¡œ ì „ì´ (Ma...\n",
      "ğŸ” ê²€ìƒ‰ìš© í…ìŠ¤íŠ¸: ## HMM (Hidden Markov Model)\n",
      "â€¢ HMM = ì‹œê°„ì— ë”°ë¥¸ ìˆ¨ê²¨ì§„ ìƒíƒœ ì „ì´ + ê´€ì¸¡ê°’ ë°œìƒ\n",
      "â€¢ ìƒíƒœëŠ” ì‹œê°„ ìˆœì„œì— ë”°ë¼ ì´ì „ ìƒíƒœ â†’ í˜„ì¬ ìƒíƒœë¡œ ì „ì´ (Ma...\n",
      "ğŸ’¬ ì´ë¯¸ì§€ ì„¤ëª…: [\"[ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: [Errno 2] No such file or directory: 'notion_images//home/work/rag/Project/rag-report-generator/data/notion_images/23da296c-8853-8075-afd1-ca7daa3c3e7f_23da296c-8853-80b2-9e07-c5d1a36f24ce.png']\", \"[ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: [Errno 2] No such file or directory: 'notion_images//home/work/rag/Project/rag-report-generator/data/notion_images/23da296c-8853-8075-afd1-ca7daa3c3e7f_23da296c-8853-8089-8f51-f7ab123680cd.png']\"]\n",
      "\n",
      "ğŸ“„ í…Œë‹ˆìŠ¤ ê¸°ë³¸ ê·œì¹™ ë° ëª¨ë©˜í…€ ì˜ˆì¸¡ ë…¼ë¬¸ ë¦¬ë·°\n",
      "ğŸ–¼ï¸ ì´ë¯¸ì§€ ê°œìˆ˜: 1\n",
      "ğŸ“ ì›ë³¸ í…ìŠ¤íŠ¸: # ë…¼ë¬¸ ë¦¬ë·° - Capturing Momentum: Tennis Match Analysis Using Machine Learning and Time Series Theory\n",
      "[I...\n",
      "ğŸ” ê²€ìƒ‰ìš© í…ìŠ¤íŠ¸: # ë…¼ë¬¸ ë¦¬ë·° - Capturing Momentum: Tennis Match Analysis Using Machine Learning and Time Series Theory\n",
      "[ì´...\n",
      "ğŸ’¬ ì´ë¯¸ì§€ ì„¤ëª…: [\"[ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: [Errno 2] No such file or directory: 'notion_images//home/work/rag/Project/rag-report-generator/data/notion_images/23da296c-8853-8075-afd1-ca7daa3c3e7f_23da296c-8853-8080-a45a-f8e974c97193.png']\"]\n",
      "\n",
      "ğŸ“„ í…Œë‹ˆìŠ¤ ëª¨ë©˜í…€ ê³„ì‚° ë° ì‹œê°í™” ê²°ê³¼\n",
      "ğŸ–¼ï¸ ì´ë¯¸ì§€ ê°œìˆ˜: 1\n",
      "ğŸ“ ì›ë³¸ í…ìŠ¤íŠ¸: ### ì „ì²´ ë°ì´í„° íˆíŠ¸ë§µ\n",
      "[Image: /home/work/rag/Project/rag-report-generator/data/notion_images/23da296c-8853-...\n",
      "ğŸ” ê²€ìƒ‰ìš© í…ìŠ¤íŠ¸: ### ì „ì²´ ë°ì´í„° íˆíŠ¸ë§µ\n",
      "[ì´ë¯¸ì§€: [ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: [Errno 2] No such file or directory: 'notion_images//home/work/rag/...\n",
      "ğŸ’¬ ì´ë¯¸ì§€ ì„¤ëª…: [\"[ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: [Errno 2] No such file or directory: 'notion_images//home/work/rag/Project/rag-report-generator/data/notion_images/23da296c-8853-8003-b94f-ed0a1e223920_23da296c-8853-804d-b575-d40c98e731fd.png']\"]\n"
     ]
    }
   ],
   "source": [
    "# ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸\n",
    "from qdrant_client import QdrantClient\n",
    "from config.settings import QDRANT_PATH, QDRANT_COLLECTION\n",
    "\n",
    "client = QdrantClient(path=QDRANT_PATH)\n",
    "\n",
    "# ì´ë¯¸ì§€ê°€ ìˆëŠ” ì²­í¬ ì°¾ê¸°\n",
    "results = client.scroll(\n",
    "    collection_name=QDRANT_COLLECTION,\n",
    "    limit=10,\n",
    "    with_payload=True\n",
    ")\n",
    "\n",
    "for point in results[0]:\n",
    "    if point.payload.get(\"has_image\"):\n",
    "        print(f\"\\nğŸ“„ {point.payload['page_title']}\")\n",
    "        print(f\"ğŸ–¼ï¸ ì´ë¯¸ì§€ ê°œìˆ˜: {len(point.payload['image_paths'])}\")\n",
    "        print(f\"ğŸ“ ì›ë³¸ í…ìŠ¤íŠ¸: {point.payload['text'][:100]}...\")\n",
    "        print(f\"ğŸ” ê²€ìƒ‰ìš© í…ìŠ¤íŠ¸: {point.payload['combined_text'][:100]}...\")\n",
    "        print(f\"ğŸ’¬ ì´ë¯¸ì§€ ì„¤ëª…: {point.payload['image_descriptions']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6149b26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "import base64\n",
    "import httpx\n",
    "from datetime import datetime\n",
    "from typing import Optional, List, Dict, Any\n",
    "from dataclasses import dataclass, field\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from notion_client import Client\n",
    "from openai import OpenAI\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import (\n",
    "    VectorParams, Distance, PointStruct,\n",
    "    Filter, FieldCondition, MatchValue,\n",
    "    PayloadSchemaType\n",
    ")\n",
    "from utils.prompt_loader import load_prompt\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2483ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========== ì„¤ì • ==========\n",
    "NOTION_TOKEN = os.environ.get(\"NOTION_TOKEN\")\n",
    "DATA_SOURCE_ID = os.environ.get(\"DATA_SOURCE_ID\")  # í•„ìˆ˜!\n",
    "IMAGE_DIR = \"notion_images\"\n",
    "\n",
    "OPENROUTER_API_KEY = os.environ.get(\"OPENROUTER_API_KEY\")\n",
    "OPENROUTER_BASE_URL = \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "AZURE_AI_CREDENTIAL = os.environ.get(\"AZURE_AI_CREDENTIAL\")\n",
    "AZURE_AI_ENDPOINT = os.environ.get(\"AZURE_AI_ENDPOINT\", \"https://models.inference.ai.azure.com\")\n",
    "\n",
    "EMBEDDING_MODEL = \"openai/text-embedding-3-large\"\n",
    "EMBEDDING_DIM = 3072\n",
    "IMAGE_MODEL = \"gpt-4.1\"\n",
    "\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 50\n",
    "IMAGE_CONTEXT_CHARS = 300\n",
    "QDRANT_COLLECTION = \"notion_docs\"\n",
    "\n",
    "\n",
    "\n",
    "# ========== ë°ì´í„° í´ë˜ìŠ¤ ==========\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    chunk_id: str\n",
    "    page_id: str\n",
    "    text: str\n",
    "    combined_text: str = \"\"\n",
    "    has_image: bool = False\n",
    "    image_paths: list = field(default_factory=list)  # âœ… ì¶”ê°€\n",
    "    image_descriptions: list = field(default_factory=list)  # âœ… ì¶”ê°€\n",
    "    page_title: str = \"\"\n",
    "    section_title: str = \"\"\n",
    "    section_path: list = field(default_factory=list)\n",
    "    heading_level: int = 0\n",
    "    chunk_index: int = 0\n",
    "    created_time: str = \"\"\n",
    "    last_edited_time: str = \"\"\n",
    "    properties: dict = field(default_factory=dict)\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Set\n",
    "\n",
    "# ========== ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ==========\n",
    "def check_existing_data(filepath: str = \"notion_data.json\") -> Dict:\n",
    "    \"\"\"\n",
    "    ê¸°ì¡´ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ ë° í†µê³„ í™•ì¸\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"exists\": bool,\n",
    "            \"count\": int,\n",
    "            \"page_ids\": set,\n",
    "            \"last_updated\": str\n",
    "        }\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"âŒ íŒŒì¼ ì—†ìŒ: {filepath}\")\n",
    "        return {\n",
    "            \"exists\": False,\n",
    "            \"count\": 0,\n",
    "            \"page_ids\": set(),\n",
    "            \"last_updated\": None\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        page_ids = {item[\"page_id\"] for item in data}\n",
    "        \n",
    "        # ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ ì°¾ê¸°\n",
    "        last_times = [item.get(\"last_edited_time\", \"\") for item in data]\n",
    "        last_updated = max(last_times) if last_times else None\n",
    "        \n",
    "        print(f\"âœ… ê¸°ì¡´ ë°ì´í„°: {len(data)}ê°œ í˜ì´ì§€\")\n",
    "        print(f\"   ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {last_updated}\")\n",
    "        \n",
    "        return {\n",
    "            \"exists\": True,\n",
    "            \"count\": len(data),\n",
    "            \"page_ids\": page_ids,\n",
    "            \"last_updated\": last_updated\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "        return {\n",
    "            \"exists\": False,\n",
    "            \"count\": 0,\n",
    "            \"page_ids\": set(),\n",
    "            \"last_updated\": None\n",
    "        }\n",
    "\n",
    "\n",
    "def check_qdrant_data(qdrant_client: QdrantClient) -> Dict:\n",
    "    \"\"\"\n",
    "    Qdrant ì»¬ë ‰ì…˜ì˜ ë°ì´í„° í™•ì¸\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"exists\": bool,\n",
    "            \"count\": int,\n",
    "            \"page_ids\": set\n",
    "        }\n",
    "    \"\"\"\n",
    "    try:\n",
    "        collections = qdrant_client.get_collections().collections\n",
    "        exists = any(c.name == QDRANT_COLLECTION for c in collections)\n",
    "        \n",
    "        if not exists:\n",
    "            print(f\"âŒ Qdrant ì»¬ë ‰ì…˜ ì—†ìŒ: {QDRANT_COLLECTION}\")\n",
    "            return {\"exists\": False, \"count\": 0, \"page_ids\": set()}\n",
    "        \n",
    "        # ì»¬ë ‰ì…˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "        info = qdrant_client.get_collection(QDRANT_COLLECTION)\n",
    "        count = info.points_count\n",
    "        \n",
    "        # ëª¨ë“  page_id ì¶”ì¶œ (ìƒ˜í”Œë§)\n",
    "        scroll_result = qdrant_client.scroll(\n",
    "            collection_name=QDRANT_COLLECTION,\n",
    "            limit=10000,\n",
    "            with_payload=[\"page_id\"]\n",
    "        )\n",
    "        \n",
    "        page_ids = {point.payload[\"page_id\"] for point in scroll_result[0]}\n",
    "        \n",
    "        print(f\"âœ… Qdrant ë°ì´í„°: {count}ê°œ ì²­í¬, {len(page_ids)}ê°œ í˜ì´ì§€\")\n",
    "        \n",
    "        return {\n",
    "            \"exists\": True,\n",
    "            \"count\": count,\n",
    "            \"page_ids\": page_ids\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Qdrant í™•ì¸ ì‹¤íŒ¨: {e}\")\n",
    "        return {\"exists\": False, \"count\": 0, \"page_ids\": set()}\n",
    "\n",
    "\n",
    "# ========== ì¦ë¶„ ìˆ˜ì§‘ ==========\n",
    "def collect_missing_pages(\n",
    "    collector: NotionDataSourceCollector,\n",
    "    existing_page_ids: Set[str],\n",
    "    filepath: str = \"notion_data.json\"\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    ê¸°ì¡´ ë°ì´í„°ì— ì—†ëŠ” ìƒˆ í˜ì´ì§€ë§Œ ìˆ˜ì§‘\n",
    "    \n",
    "    Args:\n",
    "        collector: NotionDataSourceCollector ì¸ìŠ¤í„´ìŠ¤\n",
    "        existing_page_ids: ì´ë¯¸ ìˆ˜ì§‘ëœ page_id ì§‘í•©\n",
    "        filepath: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ\n",
    "    \n",
    "    Returns:\n",
    "        ìƒˆë¡œ ìˆ˜ì§‘í•œ í˜ì´ì§€ ë°ì´í„° ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ” Notionì—ì„œ ì „ì²´ í˜ì´ì§€ ëª©ë¡ ê°€ì ¸ì˜¤ëŠ” ì¤‘...\")\n",
    "    all_pages = collector.get_all_pages_from_datasource()\n",
    "    \n",
    "    # ìƒˆ í˜ì´ì§€ í•„í„°ë§\n",
    "    new_pages = [p for p in all_pages if p[\"id\"] not in existing_page_ids]\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ë¶„ì„ ê²°ê³¼:\")\n",
    "    print(f\"   ì „ì²´ í˜ì´ì§€: {len(all_pages)}ê°œ\")\n",
    "    print(f\"   ê¸°ì¡´ í˜ì´ì§€: {len(existing_page_ids)}ê°œ\")\n",
    "    print(f\"   ìƒˆ í˜ì´ì§€: {len(new_pages)}ê°œ\")\n",
    "    \n",
    "    if not new_pages:\n",
    "        print(\"\\nâœ… ëª¨ë“  í˜ì´ì§€ê°€ ì´ë¯¸ ìˆ˜ì§‘ë˜ì–´ ìˆìŠµë‹ˆë‹¤!\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"\\nğŸš€ {len(new_pages)}ê°œ ìƒˆ í˜ì´ì§€ ìˆ˜ì§‘ ì‹œì‘...\\n\")\n",
    "    \n",
    "    new_data = []\n",
    "    for idx, page in enumerate(new_pages):\n",
    "        page_id = page[\"id\"]\n",
    "        properties = collector.extract_page_properties(page)\n",
    "        title = collector.get_page_title(properties)\n",
    "        \n",
    "        print(f\"[{idx+1}/{len(new_pages)}] ğŸ“„ {title}\")\n",
    "        \n",
    "        try:\n",
    "            blocks = collector.get_all_blocks(page_id)\n",
    "            content_lines = [collector.extract_block_content(b, page_id) for b in blocks]\n",
    "            full_content = \"\\n\".join(filter(None, content_lines))\n",
    "            \n",
    "            new_data.append({\n",
    "                \"page_id\": page_id,\n",
    "                \"title\": title,\n",
    "                \"created_time\": page.get(\"created_time\", \"\"),\n",
    "                \"last_edited_time\": page.get(\"last_edited_time\", \"\"),\n",
    "                \"properties\": properties,\n",
    "                \"content\": full_content\n",
    "            })\n",
    "            print(f\"  â†’ {len(blocks)}ê°œ ë¸”ë¡, {len(full_content)}ì\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ ì‹¤íŒ¨: {e}\")\n",
    "            new_data.append({\n",
    "                \"page_id\": page_id, \n",
    "                \"title\": title, \n",
    "                \"content\": \"\",\n",
    "                \"created_time\": page.get(\"created_time\", \"\"),\n",
    "                \"last_edited_time\": page.get(\"last_edited_time\", \"\"),\n",
    "                \"properties\": properties\n",
    "            })\n",
    "    \n",
    "    # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©\n",
    "    if os.path.exists(filepath):\n",
    "        existing_data = load_data(filepath)\n",
    "        merged_data = existing_data + new_data\n",
    "        save_data(merged_data, filepath)\n",
    "        print(f\"\\nğŸ’¾ ë³‘í•© ì™„ë£Œ: {len(existing_data)} + {len(new_data)} = {len(merged_data)}ê°œ\")\n",
    "    else:\n",
    "        save_data(new_data, filepath)\n",
    "        print(f\"\\nğŸ’¾ ìƒˆ íŒŒì¼ ìƒì„±: {len(new_data)}ê°œ\")\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "\n",
    "def update_changed_pages(\n",
    "    collector: NotionDataSourceCollector,\n",
    "    existing_data: List[Dict],\n",
    "    filepath: str = \"notion_data.json\"\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    ìˆ˜ì •ëœ í˜ì´ì§€ ì—…ë°ì´íŠ¸\n",
    "    \n",
    "    Args:\n",
    "        collector: NotionDataSourceCollector ì¸ìŠ¤í„´ìŠ¤\n",
    "        existing_data: ê¸°ì¡´ ë°ì´í„°\n",
    "        filepath: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ\n",
    "    \n",
    "    Returns:\n",
    "        ì—…ë°ì´íŠ¸ëœ ì „ì²´ ë°ì´í„°\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ”„ ìˆ˜ì •ëœ í˜ì´ì§€ í™•ì¸ ì¤‘...\")\n",
    "    \n",
    "    all_pages = collector.get_all_pages_from_datasource()\n",
    "    page_map = {p[\"id\"]: p for p in all_pages}\n",
    "    \n",
    "    updated_data = []\n",
    "    update_count = 0\n",
    "    \n",
    "    for old_item in existing_data:\n",
    "        page_id = old_item[\"page_id\"]\n",
    "        \n",
    "        if page_id not in page_map:\n",
    "            # Notionì—ì„œ ì‚­ì œëœ í˜ì´ì§€\n",
    "            print(f\"  âš ï¸ ì‚­ì œëœ í˜ì´ì§€: {old_item['title']}\")\n",
    "            continue\n",
    "        \n",
    "        new_page = page_map[page_id]\n",
    "        old_time = old_item.get(\"last_edited_time\", \"\")\n",
    "        new_time = new_page.get(\"last_edited_time\", \"\")\n",
    "        \n",
    "        # ìˆ˜ì • ì‹œê°„ ë¹„êµ\n",
    "        if new_time > old_time:\n",
    "            print(f\"  ğŸ”„ ì—…ë°ì´íŠ¸: {old_item['title']}\")\n",
    "            \n",
    "            properties = collector.extract_page_properties(new_page)\n",
    "            title = collector.get_page_title(properties)\n",
    "            \n",
    "            try:\n",
    "                blocks = collector.get_all_blocks(page_id)\n",
    "                content_lines = [collector.extract_block_content(b, page_id) for b in blocks]\n",
    "                full_content = \"\\n\".join(filter(None, content_lines))\n",
    "                \n",
    "                updated_data.append({\n",
    "                    \"page_id\": page_id,\n",
    "                    \"title\": title,\n",
    "                    \"created_time\": new_page.get(\"created_time\", \"\"),\n",
    "                    \"last_edited_time\": new_time,\n",
    "                    \"properties\": properties,\n",
    "                    \"content\": full_content\n",
    "                })\n",
    "                update_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"    âš ï¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "                updated_data.append(old_item)\n",
    "        else:\n",
    "            updated_data.append(old_item)\n",
    "    \n",
    "    if update_count > 0:\n",
    "        save_data(updated_data, filepath)\n",
    "        print(f\"\\nâœ… {update_count}ê°œ í˜ì´ì§€ ì—…ë°ì´íŠ¸ ì™„ë£Œ\")\n",
    "    else:\n",
    "        print(\"\\nâœ… ìˆ˜ì •ëœ í˜ì´ì§€ ì—†ìŒ\")\n",
    "    \n",
    "    return updated_data\n",
    "\n",
    "\n",
    "# ========== ì „ì²´ íŒŒì´í”„ë¼ì¸ ==========\n",
    "def smart_data_collection(\n",
    "    force_recreate: bool = False,\n",
    "    check_updates: bool = True,\n",
    "    filepath: str = \"notion_data.json\"\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    ìŠ¤ë§ˆíŠ¸ ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸\n",
    "    \n",
    "    Args:\n",
    "        force_recreate: Trueë©´ ì „ì²´ ì¬ìˆ˜ì§‘\n",
    "        check_updates: Trueë©´ ìˆ˜ì •ëœ í˜ì´ì§€ë„ í™•ì¸\n",
    "        filepath: ë°ì´í„° íŒŒì¼ ê²½ë¡œ\n",
    "    \n",
    "    Returns:\n",
    "        ìµœì¢… ë°ì´í„° ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ¤– Notion ìŠ¤ë§ˆíŠ¸ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    collector = NotionDataSourceCollector(NOTION_TOKEN, DATA_SOURCE_ID)\n",
    "    \n",
    "    if force_recreate:\n",
    "        print(\"\\nâ™»ï¸ ì „ì²´ ì¬ìˆ˜ì§‘ ëª¨ë“œ\")\n",
    "        if os.path.exists(filepath):\n",
    "            backup = f\"{filepath}.backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "            os.rename(filepath, backup)\n",
    "            print(f\"   ë°±ì—…: {backup}\")\n",
    "        \n",
    "        all_data = collector.collect_all()\n",
    "        save_data(all_data, filepath)\n",
    "        return all_data\n",
    "    \n",
    "    # ê¸°ì¡´ ë°ì´í„° í™•ì¸\n",
    "    existing_info = check_existing_data(filepath)\n",
    "    \n",
    "    if not existing_info[\"exists\"]:\n",
    "        print(\"\\nğŸ“¥ ì´ˆê¸° ìˆ˜ì§‘ ì‹œì‘...\")\n",
    "        all_data = collector.collect_all()\n",
    "        save_data(all_data, filepath)\n",
    "        return all_data\n",
    "    \n",
    "    # ìƒˆ í˜ì´ì§€ ìˆ˜ì§‘\n",
    "    new_data = collect_missing_pages(collector, existing_info[\"page_ids\"], filepath)\n",
    "    \n",
    "    # ìˆ˜ì •ëœ í˜ì´ì§€ ì—…ë°ì´íŠ¸\n",
    "    if check_updates and new_data:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        all_data = load_data(filepath)\n",
    "        all_data = update_changed_pages(collector, all_data, filepath)\n",
    "    else:\n",
    "        all_data = load_data(filepath)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"ğŸ‰ ì™„ë£Œ! ì´ {len(all_data)}ê°œ í˜ì´ì§€\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "\n",
    "def smart_indexing(\n",
    "    openai_client: OpenAI,\n",
    "    qdrant_client: QdrantClient,\n",
    "    image_client: OpenAI = None,\n",
    "    force_recreate: bool = False,\n",
    "    data_filepath: str = \"notion_data.json\"\n",
    "):\n",
    "    \"\"\"\n",
    "    ìŠ¤ë§ˆíŠ¸ ì¸ë±ì‹± íŒŒì´í”„ë¼ì¸\n",
    "    \n",
    "    Args:\n",
    "        openai_client: ì„ë² ë”©ìš© OpenAI í´ë¼ì´ì–¸íŠ¸\n",
    "        qdrant_client: Qdrant í´ë¼ì´ì–¸íŠ¸\n",
    "        image_client: ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±ìš© í´ë¼ì´ì–¸íŠ¸\n",
    "        force_recreate: Trueë©´ Qdrant ì»¬ë ‰ì…˜ ì¬ìƒì„±\n",
    "        data_filepath: ë°ì´í„° íŒŒì¼ ê²½ë¡œ\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ”„ Qdrant ì¸ë±ì‹± ì‹œì‘\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Qdrant ìƒíƒœ í™•ì¸\n",
    "    qdrant_info = check_qdrant_data(qdrant_client)\n",
    "    data_info = check_existing_data(data_filepath)\n",
    "    \n",
    "    if not data_info[\"exists\"]:\n",
    "        print(\"âŒ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”.\")\n",
    "        return\n",
    "    \n",
    "    # ìƒˆë¡œ ì¸ë±ì‹±í•  í˜ì´ì§€ í™•ì¸\n",
    "    if not force_recreate and qdrant_info[\"exists\"]:\n",
    "        new_page_ids = data_info[\"page_ids\"] - qdrant_info[\"page_ids\"]\n",
    "        if not new_page_ids:\n",
    "            print(\"âœ… ëª¨ë“  ë°ì´í„°ê°€ ì´ë¯¸ ì¸ë±ì‹±ë˜ì–´ ìˆìŠµë‹ˆë‹¤!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nğŸ“Š {len(new_page_ids)}ê°œ ìƒˆ í˜ì´ì§€ë¥¼ ì¸ë±ì‹±í•©ë‹ˆë‹¤.\")\n",
    "        \n",
    "        # ìƒˆ í˜ì´ì§€ë§Œ ë¡œë“œ\n",
    "        all_data = load_data(data_filepath)\n",
    "        new_data = [d for d in all_data if d[\"page_id\"] in new_page_ids]\n",
    "    else:\n",
    "        init_qdrant(qdrant_client, recreate=force_recreate)\n",
    "        new_data = load_data(data_filepath)\n",
    "    \n",
    "    # ì²­í‚¹ ë° ì„ë² ë”©\n",
    "    print(f\"\\nğŸ“ {len(new_data)}ê°œ í˜ì´ì§€ ì²˜ë¦¬ ì¤‘...\")\n",
    "    all_chunks = []\n",
    "    for page_data in new_data:\n",
    "        chunks = process_page_data(page_data, openai_client, image_client)\n",
    "        all_chunks.extend(chunks)\n",
    "    \n",
    "    print(f\"âœ… ì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„±\")\n",
    "    \n",
    "    if all_chunks:\n",
    "        embeddings = generate_embeddings(all_chunks, openai_client)\n",
    "        store_to_qdrant(all_chunks, embeddings, qdrant_client)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ‰ ì¸ë±ì‹± ì™„ë£Œ!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# ========== Notion ë°ì´í„° ìˆ˜ì§‘ (Data Source ì „ìš©) ==========\n",
    "class NotionDataSourceCollector:\n",
    "    \"\"\"Notion Data Sourceì—ì„œë§Œ ë°ì´í„° ìˆ˜ì§‘\"\"\"\n",
    "    \n",
    "    def __init__(self, token: str, data_source_id: str):\n",
    "        if not data_source_id:\n",
    "            raise ValueError(\"DATA_SOURCE_IDê°€ í•„ìš”í•©ë‹ˆë‹¤!\")\n",
    "        self.client = Client(auth=token, notion_version=os.environ.get(\"NOTION_VERSION\", \"2025-09-03\"))\n",
    "        self.data_source_id = data_source_id\n",
    "        os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "    \n",
    "    def get_all_blocks(self, block_id: str) -> List[Dict]:\n",
    "        \"\"\"ëª¨ë“  ë¸”ë¡ì„ ì¬ê·€ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸° (í˜ì´ì§€ë„¤ì´ì…˜ í¬í•¨)\"\"\"\n",
    "        all_blocks = []\n",
    "        cursor = None\n",
    "        \n",
    "        while True:\n",
    "            response = self.client.blocks.children.list(\n",
    "                block_id=block_id,\n",
    "                start_cursor=cursor\n",
    "            )\n",
    "            all_blocks.extend(response[\"results\"])\n",
    "            \n",
    "            if not response.get(\"has_more\"):\n",
    "                break\n",
    "            cursor = response[\"next_cursor\"]\n",
    "        \n",
    "        # í•˜ìœ„ ë¸”ë¡ ì¬ê·€ íƒìƒ‰\n",
    "        for block in all_blocks:\n",
    "            if block.get(\"has_children\"):\n",
    "                block[\"children\"] = self.get_all_blocks(block[\"id\"])\n",
    "        \n",
    "        return all_blocks\n",
    "    \n",
    "    def extract_rich_text(self, rich_text_list: List) -> str:\n",
    "        \"\"\"rich_text ë°°ì—´ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "        if not rich_text_list:\n",
    "            return \"\"\n",
    "        return \"\".join([t.get(\"plain_text\", \"\") for t in rich_text_list])\n",
    "    \n",
    "    def download_image(self, url: str, page_id: str, block_id: str) -> Optional[str]:\n",
    "        \"\"\"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ í›„ ë¡œì»¬ ê²½ë¡œ ë°˜í™˜\"\"\"\n",
    "        try:\n",
    "            response = httpx.get(url, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                ext = \"png\"\n",
    "                if \".\" in url.split(\"?\")[0]:\n",
    "                    ext = url.split(\"?\")[0].split(\".\")[-1][:4]\n",
    "                \n",
    "                filename = f\"{IMAGE_DIR}/{page_id}_{block_id}.{ext}\"\n",
    "                with open(filename, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                return filename\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "    \n",
    "    def extract_block_content(self, block: Dict, page_id: str, depth: int = 0) -> str:\n",
    "        \"\"\"ë¸”ë¡ì—ì„œ ëª¨ë“  ë‚´ìš© ì¶”ì¶œ\"\"\"\n",
    "        block_type = block[\"type\"]\n",
    "        block_id = block[\"id\"]\n",
    "        indent = \"  \" * depth\n",
    "        result = \"\"\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ ë¸”ë¡ë“¤\n",
    "        text_types = [\n",
    "            \"paragraph\", \"heading_1\", \"heading_2\", \"heading_3\",\n",
    "            \"bulleted_list_item\", \"numbered_list_item\", \n",
    "            \"quote\", \"toggle\", \"to_do\", \"callout\"\n",
    "        ]\n",
    "        \n",
    "        if block_type in text_types:\n",
    "            text = self.extract_rich_text(block[block_type].get(\"rich_text\", []))\n",
    "            \n",
    "            if block_type == \"heading_1\":\n",
    "                result = f\"{indent}# {text}\"\n",
    "            elif block_type == \"heading_2\":\n",
    "                result = f\"{indent}## {text}\"\n",
    "            elif block_type == \"heading_3\":\n",
    "                result = f\"{indent}### {text}\"\n",
    "            elif block_type == \"bulleted_list_item\":\n",
    "                result = f\"{indent}â€¢ {text}\"\n",
    "            elif block_type == \"numbered_list_item\":\n",
    "                result = f\"{indent}1. {text}\"\n",
    "            elif block_type == \"quote\":\n",
    "                result = f\"{indent}> {text}\"\n",
    "            elif block_type == \"to_do\":\n",
    "                checked = \"âœ…\" if block[\"to_do\"].get(\"checked\") else \"â¬œ\"\n",
    "                result = f\"{indent}{checked} {text}\"\n",
    "            elif block_type == \"callout\":\n",
    "                emoji = block[\"callout\"].get(\"icon\", {}).get(\"emoji\", \"ğŸ’¡\")\n",
    "                result = f\"{indent}{emoji} {text}\"\n",
    "            else:\n",
    "                result = f\"{indent}{text}\"\n",
    "        \n",
    "        # ì½”ë“œ ë¸”ë¡\n",
    "        elif block_type == \"code\":\n",
    "            code = self.extract_rich_text(block[\"code\"].get(\"rich_text\", []))\n",
    "            lang = block[\"code\"].get(\"language\", \"\")\n",
    "            caption = self.extract_rich_text(block[\"code\"].get(\"caption\", []))\n",
    "            result = f\"{indent}```{lang}\\n{code}\\n{indent}```\"\n",
    "            if caption:\n",
    "                result += f\"\\n{indent}Caption: {caption}\"\n",
    "        \n",
    "        # ì´ë¯¸ì§€\n",
    "        elif block_type == \"image\":\n",
    "            image_data = block[\"image\"]\n",
    "            url = None\n",
    "            if image_data[\"type\"] == \"file\":\n",
    "                url = image_data[\"file\"][\"url\"]\n",
    "            elif image_data[\"type\"] == \"external\":\n",
    "                url = image_data[\"external\"][\"url\"]\n",
    "            \n",
    "            if url:\n",
    "                local_path = self.download_image(url, page_id, block_id)\n",
    "                caption = self.extract_rich_text(image_data.get(\"caption\", []))\n",
    "                result = f\"{indent}[Image: {local_path or url}]\"\n",
    "                if caption:\n",
    "                    result += f\" - {caption}\"\n",
    "        \n",
    "        # ë¹„ë””ì˜¤\n",
    "        elif block_type == \"video\":\n",
    "            video_data = block[\"video\"]\n",
    "            url = video_data.get(\"file\", {}).get(\"url\") or video_data.get(\"external\", {}).get(\"url\", \"unknown\")\n",
    "            result = f\"{indent}[Video: {url}]\"\n",
    "        \n",
    "        # íŒŒì¼\n",
    "        elif block_type == \"file\":\n",
    "            file_data = block[\"file\"]\n",
    "            url = file_data.get(\"file\", {}).get(\"url\") or file_data.get(\"external\", {}).get(\"url\", \"unknown\")\n",
    "            name = file_data.get(\"name\", \"file\")\n",
    "            result = f\"{indent}[File: {name} - {url}]\"\n",
    "        \n",
    "        # ë¶ë§ˆí¬\n",
    "        elif block_type == \"bookmark\":\n",
    "            url = block[\"bookmark\"].get(\"url\", \"\")\n",
    "            caption = self.extract_rich_text(block[\"bookmark\"].get(\"caption\", []))\n",
    "            result = f\"{indent}[Bookmark: {url}]\"\n",
    "            if caption:\n",
    "                result += f\" - {caption}\"\n",
    "        \n",
    "        # ì„ë² ë“œ\n",
    "        elif block_type == \"embed\":\n",
    "            url = block[\"embed\"].get(\"url\", \"\")\n",
    "            result = f\"{indent}[Embed: {url}]\"\n",
    "        \n",
    "        # í…Œì´ë¸” í–‰\n",
    "        elif block_type == \"table_row\":\n",
    "            cells = block[\"table_row\"].get(\"cells\", [])\n",
    "            row_data = [self.extract_rich_text(cell) for cell in cells]\n",
    "            result = f\"{indent}| \" + \" | \".join(row_data) + \" |\"\n",
    "        \n",
    "        # êµ¬ë¶„ì„ \n",
    "        elif block_type == \"divider\":\n",
    "            result = f\"{indent}---\"\n",
    "        \n",
    "        # ë§í¬ í”„ë¦¬ë·°\n",
    "        elif block_type == \"link_preview\":\n",
    "            url = block[\"link_preview\"].get(\"url\", \"\")\n",
    "            result = f\"{indent}[Link: {url}]\"\n",
    "        \n",
    "        # PDF\n",
    "        elif block_type == \"pdf\":\n",
    "            pdf_data = block[\"pdf\"]\n",
    "            url = pdf_data.get(\"file\", {}).get(\"url\") or pdf_data.get(\"external\", {}).get(\"url\", \"unknown\")\n",
    "            result = f\"{indent}[PDF: {url}]\"\n",
    "        \n",
    "        # ìˆ˜ì‹\n",
    "        elif block_type == \"equation\":\n",
    "            expr = block[\"equation\"].get(\"expression\", \"\")\n",
    "            result = f\"{indent}[Equation: {expr}]\"\n",
    "        \n",
    "        # ìì‹ í˜ì´ì§€\n",
    "        elif block_type == \"child_page\":\n",
    "            title = block[\"child_page\"].get(\"title\", \"\")\n",
    "            result = f\"{indent}ğŸ“„ [{title}]\"\n",
    "        \n",
    "        # ìì‹ ë°ì´í„°ë² ì´ìŠ¤\n",
    "        elif block_type == \"child_database\":\n",
    "            title = block[\"child_database\"].get(\"title\", \"\")\n",
    "            result = f\"{indent}ğŸ—ƒï¸ [{title}]\"\n",
    "        \n",
    "        # ê¸°íƒ€ (table, column_list ë“±ì€ ë¬´ì‹œ)\n",
    "        elif block_type not in [\"table\", \"column_list\", \"column\", \"synced_block\"]:\n",
    "            result = f\"{indent}[{block_type}]\"\n",
    "        \n",
    "        # í•˜ìœ„ ë¸”ë¡ ì²˜ë¦¬\n",
    "        if \"children\" in block:\n",
    "            for child in block[\"children\"]:\n",
    "                child_content = self.extract_block_content(child, page_id, depth + 1)\n",
    "                if child_content:\n",
    "                    result += \"\\n\" + child_content\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def extract_page_properties(self, page: Dict) -> Dict:\n",
    "        \"\"\"í˜ì´ì§€ ì†ì„±(ë©”íƒ€ë°ì´í„°) ì¶”ì¶œ\"\"\"\n",
    "        props = page.get(\"properties\", {})\n",
    "        extracted = {}\n",
    "        \n",
    "        for name, prop in props.items():\n",
    "            prop_type = prop[\"type\"]\n",
    "            try:\n",
    "                if prop_type == \"title\":\n",
    "                    extracted[name] = self.extract_rich_text(prop[\"title\"])\n",
    "                elif prop_type == \"rich_text\":\n",
    "                    extracted[name] = self.extract_rich_text(prop[\"rich_text\"])\n",
    "                elif prop_type == \"number\":\n",
    "                    extracted[name] = prop[\"number\"]\n",
    "                elif prop_type == \"select\":\n",
    "                    extracted[name] = prop[\"select\"][\"name\"] if prop[\"select\"] else None\n",
    "                elif prop_type == \"multi_select\":\n",
    "                    extracted[name] = [s[\"name\"] for s in prop[\"multi_select\"]]\n",
    "                elif prop_type == \"date\":\n",
    "                    if prop[\"date\"]:\n",
    "                        extracted[name] = {\"start\": prop[\"date\"].get(\"start\"), \"end\": prop[\"date\"].get(\"end\")}\n",
    "                elif prop_type == \"checkbox\":\n",
    "                    extracted[name] = prop[\"checkbox\"]\n",
    "                elif prop_type == \"url\":\n",
    "                    extracted[name] = prop[\"url\"]\n",
    "                elif prop_type == \"email\":\n",
    "                    extracted[name] = prop[\"email\"]\n",
    "                elif prop_type == \"phone_number\":\n",
    "                    extracted[name] = prop[\"phone_number\"]\n",
    "                elif prop_type == \"created_time\":\n",
    "                    extracted[name] = prop[\"created_time\"]\n",
    "                elif prop_type == \"last_edited_time\":\n",
    "                    extracted[name] = prop[\"last_edited_time\"]\n",
    "                elif prop_type == \"created_by\":\n",
    "                    extracted[name] = prop[\"created_by\"].get(\"name\", prop[\"created_by\"].get(\"id\"))\n",
    "                elif prop_type == \"last_edited_by\":\n",
    "                    extracted[name] = prop[\"last_edited_by\"].get(\"name\", prop[\"last_edited_by\"].get(\"id\"))\n",
    "                elif prop_type == \"files\":\n",
    "                    extracted[name] = []\n",
    "                    for f in prop[\"files\"]:\n",
    "                        if f[\"type\"] == \"file\":\n",
    "                            extracted[name].append(f[\"file\"][\"url\"])\n",
    "                        elif f[\"type\"] == \"external\":\n",
    "                            extracted[name].append(f[\"external\"][\"url\"])\n",
    "                elif prop_type == \"relation\":\n",
    "                    extracted[name] = [r[\"id\"] for r in prop[\"relation\"]]\n",
    "                elif prop_type == \"formula\":\n",
    "                    formula = prop[\"formula\"]\n",
    "                    extracted[name] = formula.get(formula[\"type\"])\n",
    "                elif prop_type == \"rollup\":\n",
    "                    rollup = prop[\"rollup\"]\n",
    "                    extracted[name] = rollup.get(rollup[\"type\"])\n",
    "                elif prop_type == \"status\":\n",
    "                    extracted[name] = prop[\"status\"][\"name\"] if prop[\"status\"] else None\n",
    "                else:\n",
    "                    extracted[name] = f\"[{prop_type}]\"\n",
    "            except Exception:\n",
    "                extracted[name] = None\n",
    "        \n",
    "        return extracted\n",
    "    \n",
    "    def get_page_title(self, properties: Dict) -> str:\n",
    "        \"\"\"ì†ì„±ì—ì„œ ì œëª© ì¶”ì¶œ\"\"\"\n",
    "        for name, value in properties.items():\n",
    "            if isinstance(value, str) and value and name.lower() in [\"title\", \"name\", \"ì´ë¦„\", \"ì œëª©\"]:\n",
    "                return value\n",
    "        for name, value in properties.items():\n",
    "            if isinstance(value, str) and value:\n",
    "                return value\n",
    "        return \"Untitled\"\n",
    "    \n",
    "    def get_all_pages_from_datasource(self) -> List[Dict]:\n",
    "        \"\"\"Data Source APIë¡œ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° (í˜ì´ì§€ë„¤ì´ì…˜ í¬í•¨)\"\"\"\n",
    "        all_pages = []\n",
    "        cursor = None\n",
    "        \n",
    "        while True:\n",
    "            response = self.client.data_sources.query(\n",
    "                data_source_id=self.data_source_id,\n",
    "                start_cursor=cursor,\n",
    "                page_size=100\n",
    "            )\n",
    "            all_pages.extend(response[\"results\"])\n",
    "            print(f\"  ğŸ“„ {len(all_pages)}ê°œ í˜ì´ì§€ ë¡œë“œë¨...\")\n",
    "            \n",
    "            if not response.get(\"has_more\"):\n",
    "                break\n",
    "            cursor = response[\"next_cursor\"]\n",
    "        \n",
    "        return all_pages\n",
    "    \n",
    "    def collect_all(self, limit: int = None) -> List[Dict]:\n",
    "        \"\"\"Data Sourceì˜ ëª¨ë“  Notion ë°ì´í„° ìˆ˜ì§‘\"\"\"\n",
    "        print(\"ğŸš€ Notion ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘\")\n",
    "        print(f\"ğŸ“Š Data Source ID: {self.data_source_id}\\n\")\n",
    "        \n",
    "        pages = self.get_all_pages_from_datasource()\n",
    "        print(f\"\\nâœ… ì´ {len(pages)}ê°œ í˜ì´ì§€ ë°œê²¬\\n\")\n",
    "\n",
    "        if limit:\n",
    "            pages = pages[:limit]   # âœ… ì—¬ê¸°ì„œ ì œí•œ     \n",
    "\n",
    "        all_data = []\n",
    "        for idx, page in enumerate(pages):\n",
    "            page_id = page[\"id\"]\n",
    "            properties = self.extract_page_properties(page)\n",
    "            title = self.get_page_title(properties)\n",
    "            \n",
    "            print(f\"[{idx+1}/{len(pages)}] ğŸ“„ {title}\")\n",
    "            \n",
    "            try:\n",
    "                blocks = self.get_all_blocks(page_id)\n",
    "                content_lines = [self.extract_block_content(b, page_id) for b in blocks]\n",
    "                full_content = \"\\n\".join(filter(None, content_lines))\n",
    "                \n",
    "                all_data.append({\n",
    "                    \"page_id\": page_id,\n",
    "                    \"title\": title,\n",
    "                    \"created_time\": page.get(\"created_time\", \"\"),\n",
    "                    \"last_edited_time\": page.get(\"last_edited_time\", \"\"),\n",
    "                    \"properties\": properties,\n",
    "                    \"content\": full_content\n",
    "                })\n",
    "                print(f\"  â†’ {len(blocks)}ê°œ ë¸”ë¡, {len(full_content)}ì\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âš ï¸ ì‹¤íŒ¨: {e}\")\n",
    "                all_data.append({\n",
    "                    \"page_id\": page_id, \n",
    "                    \"title\": title, \n",
    "                    \"content\": \"\",\n",
    "                    \"created_time\": page.get(\"created_time\", \"\"),\n",
    "                    \"last_edited_time\": page.get(\"last_edited_time\", \"\"),\n",
    "                    \"properties\": properties\n",
    "                })\n",
    "        \n",
    "        print(f\"\\nğŸ‰ ì™„ë£Œ! ì´ {len(all_data)}ê°œ í˜ì´ì§€ ìˆ˜ì§‘\")\n",
    "        return all_data\n",
    "\n",
    "\n",
    "# ========== ì²­í‚¹ í•¨ìˆ˜ë“¤ ==========\n",
    "def split_by_markdown_headers(content: str, page_id: str, page_title: str) -> List[Dict]:\n",
    "    \"\"\"ë§ˆí¬ë‹¤ìš´ í—¤ë” ê¸°ì¤€ìœ¼ë¡œ ì„¹ì…˜ ë¶„ë¦¬\"\"\"\n",
    "    header_pattern = r'^(#{1,3})\\s+(.+)$'\n",
    "    lines = content.split('\\n')\n",
    "    sections = []\n",
    "    current = {\"title\": page_title, \"level\": 0, \"path\": [page_title], \"content\": []}\n",
    "    \n",
    "    for line in lines:\n",
    "        match = re.match(header_pattern, line)\n",
    "        if match:\n",
    "            if current[\"content\"] or current[\"title\"]:\n",
    "                sections.append(current.copy())\n",
    "            \n",
    "            level = len(match.group(1))\n",
    "            title = match.group(2).strip()\n",
    "            path = [title] if level == 1 else [page_title, title]\n",
    "            current = {\"title\": title, \"level\": level, \"path\": path, \"content\": []}\n",
    "        else:\n",
    "            current[\"content\"].append(line)\n",
    "    \n",
    "    if current[\"content\"] or current[\"title\"]:\n",
    "        sections.append(current)\n",
    "    \n",
    "    result = []\n",
    "    for sec in sections:\n",
    "        content_text = '\\n'.join(sec[\"content\"]).strip()\n",
    "        if content_text or sec[\"level\"] > 0:\n",
    "            full_text = f\"{'#' * sec['level']} {sec['title']}\\n{content_text}\".strip() if sec[\"level\"] > 0 else content_text\n",
    "            result.append({\"title\": sec[\"title\"], \"level\": sec[\"level\"], \"path\": sec[\"path\"], \"full_text\": full_text})\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def estimate_tokens(text: str) -> int:\n",
    "    \"\"\"í† í° ìˆ˜ ì¶”ì • (ëŒ€ëµ 4ì = 1í† í°)\"\"\"\n",
    "    return len(text) // 4\n",
    "\n",
    "\n",
    "def recursive_split(text: str, chunk_size: int = CHUNK_SIZE) -> List[str]:\n",
    "    \"\"\"ì¬ê·€ì  í…ìŠ¤íŠ¸ ë¶„í• \"\"\"\n",
    "    if estimate_tokens(text) <= chunk_size:\n",
    "        return [text]\n",
    "    \n",
    "    for sep in [\"\\n## \", \"\\n### \", \"\\n\\n\", \"\\nâ€¢ \", \"\\n\", \". \", \" \"]:\n",
    "        if sep in text:\n",
    "            parts = text.split(sep)\n",
    "            chunks = []\n",
    "            current = \"\"\n",
    "            for part in parts:\n",
    "                test = current + sep + part if current else part\n",
    "                if estimate_tokens(test) <= chunk_size:\n",
    "                    current = test\n",
    "                else:\n",
    "                    if current:\n",
    "                        chunks.append(current)\n",
    "                    current = part\n",
    "            if current:\n",
    "                chunks.append(current)\n",
    "            return chunks\n",
    "    \n",
    "    char_limit = chunk_size * 4\n",
    "    return [text[i:i+char_limit] for i in range(0, len(text), char_limit)]\n",
    "\n",
    "\n",
    "def find_images_in_text(text: str) -> List[Dict]:\n",
    "    \"\"\"í…ìŠ¤íŠ¸ì—ì„œ ì´ë¯¸ì§€ ë§ˆì»¤ ì°¾ê¸° - ê°œì„  ë²„ì „\"\"\"\n",
    "    pattern = r'\\[Image:\\s*([^\\]]+)\\]'\n",
    "    images = []\n",
    "    for m in re.finditer(pattern, text):\n",
    "        path = m.group(1).strip()\n",
    "        # notion_imagesë¡œ ì‹œì‘í•˜ì§€ ì•Šìœ¼ë©´ ì¶”ê°€\n",
    "        if not path.startswith(\"notion_images/\"):\n",
    "            path = f\"notion_images/{path}\"\n",
    "        images.append({\n",
    "            \"path\": path,\n",
    "            \"start\": m.start(),\n",
    "            \"end\": m.end(),\n",
    "            \"marker\": m.group(0)\n",
    "        })\n",
    "    return images\n",
    "\n",
    "\n",
    "def encode_image_to_base64(image_path: str) -> Optional[str]:\n",
    "    \"\"\"ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©\"\"\"\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def generate_image_description(image_path: str, page_title: str, section_title: str, \n",
    "                                text_before: str, text_after: str, client: OpenAI) -> str:\n",
    "    \"\"\"ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±\"\"\"\n",
    "    if image_path.startswith(\"http\"):\n",
    "        try:\n",
    "            response = httpx.get(image_path, timeout=30)\n",
    "            base64_image = base64.b64encode(response.content).decode(\"utf-8\")\n",
    "        except Exception:\n",
    "            return \"[ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨]\"\n",
    "    else:\n",
    "        base64_image = encode_image_to_base64(image_path)\n",
    "    \n",
    "    if not base64_image:\n",
    "        return \"[ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨]\"\n",
    "    \n",
    "    media_type = \"image/png\" if \".png\" in image_path.lower() else \"image/jpeg\"\n",
    "    \n",
    "    prompt = f\"\"\"ì´ë¯¸ì§€ë¥¼ ê²€ìƒ‰ì— ìœ ìš©í•˜ê²Œ 3~5ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.\n",
    "ë§¥ë½: {page_title} > {section_title}\n",
    "ì•: {text_before[:100]}\n",
    "ë’¤: {text_after[:100]}\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=IMAGE_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:{media_type};base64,{base64_image}\"}}\n",
    "            ]}],\n",
    "            max_tokens=300\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception:\n",
    "        return \"[ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì‹¤íŒ¨]\"\n",
    "\n",
    "\n",
    "def process_page_data(page_data: Dict, openai_client: OpenAI, image_client: OpenAI = None) -> List[Chunk]:\n",
    "    \"\"\"í˜ì´ì§€ ë°ì´í„°ë¥¼ ì²­í¬ë¡œ ë³€í™˜\"\"\"\n",
    "    page_id = page_data[\"page_id\"]\n",
    "    content = page_data[\"content\"]\n",
    "    page_title = page_data.get(\"title\", \"Untitled\")\n",
    "    \n",
    "    if not content.strip():\n",
    "        return []\n",
    "    \n",
    "    sections = split_by_markdown_headers(content, page_id, page_title)\n",
    "    chunks = []\n",
    "    \n",
    "    for sec_idx, section in enumerate(sections):\n",
    "        split_texts = recursive_split(section[\"full_text\"]) if estimate_tokens(section[\"full_text\"]) > CHUNK_SIZE else [section[\"full_text\"]]\n",
    "        \n",
    "        for chunk_idx, chunk_text in enumerate(split_texts):\n",
    "            chunk_id = f\"{page_id}_{sec_idx}_{chunk_idx}\"\n",
    "            chunk_images = find_images_in_text(chunk_text)\n",
    "            image_paths = [img[\"path\"] for img in chunk_images]\n",
    "            image_descriptions = []\n",
    "            \n",
    "            if chunk_images and image_client:\n",
    "                for img in chunk_images:\n",
    "                    start, end = img[\"start\"], img[\"end\"]\n",
    "                    text_before = chunk_text[max(0, start-200):start]\n",
    "                    text_after = chunk_text[end:end+200]\n",
    "                    desc = generate_image_description(img[\"path\"], page_title, section[\"title\"], text_before, text_after, image_client)\n",
    "                    image_descriptions.append(desc)\n",
    "            \n",
    "            combined_text = chunk_text\n",
    "            for img, desc in zip(chunk_images, image_descriptions):\n",
    "                combined_text = combined_text.replace(img[\"marker\"], f\"[ì´ë¯¸ì§€: {desc}]\")\n",
    "            \n",
    "            chunks.append(Chunk(\n",
    "                chunk_id=chunk_id, page_id=page_id, text=chunk_text, combined_text=combined_text,\n",
    "                has_image=len(chunk_images) > 0, image_paths=image_paths, image_descriptions=image_descriptions,\n",
    "                page_title=page_title, section_title=section[\"title\"], section_path=section[\"path\"],\n",
    "                heading_level=section[\"level\"], chunk_index=chunk_idx,\n",
    "                created_time=page_data.get(\"created_time\", \"\"),\n",
    "                last_edited_time=page_data.get(\"last_edited_time\", \"\"),\n",
    "                properties=page_data.get(\"properties\", {})\n",
    "            ))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "# ========== ì„ë² ë”© & Qdrant ==========\n",
    "def generate_embeddings(chunks: List[Chunk], client: OpenAI) -> List[List[float]]:\n",
    "    \"\"\"ì²­í¬ë“¤ì˜ ì„ë² ë”© ìƒì„±\"\"\"\n",
    "    texts = [c.combined_text for c in chunks]\n",
    "    print(f\"\\nğŸ”¢ ì„ë² ë”© ìƒì„± ì¤‘... ({len(texts)}ê°œ)\")\n",
    "    \n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(texts), 100):\n",
    "        batch = texts[i:i+100]\n",
    "        response = client.embeddings.create(model=EMBEDDING_MODEL, input=batch)\n",
    "        all_embeddings.extend([d.embedding for d in response.data])\n",
    "        print(f\"  â†’ {min(i+100, len(texts))}/{len(texts)}\")\n",
    "    \n",
    "    print(f\"  â†’ ì™„ë£Œ (ì°¨ì›: {len(all_embeddings[0])})\")\n",
    "    return all_embeddings\n",
    "\n",
    "\n",
    "def init_qdrant(client: QdrantClient, recreate: bool = False):\n",
    "    \"\"\"Qdrant ì»¬ë ‰ì…˜ ì´ˆê¸°í™”\"\"\"\n",
    "    exists = any(c.name == QDRANT_COLLECTION for c in client.get_collections().collections)\n",
    "    \n",
    "    if exists and recreate:\n",
    "        client.delete_collection(QDRANT_COLLECTION)\n",
    "        exists = False\n",
    "    \n",
    "    if not exists:\n",
    "        client.create_collection(\n",
    "            collection_name=QDRANT_COLLECTION,\n",
    "            vectors_config=VectorParams(size=EMBEDDING_DIM, distance=Distance.COSINE)\n",
    "        )\n",
    "        for field, ftype in [(\"page_id\", PayloadSchemaType.KEYWORD), (\"page_title\", PayloadSchemaType.KEYWORD)]:\n",
    "            client.create_payload_index(collection_name=QDRANT_COLLECTION, field_name=field, field_schema=ftype)\n",
    "        print(f\"âœ… ì»¬ë ‰ì…˜ ìƒì„±: {QDRANT_COLLECTION}\")\n",
    "\n",
    "\n",
    "def store_to_qdrant(chunks: List[Chunk], embeddings: List[List[float]], client: QdrantClient):\n",
    "    \"\"\"Qdrantì— ì²­í¬ ì €ì¥ - ì´ë¯¸ì§€ ì •ë³´ í¬í•¨\"\"\"\n",
    "    points = []\n",
    "    for chunk, emb in zip(chunks, embeddings):\n",
    "        pid = hashlib.md5(chunk.chunk_id.encode()).hexdigest()[:32]\n",
    "        pid = f\"{pid[:8]}-{pid[8:12]}-{pid[12:16]}-{pid[16:20]}-{pid[20:]}\"\n",
    "        \n",
    "        props = {k: (v if isinstance(v, (str, int, float, bool, list)) else str(v)) \n",
    "                for k, v in chunk.properties.items()}\n",
    "        \n",
    "        points.append(PointStruct(id=pid, vector=emb, payload={\n",
    "            \"chunk_id\": chunk.chunk_id,\n",
    "            \"page_id\": chunk.page_id,\n",
    "            \"text\": chunk.text,\n",
    "            \"combined_text\": chunk.combined_text,\n",
    "            \"has_image\": chunk.has_image,\n",
    "            \"image_paths\": chunk.image_paths,  # âœ… ì´ë¯¸ì§€ ê²½ë¡œë“¤\n",
    "            \"image_descriptions\": chunk.image_descriptions,  # âœ… ì´ë¯¸ì§€ ì„¤ëª…ë“¤\n",
    "            \"page_title\": chunk.page_title,\n",
    "            \"section_title\": chunk.section_title,\n",
    "            \"section_path\": chunk.section_path,\n",
    "            \"properties\": props\n",
    "        }))\n",
    "    \n",
    "    for i in range(0, len(points), 100):\n",
    "        client.upsert(collection_name=QDRANT_COLLECTION, points=points[i:i+100])\n",
    "    \n",
    "    print(f\"âœ… {len(points)}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "\n",
    "# ========== ê²€ìƒ‰ í•¨ìˆ˜ ê°œì„  ==========\n",
    "def search_with_images(query: str, openai_client: OpenAI, qdrant_client: QdrantClient, limit: int = 10) -> List[Dict]:\n",
    "    \"\"\"ì´ë¯¸ì§€ ì •ë³´ë¥¼ í¬í•¨í•œ ë²¡í„° ê²€ìƒ‰\"\"\"\n",
    "    response = openai_client.embeddings.create(model=EMBEDDING_MODEL, input=[query])\n",
    "    results = qdrant_client.query_points(\n",
    "        collection_name=QDRANT_COLLECTION, \n",
    "        query=response.data[0].embedding, \n",
    "        limit=limit,\n",
    "        with_payload=True\n",
    "    )\n",
    "    \n",
    "    return [{\n",
    "        \"score\": h.score,\n",
    "        \"page_title\": h.payload[\"page_title\"],\n",
    "        \"section_title\": h.payload[\"section_title\"],\n",
    "        \"text\": h.payload[\"text\"],\n",
    "        \"combined_text\": h.payload.get(\"combined_text\", \"\"),\n",
    "        \"has_image\": h.payload.get(\"has_image\", False),\n",
    "        \"image_paths\": h.payload.get(\"image_paths\", []),\n",
    "        \"image_descriptions\": h.payload.get(\"image_descriptions\", [])\n",
    "    } for h in results.points]\n",
    "\n",
    "\n",
    "# ========== ë°ì´í„° ì €ì¥/ë¡œë“œ ==========\n",
    "def save_data(data: List[Dict], filepath: str = \"notion_data.json\"):\n",
    "    \"\"\"ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"ğŸ’¾ ì €ì¥: {filepath}\")\n",
    "\n",
    "\n",
    "def load_data(filepath: str = \"notion_data.json\") -> List[Dict]:\n",
    "    \"\"\"JSON íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ\"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86a0b89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Notion RAG Pipeline (ìŠ¤ë§ˆíŠ¸ ìˆ˜ì§‘ ëª¨ë“œ)\n",
      "============================================================\n",
      "âœ… ê¸°ì¡´ ë°ì´í„°: 10ê°œ í˜ì´ì§€\n",
      "   ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: 2025-12-14T08:40:00.000Z\n",
      "âœ… ê¸°ì¡´ ë°ì´í„° ë°œê²¬: 10ê°œ í˜ì´ì§€\n",
      "\n",
      "ğŸ” Notionì—ì„œ ì „ì²´ í˜ì´ì§€ ëª©ë¡ ê°€ì ¸ì˜¤ëŠ” ì¤‘...\n",
      "  ğŸ“„ 89ê°œ í˜ì´ì§€ ë¡œë“œë¨...\n",
      "\n",
      "ğŸ“Š ë¶„ì„ ê²°ê³¼:\n",
      "   ì „ì²´ í˜ì´ì§€: 89ê°œ\n",
      "   ê¸°ì¡´ í˜ì´ì§€: 10ê°œ\n",
      "   ìƒˆ í˜ì´ì§€: 79ê°œ\n",
      "\n",
      "ğŸš€ 79ê°œ ìƒˆ í˜ì´ì§€ ìˆ˜ì§‘ ì‹œì‘...\n",
      "\n",
      "[1/79] ğŸ“„ ë™ì˜ìƒ ì¶”ì²œ ê¸°ìˆ  ì¡°ì‚¬\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… ê¸°ì¡´ ë°ì´í„° ë°œê²¬: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexisting_info[\u001b[33m'\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mê°œ í˜ì´ì§€\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# ìƒˆ í˜ì´ì§€ ìˆ˜ì§‘\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m new_data = \u001b[43mcollect_missing_pages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexisting_info\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpage_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# ìˆ˜ì •ëœ í˜ì´ì§€ ì—…ë°ì´íŠ¸\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_updates:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 183\u001b[39m, in \u001b[36mcollect_missing_pages\u001b[39m\u001b[34m(collector, existing_page_ids, filepath)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(new_pages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] ğŸ“„ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     blocks = \u001b[43mcollector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_all_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m     content_lines = [collector.extract_block_content(b, page_id) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m blocks]\n\u001b[32m    185\u001b[39m     full_content = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, content_lines))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 446\u001b[39m, in \u001b[36mNotionDataSourceCollector.get_all_blocks\u001b[39m\u001b[34m(self, block_id)\u001b[39m\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m all_blocks:\n\u001b[32m    445\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m block.get(\u001b[33m\"\u001b[39m\u001b[33mhas_children\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m         block[\u001b[33m\"\u001b[39m\u001b[33mchildren\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_all_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m all_blocks\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 446\u001b[39m, in \u001b[36mNotionDataSourceCollector.get_all_blocks\u001b[39m\u001b[34m(self, block_id)\u001b[39m\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m all_blocks:\n\u001b[32m    445\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m block.get(\u001b[33m\"\u001b[39m\u001b[33mhas_children\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m         block[\u001b[33m\"\u001b[39m\u001b[33mchildren\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_all_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m all_blocks\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 433\u001b[39m, in \u001b[36mNotionDataSourceCollector.get_all_blocks\u001b[39m\u001b[34m(self, block_id)\u001b[39m\n\u001b[32m    430\u001b[39m cursor = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchildren\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlist\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblock_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_cursor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcursor\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m     all_blocks.extend(response[\u001b[33m\"\u001b[39m\u001b[33mresults\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response.get(\u001b[33m\"\u001b[39m\u001b[33mhas_more\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rag/Project/rag-report-generator/.venv/lib/python3.11/site-packages/notion_client/api_endpoints.py:35\u001b[39m, in \u001b[36mBlocksChildrenEndpoint.list\u001b[39m\u001b[34m(self, block_id, **kwargs)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m, block_id: \u001b[38;5;28mstr\u001b[39m, **kwargs: Any) -> SyncAsync[Any]:\n\u001b[32m     31\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a paginated array of child [block objects](https://developers.notion.com/reference/block) contained in the block.\u001b[39;00m\n\u001b[32m     32\u001b[39m \n\u001b[32m     33\u001b[39m \u001b[33;03m    *[ğŸ”— Endpoint documentation](https://developers.notion.com/reference/get-block-children)*\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mblocks/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mblock_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/children\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpick\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstart_cursor\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpage_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rag/Project/rag-report-generator/.venv/lib/python3.11/site-packages/notion_client/client.py:236\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, path, method, query, body, form_data, auth)\u001b[39m\n\u001b[32m    234\u001b[39m request = \u001b[38;5;28mself\u001b[39m._build_request(method, path, query, body, form_data, auth)\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException:\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RequestTimeoutError()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rag/Project/rag-report-generator/.venv/lib/python3.11/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rag/Project/rag-report-generator/.venv/lib/python3.11/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rag/Project/rag-report-generator/.venv/lib/python3.11/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rag/Project/rag-report-generator/.venv/lib/python3.11/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rag/Project/rag-report-generator/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rag/Project/rag-report-generator/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rag/Project/rag-report-generator/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rag/Project/rag-report-generator/.venv/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rag/Project/rag-report-generator/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rag/Project/rag-report-generator/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rag/Project/rag-report-generator/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rag/Project/rag-report-generator/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/rag/Project/rag-report-generator/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.14-linux-x86_64-gnu/lib/python3.11/ssl.py:1295\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1291\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1292\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1293\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1294\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.14-linux-x86_64-gnu/lib/python3.11/ssl.py:1168\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1166\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1167\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "data_file = \"notion_data.json\"\n",
    "\n",
    "# ì„¤ì •\n",
    "force_recreate = False  # True: ì „ì²´ ì¬ìˆ˜ì§‘, False: ì¦ë¶„ ìˆ˜ì§‘\n",
    "check_updates = True    # ìˆ˜ì •ëœ í˜ì´ì§€ë„ í™•ì¸í• ì§€ ì—¬ë¶€\n",
    "skip_fetch = False      # True: ë°ì´í„° ìˆ˜ì§‘ ê±´ë„ˆë›°ê¸°\n",
    "\n",
    "print(\"ğŸš€ Notion RAG Pipeline (ìŠ¤ë§ˆíŠ¸ ìˆ˜ì§‘ ëª¨ë“œ)\\n\" + \"=\" * 60)\n",
    "\n",
    "# í™˜ê²½ë³€ìˆ˜ ì²´í¬\n",
    "if not skip_fetch:\n",
    "    if not NOTION_TOKEN:\n",
    "        print(\"âŒ NOTION_TOKEN í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤!\")\n",
    "        sys.exit(1)\n",
    "    if not DATA_SOURCE_ID:\n",
    "        print(\"âŒ DATA_SOURCE_ID í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤!\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if not OPENROUTER_API_KEY:\n",
    "    print(\"âŒ OPENROUTER_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤!\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# 1ï¸âƒ£ ë°ì´í„° ìˆ˜ì§‘ (ìŠ¤ë§ˆíŠ¸ ëª¨ë“œ)\n",
    "if skip_fetch:\n",
    "    print(\"ğŸ“‚ ì €ì¥ëœ ë°ì´í„° ë¡œë“œ...\")\n",
    "    all_data = load_data(data_file)\n",
    "else:\n",
    "    # ìŠ¤ë§ˆíŠ¸ ìˆ˜ì§‘: ìƒˆ í˜ì´ì§€ë§Œ ì¶”ê°€, ìˆ˜ì •ëœ í˜ì´ì§€ ì—…ë°ì´íŠ¸\n",
    "    collector = NotionDataSourceCollector(NOTION_TOKEN, DATA_SOURCE_ID)\n",
    "    \n",
    "    existing_info = check_existing_data(data_file)\n",
    "    \n",
    "    if existing_info[\"exists\"] and not force_recreate:\n",
    "        print(f\"âœ… ê¸°ì¡´ ë°ì´í„° ë°œê²¬: {existing_info['count']}ê°œ í˜ì´ì§€\")\n",
    "        \n",
    "        # ìƒˆ í˜ì´ì§€ ìˆ˜ì§‘\n",
    "        new_data = collect_missing_pages(collector, existing_info[\"page_ids\"], data_file)\n",
    "        \n",
    "        # ìˆ˜ì •ëœ í˜ì´ì§€ ì—…ë°ì´íŠ¸\n",
    "        if check_updates:\n",
    "            all_data = load_data(data_file)\n",
    "            all_data = update_changed_pages(collector, all_data, data_file)\n",
    "        else:\n",
    "            all_data = load_data(data_file)\n",
    "        \n",
    "        if not new_data:\n",
    "            print(\"\\nâœ¨ ëª¨ë“  ë°ì´í„°ê°€ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤!\")\n",
    "    else:\n",
    "        # ì´ˆê¸° ìˆ˜ì§‘ ë˜ëŠ” ì „ì²´ ì¬ìˆ˜ì§‘\n",
    "        print(\"ğŸ“¥ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...\")\n",
    "        all_data = collector.collect_all()  # limit ì œê±° (ì „ì²´ ìˆ˜ì§‘)\n",
    "        save_data(all_data, data_file)\n",
    "\n",
    "if not all_data:\n",
    "    print(\"âš ï¸ ë°ì´í„° ì—†ìŒ\")\n",
    "    sys.exit(0)\n",
    "\n",
    "# 2ï¸âƒ£ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "openai_client = OpenAI(\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    base_url=OPENROUTER_BASE_URL\n",
    ")\n",
    "\n",
    "image_client = (\n",
    "    OpenAI(api_key=AZURE_AI_CREDENTIAL, base_url=AZURE_AI_ENDPOINT)\n",
    "    if AZURE_AI_CREDENTIAL else None\n",
    ")\n",
    "\n",
    "qdrant_client = QdrantClient(path=\"./qdrant_data\")\n",
    "\n",
    "# 3ï¸âƒ£ ìŠ¤ë§ˆíŠ¸ ì¸ë±ì‹±\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ”„ Qdrant ì¸ë±ì‹±\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "qdrant_info = check_qdrant_data(qdrant_client)\n",
    "data_info = check_existing_data(data_file)\n",
    "\n",
    "# ìƒˆë¡œ ì¸ë±ì‹±í•  í˜ì´ì§€ í™•ì¸\n",
    "if not force_recreate and qdrant_info[\"exists\"]:\n",
    "    new_page_ids = data_info[\"page_ids\"] - qdrant_info[\"page_ids\"]\n",
    "    \n",
    "    if not new_page_ids:\n",
    "        print(\"âœ… ëª¨ë“  ë°ì´í„°ê°€ ì´ë¯¸ ì¸ë±ì‹±ë˜ì–´ ìˆìŠµë‹ˆë‹¤!\")\n",
    "        pages_to_index = []\n",
    "    else:\n",
    "        print(f\"\\nğŸ“Š {len(new_page_ids)}ê°œ ìƒˆ í˜ì´ì§€ë¥¼ ì¸ë±ì‹±í•©ë‹ˆë‹¤.\")\n",
    "        pages_to_index = [d for d in all_data if d[\"page_id\"] in new_page_ids]\n",
    "else:\n",
    "    # ì „ì²´ ì¬ì¸ë±ì‹±\n",
    "    print(\"\\nâ™»ï¸ ì „ì²´ ì¬ì¸ë±ì‹± ì‹œì‘...\")\n",
    "    init_qdrant(qdrant_client, recreate=True)\n",
    "    pages_to_index = all_data\n",
    "\n",
    "# 4ï¸âƒ£ ì²­í‚¹ ë° ì„ë² ë”© (í•„ìš”í•œ í˜ì´ì§€ë§Œ)\n",
    "if pages_to_index:\n",
    "    print(f\"\\nğŸ“ {len(pages_to_index)}ê°œ í˜ì´ì§€ ì²­í‚¹ ì¤‘...\")\n",
    "    all_chunks = []\n",
    "    \n",
    "    for page in pages_to_index:\n",
    "        chunks = process_page_data(page, openai_client, image_client)\n",
    "        all_chunks.extend(chunks)\n",
    "        print(f\"  {page.get('title', 'Untitled')}: {len(chunks)}ê°œ ì²­í¬\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„±\")\n",
    "    \n",
    "    if all_chunks:\n",
    "        embeddings = generate_embeddings(all_chunks, openai_client)\n",
    "        store_to_qdrant(all_chunks, embeddings, qdrant_client)\n",
    "    else:\n",
    "        print(\"âš ï¸ ìƒì„±ëœ ì²­í¬ ì—†ìŒ\")\n",
    "\n",
    "# 5ï¸âƒ£ í…ŒìŠ¤íŠ¸ ê²€ìƒ‰\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ§ª ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_queries = [\"í”„ë¡œì íŠ¸ ëª©í‘œ\", \"ë°ì´í„° ì²˜ë¦¬\"]\n",
    "\n",
    "for q in test_queries:\n",
    "    print(f\"\\nğŸ” ì¿¼ë¦¬: '{q}'\")\n",
    "    results = search_with_images(q, openai_client, qdrant_client, limit=3)\n",
    "    \n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"\\n  [{i}] {r['page_title']} > {r['section_title']}\")\n",
    "        print(f\"      ì ìˆ˜: {r['score']:.4f}\")\n",
    "        print(f\"      ì´ë¯¸ì§€: {len(r['image_paths'])}ê°œ\")\n",
    "        print(f\"      ë‚´ìš©: {r['text'][:100]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ‰ ì™„ë£Œ!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dbf125",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-report-generator (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
