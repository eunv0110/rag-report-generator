import os
import json
from typing import Dict, List, Set
from datetime import datetime
from pathlib import Path

def check_existing_data(filepath: str) -> Dict:
    """ê¸°ì¡´ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ ë° í†µê³„ í™•ì¸"""
    if not os.path.exists(filepath):
        print(f"âŒ íŒŒì¼ ì—†ìŒ: {filepath}")
        return {
            "exists": False,
            "count": 0,
            "page_ids": set(),
            "last_updated": None
        }
    
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        page_ids = {item["page_id"] for item in data}
        last_times = [item.get("last_edited_time", "") for item in data]
        last_updated = max(last_times) if last_times else None
        
        print(f"âœ… ê¸°ì¡´ ë°ì´í„°: {len(data)}ê°œ í˜ì´ì§€")
        print(f"   ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {last_updated}")
        
        return {
            "exists": True,
            "count": len(data),
            "page_ids": page_ids,
            "last_updated": last_updated
        }
    except Exception as e:
        print(f"âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {
            "exists": False,
            "count": 0,
            "page_ids": set(),
            "last_updated": None
        }


def collect_missing_pages(collector, existing_page_ids: Set[str], filepath: str, limit: int = None) -> List[Dict]:
    """ê¸°ì¡´ ë°ì´í„°ì— ì—†ëŠ” ìƒˆ í˜ì´ì§€ë§Œ ìˆ˜ì§‘"""
    print("\nğŸ” Notionì—ì„œ ì „ì²´ í˜ì´ì§€ ëª©ë¡ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
    all_pages = collector.get_all_pages_from_datasource()
    
    new_pages = [p for p in all_pages if p["id"] not in existing_page_ids]
    
    # âœ… limit ì ìš©
    if limit and len(new_pages) > limit:
        print(f"âš ï¸  {len(new_pages)}ê°œ ìƒˆ í˜ì´ì§€ ì¤‘ {limit}ê°œë§Œ ìˆ˜ì§‘")
        new_pages = new_pages[:limit]
    
    print(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼:")
    print(f"   ì „ì²´ í˜ì´ì§€: {len(all_pages)}ê°œ")
    print(f"   ê¸°ì¡´ í˜ì´ì§€: {len(existing_page_ids)}ê°œ")
    print(f"   ìƒˆ í˜ì´ì§€: {len(new_pages)}ê°œ")
    
    if not new_pages:
        print("\nâœ… ëª¨ë“  í˜ì´ì§€ê°€ ì´ë¯¸ ìˆ˜ì§‘ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
        return []
    
    print(f"\nğŸš€ {len(new_pages)}ê°œ ìƒˆ í˜ì´ì§€ ìˆ˜ì§‘ ì‹œì‘...\n")
    
    new_data = []
    for idx, page in enumerate(new_pages):
        page_id = page["id"]
        properties = collector.extract_page_properties(page)
        title = collector.get_page_title(properties)
        
        print(f"[{idx+1}/{len(new_pages)}] ğŸ“„ {title}")
        
        try:
            blocks = collector.get_all_blocks(page_id)
            content_lines = [collector.extract_block_content(b, page_id) for b in blocks]
            full_content = "\n".join(filter(None, content_lines))
            
            new_data.append({
                "page_id": page_id,
                "title": title,
                "created_time": page.get("created_time", ""),
                "last_edited_time": page.get("last_edited_time", ""),
                "properties": properties,
                "content": full_content
            })
            print(f"  â†’ {len(blocks)}ê°œ ë¸”ë¡, {len(full_content)}ì")
        except Exception as e:
            print(f"  âš ï¸ ì‹¤íŒ¨: {e}")
            new_data.append({
                "page_id": page_id,
                "title": title,
                "content": "",
                "created_time": page.get("created_time", ""),
                "last_edited_time": page.get("last_edited_time", ""),
                "properties": properties
            })
    
    # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©
    if os.path.exists(filepath):
        from utils.file_utils import load_json, save_json
        existing_data = load_json(filepath)
        merged_data = existing_data + new_data
        save_json(merged_data, filepath)
        print(f"\nğŸ’¾ ë³‘í•© ì™„ë£Œ: {len(existing_data)} + {len(new_data)} = {len(merged_data)}ê°œ")
    else:
        from utils.file_utils import save_json
        save_json(new_data, filepath)
        print(f"\nğŸ’¾ ìƒˆ íŒŒì¼ ìƒì„±: {len(new_data)}ê°œ")
    
    return new_data


def update_changed_pages(collector, existing_data: List[Dict], filepath: str) -> List[Dict]:
    """ìˆ˜ì •ëœ í˜ì´ì§€ ì—…ë°ì´íŠ¸"""
    print("\nğŸ”„ ìˆ˜ì •ëœ í˜ì´ì§€ í™•ì¸ ì¤‘...")
    
    all_pages = collector.get_all_pages_from_datasource()
    page_map = {p["id"]: p for p in all_pages}
    
    updated_data = []
    update_count = 0
    
    for old_item in existing_data:
        page_id = old_item["page_id"]
        
        if page_id not in page_map:
            print(f"  âš ï¸ ì‚­ì œëœ í˜ì´ì§€: {old_item['title']}")
            continue
        
        new_page = page_map[page_id]
        old_time = old_item.get("last_edited_time", "")
        new_time = new_page.get("last_edited_time", "")
        
        if new_time > old_time:
            print(f"  ğŸ”„ ì—…ë°ì´íŠ¸: {old_item['title']}")
            
            properties = collector.extract_page_properties(new_page)
            title = collector.get_page_title(properties)
            
            try:
                blocks = collector.get_all_blocks(page_id)
                content_lines = [collector.extract_block_content(b, page_id) for b in blocks]
                full_content = "\n".join(filter(None, content_lines))
                
                updated_data.append({
                    "page_id": page_id,
                    "title": title,
                    "created_time": new_page.get("created_time", ""),
                    "last_edited_time": new_time,
                    "properties": properties,
                    "content": full_content
                })
                update_count += 1
            except Exception as e:
                print(f"    âš ï¸ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
                updated_data.append(old_item)
        else:
            updated_data.append(old_item)
    
    if update_count > 0:
        from utils.file_utils import save_json
        save_json(updated_data, filepath)
        print(f"\nâœ… {update_count}ê°œ í˜ì´ì§€ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
    else:
        print("\nâœ… ìˆ˜ì •ëœ í˜ì´ì§€ ì—†ìŒ")
    
    return updated_data