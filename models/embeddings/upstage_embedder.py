from typing import List
import time
from openai import OpenAI
from langchain_core.embeddings import Embeddings

class UpstageEmbedder(Embeddings):
    """
    Upstage APIë¥¼ ì‚¬ìš©í•œ ì„ë² ë”© ìƒì„±ê¸°
    UpstageëŠ” OpenAI í˜¸í™˜ APIë¥¼ ì œê³µí•˜ë¯€ë¡œ OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """

    def __init__(
        self,
        model: str = "solar-embedding-1-large-passage",
        query_model: str = None,
        api_key: str = None,
        base_url: str = "https://api.upstage.ai/v1/solar",
        batch_size: int = 100,
        use_cache: bool = True,
        cache_dir: str = "data/evaluation/embedding_cache"
    ):
        """
        Args:
            model: Upstage ì„ë² ë”© ëª¨ë¸ëª… (ë¬¸ì„œìš©)
                - solar-embedding-1-large-passage: ë¬¸ì„œìš© (1024 dim)
            query_model: Upstage ì¿¼ë¦¬ ì„ë² ë”© ëª¨ë¸ëª… (ê²€ìƒ‰ ì‹œ ì‚¬ìš©)
                - solar-embedding-1-large-query: ì¿¼ë¦¬ìš© (1024 dim)
                - Noneì¸ ê²½ìš° modelê³¼ ë™ì¼í•œ ëª¨ë¸ ì‚¬ìš©
            api_key: Upstage API í‚¤
            base_url: Upstage API ì—”ë“œí¬ì¸íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
            cache_dir: ìºì‹œ ì €ì¥ ê²½ë¡œ
        """
        self.model = model  # ë¬¸ì„œ ì„ë² ë”©ìš©
        self.query_model = query_model if query_model else model  # ì¿¼ë¦¬ ì„ë² ë”©ìš©
        self.batch_size = batch_size
        self.client = OpenAI(api_key=api_key, base_url=base_url)

        print(f"ğŸ“„ ë¬¸ì„œ ì„ë² ë”© ëª¨ë¸: {self.model}")
        print(f"ğŸ” ì¿¼ë¦¬ ì„ë² ë”© ëª¨ë¸: {self.query_model}")

        # ìºì‹œ ì„¤ì •
        self.use_cache = use_cache
        self.cache = None
        if use_cache:
            try:
                from utils.embedding_cache import EmbeddingCache
                self.cache = EmbeddingCache(cache_dir=cache_dir)
                print(f"âœ… ì„ë² ë”© ìºì‹œ í™œì„±í™” (ìºì‹œ í•­ëª©: {len(self.cache.cache)}ê°œ)")
            except Exception as e:
                print(f"âš ï¸ ìºì‹œ ì´ˆê¸°í™” ì‹¤íŒ¨, ìºì‹œ ì—†ì´ ì§„í–‰: {e}")
                self.use_cache = False

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """
        ë°°ì¹˜ ì„ë² ë”© ìƒì„± (ë¬¸ì„œìš©)

        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        all_embeddings = []

        for i in range(0, len(texts), self.batch_size):
            batch = texts[i:i + self.batch_size]

            # ë¹ˆ í…ìŠ¤íŠ¸ í•„í„°ë§
            batch = [text.strip() if text else " " for text in batch]

            # ì¬ì‹œë„ ë¡œì§ (ìµœëŒ€ 3ë²ˆ)
            for attempt in range(3):
                try:
                    response = self.client.embeddings.create(
                        model=self.model,
                        input=batch
                    )

                    # ì‘ë‹µ ê²€ì¦
                    if not response.data:
                        raise ValueError("ë¹ˆ ì‘ë‹µ ë°›ìŒ")

                    embeddings = [d.embedding for d in response.data]
                    all_embeddings.extend(embeddings)
                    print(f"  â†’ {min(i + self.batch_size, len(texts))}/{len(texts)} ì„ë² ë”© ìƒì„±")

                    # API ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€
                    time.sleep(0.5)
                    break

                except Exception as e:
                    if attempt < 2:
                        wait_time = (attempt + 1) * 2
                        print(f"  âš ï¸ ì¬ì‹œë„ {attempt + 1}/3 (ëŒ€ê¸°: {wait_time}ì´ˆ): {e}")
                        time.sleep(wait_time)
                    else:
                        print(f"  âŒ ë°°ì¹˜ {i}~{i+len(batch)} ì‹¤íŒ¨: {e}")
                        # ê°œë³„ í…ìŠ¤íŠ¸ë¡œ ì¬ì‹œë„
                        print(f"  ğŸ”„ ê°œë³„ í…ìŠ¤íŠ¸ë¡œ ì¬ì‹œë„ ì¤‘...")
                        batch_embeddings = self._embed_one_by_one(batch)
                        all_embeddings.extend(batch_embeddings)
                        break

        return all_embeddings

    def _embed_one_by_one(self, texts: List[str]) -> List[List[float]]:
        """
        ê°œë³„ í…ìŠ¤íŠ¸ ì„ë² ë”© (í´ë°±)

        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        embeddings = []
        for text in texts:
            try:
                response = self.client.embeddings.create(
                    model=self.model,
                    input=[text.strip() if text else " "]
                )
                embeddings.append(response.data[0].embedding)
                time.sleep(0.3)
            except Exception as e:
                print(f"    âš ï¸ í…ìŠ¤íŠ¸ ì„ë² ë”© ì‹¤íŒ¨: {text[:50]}... - {e}")
                # ì‹¤íŒ¨í•œ í…ìŠ¤íŠ¸ëŠ” ì œë¡œ ë²¡í„°ë¡œ ëŒ€ì²´ (1024 dim)
                embeddings.append([0.0] * 1024)
        return embeddings

    def embed_query(self, text: str) -> List[float]:
        """
        ë‹¨ì¼ ì¿¼ë¦¬ ì„ë² ë”© (ìºì‹œ ì§€ì›)
        ì¿¼ë¦¬ ì „ìš© ëª¨ë¸(query_model)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

        Args:
            text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„°
        """
        # ìºì‹œì—ì„œ í™•ì¸ (ì¿¼ë¦¬ ëª¨ë¸ ê¸°ì¤€)
        if self.use_cache and self.cache:
            cached = self.cache.get(text, model=self.query_model)
            if cached is not None:
                return cached

        # ìºì‹œ ë¯¸ìŠ¤ - API í˜¸ì¶œ (ì¿¼ë¦¬ ëª¨ë¸ ì‚¬ìš©)
        try:
            response = self.client.embeddings.create(
                model=self.query_model,  # ì¿¼ë¦¬ ì „ìš© ëª¨ë¸ ì‚¬ìš©
                input=[text]
            )
            embedding = response.data[0].embedding

            # ìºì‹œì— ì €ì¥ (ì¿¼ë¦¬ ëª¨ë¸ ê¸°ì¤€)
            if self.use_cache and self.cache:
                self.cache.set(text, embedding, model=self.query_model)

            return embedding
        except Exception as e:
            print(f"âŒ ì¿¼ë¦¬ ì„ë² ë”© ì‹¤íŒ¨: {e}")
            raise

    def save_cache(self):
        """ìºì‹œë¥¼ íŒŒì¼ì— ì €ì¥"""
        if self.use_cache and self.cache:
            success = self.cache.save()
            if success:
                self.cache.print_stats()
            return success
        return False
