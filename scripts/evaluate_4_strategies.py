#!/usr/bin/env python3
"""4ê°€ì§€ Retrieval ì „ëµ ì„±ëŠ¥ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

í‰ê°€ ì „ëµ:
  1. RRF (Baseline)
  2. RRF + MultiQuery
  3. RRF + MultiQuery + LongContext
  4. RRF + LongContext + TimeWeighted

ì‚¬ìš©ë²•:
    python scripts/evaluate_4_strategies.py

    # íŠ¹ì • ì „ëµë§Œ í‰ê°€
    python scripts/evaluate_4_strategies.py --strategies 1 2 3

    # Top-K ì„¤ì •
    python scripts/evaluate_4_strategies.py --top-k 15
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

import subprocess
import argparse
from typing import List

# í‰ê°€ ì „ëµ ì •ì˜
STRATEGIES = {
    1: {
        "name": "RRF (Baseline)",
        "cmd": [
            "python", "scripts/evaluate_retriever.py",
            "--retriever", "ensemble_rrf"
        ]
    },
    2: {
        "name": "RRF + MultiQuery",
        "cmd": [
            "python", "scripts/evaluate_retriever.py",
            "--retriever", "multiquery",
            "--base-retriever", "ensemble_rrf",
            "--num-queries", "3"
        ]
    },
    3: {
        "name": "RRF + MultiQuery + LongContext",
        "cmd": [
            "python", "scripts/evaluate_retriever.py",
            "--retriever", "multiquery",
            "--base-retriever", "ensemble_rrf_longcontext",
            "--num-queries", "3"
        ]
    },
    4: {
        "name": "RRF + LongContext + TimeWeighted",
        "cmd": [
            "python", "scripts/evaluate_retriever.py",
            "--retriever", "ensemble_rrf_timeweighted_longcontext",
            "--decay-rate", "0.01"
        ]
    }
}


def run_evaluation(
    strategy_num: int,
    dataset: str,
    top_k: int,
    version: str
) -> bool:
    """ë‹¨ì¼ í‰ê°€ ì „ëµ ì‹¤í–‰

    Args:
        strategy_num: ì „ëµ ë²ˆí˜¸
        dataset: í‰ê°€ ë°ì´í„°ì…‹ ê²½ë¡œ
        top_k: Top-K ê°’
        version: ë²„ì „ íƒœê·¸

    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    strategy = STRATEGIES[strategy_num]

    print("\n" + "=" * 70)
    print(f"{strategy_num}ï¸âƒ£  {strategy['name']} í‰ê°€ ì¤‘...")
    print("=" * 70)

    # ëª…ë ¹ì–´ êµ¬ì„±
    cmd = strategy["cmd"] + [
        "--dataset", dataset,
        "--top-k", str(top_k),
        "--version", version
    ]

    try:
        # í‰ê°€ ì‹¤í–‰
        result = subprocess.run(cmd, check=True)
        print(f"âœ… {strategy['name']} í‰ê°€ ì™„ë£Œ")
        return True

    except subprocess.CalledProcessError as e:
        print(f"âŒ {strategy['name']} í‰ê°€ ì‹¤íŒ¨: {e}")
        return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="4ê°€ì§€ Retrieval ì „ëµ ì„±ëŠ¥ í‰ê°€",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )

    parser.add_argument(
        "--strategies",
        type=int,
        nargs="+",
        choices=[1, 2, 3, 4],
        default=[1, 2, 3, 4],
        help="í‰ê°€í•  ì „ëµ ë²ˆí˜¸ (ê¸°ë³¸ê°’: ëª¨ë‘)"
    )

    parser.add_argument(
        "--dataset",
        type=str,
        default="/home/work/rag/Project/rag-report-generator/data/evaluation/merged_qa_dataset.json",
        help="í‰ê°€ ë°ì´í„°ì…‹ ê²½ë¡œ"
    )

    parser.add_argument(
        "--top-k",
        type=int,
        default=10,
        help="Top-K ê°’ (ê¸°ë³¸ê°’: 10)"
    )

    parser.add_argument(
        "--version",
        type=str,
        default="v1",
        help="ë²„ì „ íƒœê·¸ (ê¸°ë³¸ê°’: v1)"
    )

    args = parser.parse_args()

    # ì‹œì‘ ë©”ì‹œì§€
    print("=" * 70)
    print("ğŸš€ 4ê°€ì§€ Retrieval ì „ëµ ì„±ëŠ¥ í‰ê°€ ì‹œì‘")
    print("=" * 70)
    print(f"\nğŸ“Š í‰ê°€ ì„¤ì •:")
    print(f"   - Dataset: {args.dataset}")
    print(f"   - Top-K: {args.top_k}")
    print(f"   - Version: {args.version}")
    print(f"   - í‰ê°€í•  ì „ëµ: {args.strategies}")

    # ê° ì „ëµ í‰ê°€
    results = {}
    for strategy_num in sorted(args.strategies):
        success = run_evaluation(
            strategy_num=strategy_num,
            dataset=args.dataset,
            top_k=args.top_k,
            version=args.version
        )
        results[strategy_num] = success

    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 70)
    print("ğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print("=" * 70)

    for strategy_num in sorted(args.strategies):
        strategy_name = STRATEGIES[strategy_num]["name"]
        status = "âœ… ì„±ê³µ" if results[strategy_num] else "âŒ ì‹¤íŒ¨"
        print(f"{strategy_num}. {strategy_name:<40} {status}")

    # ì™„ë£Œ ë©”ì‹œì§€
    success_count = sum(results.values())
    total_count = len(results)

    print("\n" + "=" * 70)
    if success_count == total_count:
        print("âœ… ëª¨ë“  í‰ê°€ ì™„ë£Œ!")
    else:
        print(f"âš ï¸  ì¼ë¶€ í‰ê°€ ì‹¤íŒ¨ ({success_count}/{total_count} ì„±ê³µ)")
    print("=" * 70)

    print("\nğŸ“Š ë‹¤ìŒ ë‹¨ê³„:")
    print("   1. Langfuse ëŒ€ì‹œë³´ë“œì—ì„œ ê²°ê³¼ í™•ì¸: https://cloud.langfuse.com")
    print("   2. ê° ì „ëµë³„ stats íŒŒì¼ í™•ì¸:")
    print("      - data/evaluation/ensemble_rrf_evaluation_stats.json")
    print("      - data/evaluation/multiquery_ensemble_rrf_evaluation_stats.json")
    print("      - data/evaluation/multiquery_ensemble_rrf_longcontext_evaluation_stats.json")
    print("      - data/evaluation/ensemble_rrf_timeweighted_longcontext_0.01_evaluation_stats.json")
    print("\n   3. ë¹„êµ ë¶„ì„ì„ ìœ„í•´ Langfuseì—ì„œ CSV export í›„:")
    print("      python scripts/compare_evaluation_results.py")
    print()

    # ì‹¤íŒ¨í•œ ê²½ìš° ì¢…ë£Œ ì½”ë“œ 1 ë°˜í™˜
    if success_count < total_count:
        sys.exit(1)


if __name__ == "__main__":
    main()
