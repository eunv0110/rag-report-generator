#!/usr/bin/env python3
"""BM25 vs Dense Retrieval ì„±ëŠ¥ ë¹„êµ"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

import json
from typing import List, Dict, Any
from qdrant_client import QdrantClient
from config.settings import QDRANT_PATH
from retrievers.bm25_retriever import BM25Retriever
from retrievers.dense_retriever import DenseRetriever
from utils.langfuse_utils import get_langfuse_client
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import context_precision, context_recall
from langchain.chat_models import init_chat_model
import time
import os
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


def load_evaluation_dataset(file_path: str) -> List[Dict[str, Any]]:
    """í‰ê°€ìš© ë°ì´í„°ì…‹ ë¡œë“œ"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def evaluate_retriever(
    retriever,
    retriever_name: str,
    eval_data: List[Dict[str, Any]],
    top_k: int = 5
) -> Dict[str, Any]:
    """
    íŠ¹ì • ë¦¬íŠ¸ë¦¬ë²„ í‰ê°€

    Args:
        retriever: í‰ê°€í•  ë¦¬íŠ¸ë¦¬ë²„
        retriever_name: ë¦¬íŠ¸ë¦¬ë²„ ì´ë¦„
        eval_data: í‰ê°€ ë°ì´í„°
        top_k: ê²€ìƒ‰í•  ìƒìœ„ kê°œ ë¬¸ì„œ

    Returns:
        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    print(f"\n{'=' * 60}")
    print(f"ğŸ” {retriever_name} í‰ê°€ ì¤‘...")
    print(f"{'=' * 60}")

    questions = []
    ground_truths = []
    contexts_list = []
    answers = []

    # ê²€ìƒ‰ ì‹œê°„ ì¸¡ì •
    search_times = []

    for item in eval_data:
        question = item["question"]
        ground_truth = item["ground_truth"]

        # ê²€ìƒ‰ ìˆ˜í–‰ ë° ì‹œê°„ ì¸¡ì •
        start_time = time.time()
        search_results = retriever.search(question, top_k=top_k)
        search_time = time.time() - start_time
        search_times.append(search_time)

        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        contexts = [result.combined_text for result in search_results]
        answer = contexts[0] if contexts else "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"

        questions.append(question)
        ground_truths.append(ground_truth)
        contexts_list.append(contexts)
        answers.append(answer)

    # Azure AI LLM ì´ˆê¸°í™”
    llm = init_chat_model("azure_ai:gpt-5.1")

    # Ragas í‰ê°€
    ragas_dataset = Dataset.from_dict({
        "question": questions,
        "ground_truth": ground_truths,
        "contexts": contexts_list,
        "answer": answers,
    })

    metrics = [context_precision, context_recall]

    try:
        results = evaluate(ragas_dataset, metrics=metrics, llm=llm)

        # í‰ê·  ê²€ìƒ‰ ì‹œê°„ ì¶”ê°€
        avg_search_time = sum(search_times) / len(search_times) if search_times else 0

        return {
            "retriever": retriever_name,
            "context_precision": float(results['context_precision']),
            "context_recall": float(results['context_recall']),
            "avg_search_time": avg_search_time,
            "total_queries": len(questions)
        }

    except Exception as e:
        print(f"âŒ {retriever_name} í‰ê°€ ì‹¤íŒ¨: {e}")
        return {
            "retriever": retriever_name,
            "error": str(e)
        }


def compare_retrievers(
    dataset_path: str = "data/evaluation/sample_qa.json",
    top_k: int = 5,
    use_korean_tokenizer: bool = True
):
    """
    BM25ì™€ Dense ê²€ìƒ‰ ì„±ëŠ¥ ë¹„êµ

    Args:
        dataset_path: í‰ê°€ ë°ì´í„°ì…‹ ê²½ë¡œ
        top_k: ê²€ìƒ‰í•  ìƒìœ„ kê°œ ë¬¸ì„œ
        use_korean_tokenizer: BM25ì—ì„œ í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì‚¬ìš© ì—¬ë¶€
    """
    print("=" * 60)
    print("ğŸ“Š BM25 vs Dense Retrieval ì„±ëŠ¥ ë¹„êµ")
    print("=" * 60)

    # Langfuse í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    langfuse = get_langfuse_client()

    # Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    qdrant_client = QdrantClient(path=QDRANT_PATH)

    # í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ
    eval_data = load_evaluation_dataset(dataset_path)
    print(f"\nâœ… í‰ê°€ ë°ì´í„°: {len(eval_data)}ê°œ ì§ˆë¬¸")

    # ë¦¬íŠ¸ë¦¬ë²„ë“¤ ì´ˆê¸°í™”
    print("\nğŸ“¦ ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™” ì¤‘...")
    bm25_retriever = BM25Retriever(qdrant_client, use_korean_tokenizer=use_korean_tokenizer)
    dense_retriever = DenseRetriever(qdrant_client)

    # ê° ë¦¬íŠ¸ë¦¬ë²„ í‰ê°€
    results = []

    # BM25 í‰ê°€
    bm25_results = evaluate_retriever(
        bm25_retriever,
        "BM25",
        eval_data,
        top_k=top_k
    )
    results.append(bm25_results)

    # Dense í‰ê°€
    dense_results = evaluate_retriever(
        dense_retriever,
        "Dense (Vector)",
        eval_data,
        top_k=top_k
    )
    results.append(dense_results)

    # ë¹„êµ ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“ˆ ì¢…í•© ë¹„êµ ê²°ê³¼")
    print("=" * 60)

    print(f"\n{'Retriever':<20} {'Precision':<12} {'Recall':<12} {'Avg Time (ms)':<15}")
    print("-" * 60)

    for result in results:
        if "error" not in result:
            print(
                f"{result['retriever']:<20} "
                f"{result['context_precision']:<12.4f} "
                f"{result['context_recall']:<12.4f} "
                f"{result['avg_search_time']*1000:<15.2f}"
            )
        else:
            print(f"{result['retriever']:<20} ERROR: {result['error']}")

    # ìŠ¹ì íŒì •
    if len(results) == 2 and "error" not in results[0] and "error" not in results[1]:
        print("\n" + "=" * 60)
        print("ğŸ† ìŠ¹ì íŒì •")
        print("=" * 60)

        bm25_score = results[0]['context_precision'] + results[0]['context_recall']
        dense_score = results[1]['context_precision'] + results[1]['context_recall']

        if bm25_score > dense_score:
            winner = "BM25"
            diff = bm25_score - dense_score
        elif dense_score > bm25_score:
            winner = "Dense (Vector)"
            diff = dense_score - bm25_score
        else:
            winner = "ë¬´ìŠ¹ë¶€"
            diff = 0

        print(f"\nğŸ¥‡ {winner}")
        if diff > 0:
            print(f"   ì ìˆ˜ ì°¨ì´: {diff:.4f}")

        print(f"\nğŸ’¡ ë¶„ì„:")
        if results[0]['context_precision'] > results[1]['context_precision']:
            print(f"   - BM25ê°€ ì •ë°€ë„ê°€ ë” ë†’ìŒ (+{results[0]['context_precision'] - results[1]['context_precision']:.4f})")
        else:
            print(f"   - Denseê°€ ì •ë°€ë„ê°€ ë” ë†’ìŒ (+{results[1]['context_precision'] - results[0]['context_precision']:.4f})")

        if results[0]['context_recall'] > results[1]['context_recall']:
            print(f"   - BM25ê°€ ì¬í˜„ìœ¨ì´ ë” ë†’ìŒ (+{results[0]['context_recall'] - results[1]['context_recall']:.4f})")
        else:
            print(f"   - Denseê°€ ì¬í˜„ìœ¨ì´ ë” ë†’ìŒ (+{results[1]['context_recall'] - results[0]['context_recall']:.4f})")

        if results[0]['avg_search_time'] < results[1]['avg_search_time']:
            print(f"   - BM25ê°€ ë” ë¹ ë¦„ ({results[0]['avg_search_time']*1000:.2f}ms vs {results[1]['avg_search_time']*1000:.2f}ms)")
        else:
            print(f"   - Denseê°€ ë” ë¹ ë¦„ ({results[1]['avg_search_time']*1000:.2f}ms vs {results[0]['avg_search_time']*1000:.2f}ms)")

    # Langfuseì— ê²°ê³¼ ë¡œê¹…
    if langfuse:
        print("\nğŸ“Š Langfuseì— ê²°ê³¼ ì—…ë¡œë“œ ì¤‘...")

        for result in results:
            if "error" not in result:
                retriever_name = result['retriever'].lower().replace(" ", "_").replace("(", "").replace(")", "")

                langfuse.score(
                    name=f"{retriever_name}_context_precision",
                    value=result['context_precision'],
                    data_type="numeric",
                    comment=f"{result['retriever']} - Context Precision"
                )

                langfuse.score(
                    name=f"{retriever_name}_context_recall",
                    value=result['context_recall'],
                    data_type="numeric",
                    comment=f"{result['retriever']} - Context Recall"
                )

                langfuse.score(
                    name=f"{retriever_name}_avg_search_time",
                    value=result['avg_search_time'],
                    data_type="numeric",
                    comment=f"{result['retriever']} - Average Search Time (seconds)"
                )

        langfuse.flush()
        print("âœ… Langfuse ì—…ë¡œë“œ ì™„ë£Œ")

    # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    output_file = Path(dataset_path).parent / "comparison_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ’¾ ë¹„êµ ê²°ê³¼ ì €ì¥: {output_file}")

    return results


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description="BM25 vs Dense ê²€ìƒ‰ ì„±ëŠ¥ ë¹„êµ")
    parser.add_argument(
        "--dataset",
        type=str,
        default="data/evaluation/sample_qa.json",
        help="í‰ê°€ ë°ì´í„°ì…‹ ê²½ë¡œ"
    )
    parser.add_argument(
        "--top-k",
        type=int,
        default=5,
        help="ê²€ìƒ‰í•  ìƒìœ„ kê°œ ë¬¸ì„œ"
    )
    parser.add_argument(
        "--no-korean-tokenizer",
        action="store_true",
        help="BM25ì—ì„œ í•œêµ­ì–´ í† í¬ë‚˜ì´ì € ì‚¬ìš© ì•ˆ í•¨"
    )

    args = parser.parse_args()

    compare_retrievers(
        dataset_path=args.dataset,
        top_k=args.top_k,
        use_korean_tokenizer=not args.no_korean_tokenizer
    )


if __name__ == "__main__":
    main()
