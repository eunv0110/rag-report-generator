#!/usr/bin/env python3
"""í†µí•© QA ë°ì´í„°ì…‹ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ - ì—¬ëŸ¬ QA íƒ€ì… ì§€ì›"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

import json
import random
from typing import List, Dict, Any, Optional, Literal
from openai import AzureOpenAI, OpenAI
from config.settings import (
    AZURE_AI_CREDENTIAL,
    AZURE_AI_ENDPOINT,
    OPENROUTER_API_KEY,
    OPENROUTER_BASE_URL,
    DATA_DIR
)
from utils.file_utils import load_json, save_json

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„í¬íŠ¸
sys.path.insert(0, str(Path(__file__).parent.parent / "prompts" / "templates" / "data"))
from qa_prompts import (
    SIMPLE_QUESTION_PROMPT,
    SIMPLE_ANSWER_PROMPT,
    CONCISE_QUESTION_PROMPT,
    CONCISE_ANSWER_PROMPT,
    STANDARD_QA_PROMPT,
    HARD_QA_PROMPT,
    USER_SCENARIO_QUESTION_PROMPT,
    USER_SCENARIO_ANSWER_PROMPT
)


# ============================================================================
# ìƒìˆ˜ ì •ì˜
# ============================================================================
QA_TYPE = Literal["simple", "concise", "standard", "hard", "user_scenario"]

DEFAULT_NUM_SAMPLES = 20
MAX_TEXT_LENGTH = 1000
MIN_CONTENT_LENGTH = 100
DEFAULT_OUTPUT_DIR = "data/evaluation"

# Hard QA ì„¤ì • (ì—¬ëŸ¬ ë¬¸ì„œ ì°¸ì¡°)
MIN_DOCS_PER_QUESTION = 1
MAX_DOCS_PER_QUESTION = 3
MULTI_DOC_PROBABILITY = 0.5


# ============================================================================
# QA íƒ€ì…ë³„ ì„¤ì •
# ============================================================================
QA_TYPE_CONFIG = {
    "simple": {
        "description": "ê°„ë‹¨í•œ ì§ˆë¬¸-ë‹µë³€ (3-5ë¬¸ì¥)",
        "question_prompt": SIMPLE_QUESTION_PROMPT,
        "answer_prompt": SIMPLE_ANSWER_PROMPT,
        "llm_provider": "azure",
        "model": "gpt-4.1",
        "temperature": 0.5,
        "max_answer_tokens": 400,
        "category": "simple_qa",
        "difficulty": "easy"
    },
    "concise": {
        "description": "ê°„ê²°í•œ ì§ˆë¬¸-ë‹µë³€ (100-300ì)",
        "question_prompt": CONCISE_QUESTION_PROMPT,
        "answer_prompt": CONCISE_ANSWER_PROMPT,
        "llm_provider": "azure",
        "model": "gpt-4.1",
        "temperature": 0.3,
        "max_answer_tokens": 200,
        "category": "concise_qa",
        "difficulty": "medium",
        "max_answer_length": 400
    },
    "standard": {
        "description": "í‘œì¤€ ì§ˆë¬¸-ë‹µë³€ (100-300ë‹¨ì–´)",
        "question_prompt": STANDARD_QA_PROMPT,
        "answer_prompt": None,  # ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        "llm_provider": "azure",
        "model": "gpt-4.1",
        "temperature": 0.7,
        "max_answer_tokens": 600,
        "category": "llm_generated",
        "difficulty": "medium"
    },
    "hard": {
        "description": "ì–´ë ¤ìš´ ì§ˆë¬¸-ë‹µë³€ (ì—¬ëŸ¬ ë¬¸ì„œ ì°¸ì¡°)",
        "question_prompt": HARD_QA_PROMPT,
        "answer_prompt": None,  # ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        "llm_provider": "openrouter",
        "model": "openai/gpt-4o",
        "temperature": 0.8,
        "max_answer_tokens": 800,
        "category": "llm_generated_hard",
        "difficulty": "hard",
        "multi_doc": True
    },
    "user_scenario": {
        "description": "ì‚¬ìš©ì ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ ì§ˆë¬¸-ë‹µë³€",
        "question_prompt": USER_SCENARIO_QUESTION_PROMPT,
        "answer_prompt": USER_SCENARIO_ANSWER_PROMPT,
        "llm_provider": "azure",
        "model": "gpt-4.1",
        "temperature": 0.5,
        "max_answer_tokens": 400,
        "category": "simple_user_scenario",
        "difficulty": "easy"
    }
}


# ============================================================================
# ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
# ============================================================================
def load_notion_data() -> List[Dict[str, Any]]:
    """Notion ë°ì´í„° ë¡œë“œ"""
    data_file = DATA_DIR / "notion_data.json"

    if not data_file.exists():
        raise FileNotFoundError(
            f"Notion ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {data_file}\n"
            "ë¨¼ì € 'python scripts/build_vectordb.py'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
        )

    data = load_json(str(data_file))

    # ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
    if isinstance(data, list):
        pages = data
    else:
        pages = data.get("pages", [])

    # ì œëª©ì´ ìˆëŠ” í˜ì´ì§€ë§Œ í•„í„°ë§
    return [page for page in pages if page.get("title")]


# ============================================================================
# í…ìŠ¤íŠ¸ ì²˜ë¦¬ í•¨ìˆ˜
# ============================================================================
def extract_text_from_page(page: Dict[str, Any], max_length: int = MAX_TEXT_LENGTH) -> str:
    """
    í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ

    Args:
        page: Notion í˜ì´ì§€ ë°ì´í„°
        max_length: ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´

    Returns:
        ì¶”ì¶œëœ í…ìŠ¤íŠ¸
    """
    content = page.get("content", "")

    # Case 1: ë¬¸ìì—´ í˜•ì‹ì˜ content
    if isinstance(content, str):
        return _clean_string_content(content, max_length)

    # Case 2: blocks í˜•ì‹ì˜ content
    blocks = page.get("blocks", [])
    if blocks:
        content = _extract_from_blocks(blocks)
        if len(content) > max_length:
            content = content[:max_length] + "..."
        return content

    return ""


def _clean_string_content(content: str, max_length: int) -> str:
    """ë¬¸ìì—´ content ì •ì œ"""
    import re

    # ì´ë¯¸ì§€ íƒœê·¸ ì œê±°
    content = re.sub(r'\[Image:.*?\]', '', content)

    # ê¸¸ì´ ì œí•œ
    if len(content) > max_length:
        content = content[:max_length] + "..."

    return content.strip()


def _extract_from_blocks(blocks: List[Dict]) -> str:
    """blocks í˜•ì‹ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    texts = []

    for block in blocks:
        block_type = block.get("type")

        # ì¼ë°˜ í…ìŠ¤íŠ¸ ë¸”ë¡
        if block_type in ["paragraph", "heading_1", "heading_2", "heading_3"]:
            texts.extend(_extract_rich_text(block.get(block_type, {})))

        # ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ
        elif block_type == "bulleted_list_item":
            rich_texts = _extract_rich_text(block.get("bulleted_list_item", {}))
            texts.extend([f"â€¢ {text}" for text in rich_texts])

    return " ".join(texts)


def _extract_rich_text(block_content: Dict) -> List[str]:
    """rich_textì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    texts = []

    for rt in block_content.get("rich_text", []):
        text = rt.get("plain_text", "").strip()
        if text:
            texts.append(text)

    return texts


# ============================================================================
# LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
# ============================================================================
def initialize_llm_client(provider: str):
    """
    LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”

    Args:
        provider: "azure" ë˜ëŠ” "openrouter"

    Returns:
        (client, model) íŠœí”Œ
    """
    if provider == "azure":
        if not AZURE_AI_CREDENTIAL:
            raise ValueError("AZURE_AI_CREDENTIALì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        client = AzureOpenAI(
            api_key=AZURE_AI_CREDENTIAL,
            azure_endpoint=AZURE_AI_ENDPOINT,
            api_version="2024-02-01"
        )
        model = "gpt-4.1"

    elif provider == "openrouter":
        if not OPENROUTER_API_KEY:
            raise ValueError("OPENROUTER_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        client = OpenAI(
            api_key=OPENROUTER_API_KEY,
            base_url=OPENROUTER_BASE_URL
        )
        model = "openai/gpt-4o"

    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM provider: {provider}")

    return client, model


# ============================================================================
# QA ìƒì„± í•¨ìˆ˜ (2ë‹¨ê³„: ì§ˆë¬¸ + ë‹µë³€)
# ============================================================================
def generate_two_step_qa(
    page: Dict[str, Any],
    client,
    config: Dict[str, Any]
) -> Optional[Dict[str, Any]]:
    """
    2ë‹¨ê³„ QA ìƒì„± (ì§ˆë¬¸ ìƒì„± -> ë‹µë³€ ìƒì„±)

    simple, concise, user_scenario íƒ€ì…ì—ì„œ ì‚¬ìš©
    """
    title = page.get("title", "Untitled")
    page_id = page.get("page_id", "")
    content = extract_text_from_page(page)

    if len(content) < MIN_CONTENT_LENGTH:
        return None

    # ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ë©´ ì œí•œ
    max_content = 800 if config["category"] == "concise_qa" else 1000
    if len(content) > max_content:
        content = content[:max_content] + "..."

    try:
        # 1. ì§ˆë¬¸ ìƒì„±
        question_prompt = config["question_prompt"].format(
            title=title,
            content=content
        )

        question_response = client.chat.completions.create(
            model=config["model"],
            messages=[{"role": "user", "content": question_prompt}],
            temperature=0.7,
            max_tokens=200,
            response_format={"type": "json_object"}
        )

        question_data = json.loads(question_response.choices[0].message.content)
        question = question_data.get("question", "")

        if not question:
            return None

        # 2. ë‹µë³€ ìƒì„±
        answer_prompt = config["answer_prompt"].format(
            question=question,
            content=content
        )

        # concise íƒ€ì…ì€ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€
        messages = []
        if config["category"] == "concise_qa":
            messages.append({
                "role": "system",
                "content": "ë‹¹ì‹ ì€ ê°„ê²°í•˜ê³  í•µì‹¬ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 100-300ì ì´ë‚´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."
            })

        messages.append({"role": "user", "content": answer_prompt})

        answer_response = client.chat.completions.create(
            model=config["model"],
            messages=messages,
            temperature=config["temperature"],
            max_tokens=config["max_answer_tokens"]
        )

        answer = answer_response.choices[0].message.content.strip()

        # ë‹µë³€ ê¸¸ì´ ê²€ì¦ (concise íƒ€ì…)
        if "max_answer_length" in config and len(answer) > config["max_answer_length"]:
            return None

        return {
            "question": question,
            "ground_truth": answer,
            "context_page_id": page_id,
            "metadata": {
                "category": config["category"],
                "difficulty": config["difficulty"],
                "source": f"llm_{config['llm_provider']}",
                "page_title": title,
                "answer_length": len(answer),
                "key_topics": question_data.get("key_topics", [])
            }
        }

    except Exception as e:
        print(f"  âŒ ì˜¤ë¥˜: {e}")
        return None


# ============================================================================
# QA ìƒì„± í•¨ìˆ˜ (1ë‹¨ê³„: í†µí•©)
# ============================================================================
def generate_one_step_qa(
    page: Dict[str, Any],
    client,
    config: Dict[str, Any]
) -> Optional[Dict[str, Any]]:
    """
    1ë‹¨ê³„ QA ìƒì„± (ì§ˆë¬¸ê³¼ ë‹µë³€ì„ í•œ ë²ˆì— ìƒì„±)

    standard íƒ€ì…ì—ì„œ ì‚¬ìš©
    """
    title = page.get("title", "Untitled")
    page_id = page.get("page_id", "")
    content = extract_text_from_page(page)

    if len(content) < MIN_CONTENT_LENGTH:
        return None

    try:
        prompt = config["question_prompt"].format(
            page_title=title,
            content=content
        )

        response = client.chat.completions.create(
            model=config["model"],
            messages=[{"role": "user", "content": prompt}],
            temperature=config["temperature"],
            max_tokens=config["max_answer_tokens"],
            response_format={"type": "json_object"}
        )

        result = json.loads(response.choices[0].message.content)

        return {
            "question": result.get("question", ""),
            "ground_truth": result.get("ground_truth", ""),
            "context_page_id": page_id,
            "metadata": {
                "category": config["category"],
                "difficulty": result.get("difficulty", config["difficulty"]),
                "source": f"llm_{config['llm_provider']}",
                "page_title": title,
                "key_concepts": result.get("key_concepts", [])
            }
        }

    except Exception as e:
        print(f"  âŒ ì˜¤ë¥˜: {e}")
        return None


# ============================================================================
# Hard QA ìƒì„± í•¨ìˆ˜ (ì—¬ëŸ¬ ë¬¸ì„œ ì°¸ì¡°)
# ============================================================================
def select_documents_for_question(
    pages: List[Dict[str, Any]],
    used_page_ids: set
) -> List[Dict[str, Any]]:
    """ì§ˆë¬¸ ìƒì„±ì„ ìœ„í•œ ë¬¸ì„œ ì„ íƒ (1-3ê°œ)"""
    available_pages = [p for p in pages if p.get("page_id") not in used_page_ids]

    if not available_pages:
        return []

    # ë¬¸ì„œ ê°œìˆ˜ ê²°ì •
    if random.random() < MULTI_DOC_PROBABILITY and len(available_pages) >= 2:
        num_docs = random.randint(2, min(MAX_DOCS_PER_QUESTION, len(available_pages)))
    else:
        num_docs = 1

    return random.sample(available_pages, num_docs)


def format_documents_for_prompt(pages: List[Dict[str, Any]]) -> str:
    """ì—¬ëŸ¬ ë¬¸ì„œë¥¼ í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…"""
    if len(pages) == 1:
        page = pages[0]
        return f"- ë¬¸ì„œ ì œëª©: {page.get('title', 'Untitled')}\n- ë¬¸ì„œ ë‚´ìš©:\n{extract_text_from_page(page)}"

    # ì—¬ëŸ¬ ë¬¸ì„œì¸ ê²½ìš°
    formatted_docs = []
    for i, page in enumerate(pages, 1):
        title = page.get('title', 'Untitled')
        content = extract_text_from_page(page, max_length=600)
        formatted_docs.append(f"ë¬¸ì„œ {i}: \"{title}\"\në‚´ìš©: {content}")

    return "\n\n".join(formatted_docs)


def generate_hard_qa(
    pages: List[Dict[str, Any]],
    client,
    config: Dict[str, Any]
) -> Optional[Dict[str, Any]]:
    """Hard QA ìƒì„± (ì—¬ëŸ¬ ë¬¸ì„œ ì°¸ì¡° ê°€ëŠ¥)"""
    total_content_length = sum(len(extract_text_from_page(p)) for p in pages)

    if total_content_length < MIN_CONTENT_LENGTH:
        return None

    try:
        formatted_documents = format_documents_for_prompt(pages)
        prompt = config["question_prompt"].format(documents=formatted_documents)

        response = client.chat.completions.create(
            model=config["model"],
            messages=[{"role": "user", "content": prompt}],
            temperature=config["temperature"],
            max_tokens=config["max_answer_tokens"],
            response_format={"type": "json_object"}
        )

        result = json.loads(response.choices[0].message.content)

        page_titles = [p.get("title", "Untitled") for p in pages]
        page_ids = [p.get("page_id", "") for p in pages]

        return {
            "question": result.get("question", ""),
            "ground_truth": result.get("ground_truth", ""),
            "context_page_id": page_ids,
            "metadata": {
                "category": config["category"],
                "difficulty": config["difficulty"],
                "source": f"llm_{config['llm_provider']}",
                "page_titles": page_titles,
                "num_referenced_docs": len(pages),
                "referenced_docs": result.get("referenced_docs", page_titles),
                "reasoning_type": result.get("reasoning_type", "unknown")
            }
        }

    except Exception as e:
        print(f"  âŒ ì˜¤ë¥˜: {e}")
        return None


# ============================================================================
# ë©”ì¸ QA ìƒì„± í•¨ìˆ˜
# ============================================================================
def generate_qa_dataset(
    qa_type: QA_TYPE,
    num_samples: int = DEFAULT_NUM_SAMPLES,
    output_path: Optional[str] = None
) -> List[Dict[str, Any]]:
    """
    QA ë°ì´í„°ì…‹ ìƒì„±

    Args:
        qa_type: QA íƒ€ì… (simple, concise, standard, hard, user_scenario)
        num_samples: ìƒì„±í•  ìƒ˜í”Œ ìˆ˜
        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ

    Returns:
        ìƒì„±ëœ QA ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    if qa_type not in QA_TYPE_CONFIG:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” QA íƒ€ì…: {qa_type}")

    config = QA_TYPE_CONFIG[qa_type]

    print("=" * 60)
    print(f"ğŸ“ {config['description']} ë°ì´í„°ì…‹ ìƒì„±")
    print("=" * 60)

    # Notion ë°ì´í„° ë¡œë“œ
    print(f"\nğŸ“‚ Notion ë°ì´í„° ë¡œë“œ ì¤‘...")
    pages = load_notion_data()
    print(f"âœ… {len(pages)}ê°œ í˜ì´ì§€ ë¡œë“œ")

    # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    client, model = initialize_llm_client(config["llm_provider"])
    print(f"âœ… LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”: {config['llm_provider']} ({model})")

    # QA ìƒì„±
    print(f"\nğŸ¤– QA ìƒì„± ì¤‘ (ëª©í‘œ: {num_samples}ê°œ)...\n")

    qa_data = []
    used_page_ids = set()
    attempted = 0
    max_attempts = num_samples * 3

    random.shuffle(pages)

    # Hard QAëŠ” ì—¬ëŸ¬ ë¬¸ì„œ ì²˜ë¦¬
    if config.get("multi_doc", False):
        while len(qa_data) < num_samples and attempted < max_attempts:
            attempted += 1

            selected_pages = select_documents_for_question(pages, used_page_ids)
            if not selected_pages:
                break

            for page in selected_pages:
                used_page_ids.add(page.get("page_id"))

            print(f"[{len(qa_data)+1}/{num_samples}] {len(selected_pages)}ê°œ ë¬¸ì„œ ", end="", flush=True)

            qa_item = generate_hard_qa(selected_pages, client, config)
            if qa_item:
                qa_data.append(qa_item)
                titles = [p.get("title", "Untitled")[:30] for p in selected_pages]
                print(f"âœ… {', '.join(titles)}")
            else:
                print("â­ï¸ ìŠ¤í‚µ")

    # ë‹¤ë¥¸ QA íƒ€ì…ì€ ë‹¨ì¼ ë¬¸ì„œ ì²˜ë¦¬
    else:
        for page in pages:
            if len(qa_data) >= num_samples or attempted >= max_attempts:
                break

            attempted += 1
            print(f"[{len(qa_data)+1}/{num_samples}] ", end="", flush=True)

            # 2ë‹¨ê³„ QA ìƒì„±
            if config["answer_prompt"]:
                qa_item = generate_two_step_qa(page, client, config)
            # 1ë‹¨ê³„ QA ìƒì„±
            else:
                qa_item = generate_one_step_qa(page, client, config)

            if qa_item:
                qa_data.append(qa_item)
                print(f"âœ… {page.get('title', 'Untitled')[:40]}")
            else:
                print("â­ï¸ ìŠ¤í‚µ")

    # ì €ì¥
    if output_path is None:
        output_path = f"{DEFAULT_OUTPUT_DIR}/{qa_type}_qa_dataset.json"

    output_path_obj = Path(output_path)
    output_path_obj.parent.mkdir(parents=True, exist_ok=True)
    save_json(qa_data, str(output_path_obj))

    # ê²°ê³¼ ì¶œë ¥
    print(f"\n{'=' * 60}")
    print(f"âœ… QA ìƒì„± ì™„ë£Œ: {len(qa_data)}ê°œ")
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_path}")
    print(f"{'=' * 60}")

    # í†µê³„ ì¶œë ¥
    _print_statistics(qa_data, qa_type)

    return qa_data


def _print_statistics(qa_data: List[Dict[str, Any]], qa_type: str):
    """í†µê³„ ì¶œë ¥"""
    if not qa_data:
        return

    avg_q_len = sum(len(item["question"]) for item in qa_data) / len(qa_data)
    avg_a_len = sum(len(item["ground_truth"]) for item in qa_data) / len(qa_data)

    print(f"\nğŸ“Š í†µê³„:")
    print(f"   í‰ê·  ì§ˆë¬¸ ê¸¸ì´: {avg_q_len:.0f}ì")
    print(f"   í‰ê·  ë‹µë³€ ê¸¸ì´: {avg_a_len:.0f}ì")

    # Concise íƒ€ì… ì¶”ê°€ í†µê³„
    if qa_type == "concise":
        min_a_len = min(len(item["ground_truth"]) for item in qa_data)
        max_a_len = max(len(item["ground_truth"]) for item in qa_data)
        print(f"   ë‹µë³€ ê¸¸ì´ ë²”ìœ„: {min_a_len}~{max_a_len}ì")

        short = sum(1 for item in qa_data if len(item["ground_truth"]) < 150)
        medium = sum(1 for item in qa_data if 150 <= len(item["ground_truth"]) <= 250)
        long = sum(1 for item in qa_data if len(item["ground_truth"]) > 250)

        print(f"\n   ë‹µë³€ ê¸¸ì´ ë¶„í¬:")
        print(f"      ì§§ìŒ (<150ì): {short}ê°œ ({short/len(qa_data)*100:.1f}%)")
        print(f"      ì ì • (150-250ì): {medium}ê°œ ({medium/len(qa_data)*100:.1f}%)")
        print(f"      ê¸º (>250ì): {long}ê°œ ({long/len(qa_data)*100:.1f}%)")

    # Hard íƒ€ì… ì¶”ê°€ í†µê³„
    if qa_type == "hard":
        single_doc = sum(1 for item in qa_data if item["metadata"]["num_referenced_docs"] == 1)
        multi_doc = sum(1 for item in qa_data if item["metadata"]["num_referenced_docs"] > 1)

        print(f"\n   ë¬¸ì„œ ì°¸ì¡° í†µê³„:")
        print(f"      ë‹¨ì¼ ë¬¸ì„œ: {single_doc}ê°œ ({single_doc/len(qa_data)*100:.1f}%)")
        print(f"      ì—¬ëŸ¬ ë¬¸ì„œ: {multi_doc}ê°œ ({multi_doc/len(qa_data)*100:.1f}%)")

        if multi_doc > 0:
            avg_docs = sum(
                item["metadata"]["num_referenced_docs"]
                for item in qa_data if item["metadata"]["num_referenced_docs"] > 1
            ) / multi_doc
            print(f"      í‰ê·  ì°¸ì¡° ë¬¸ì„œ ìˆ˜ (ì—¬ëŸ¬ ë¬¸ì„œ ì§ˆë¬¸): {avg_docs:.1f}ê°œ")

    # ìƒ˜í”Œ ì¶œë ¥
    print(f"\nğŸ“‹ ìƒ˜í”Œ QA:")
    for i, item in enumerate(qa_data[:3], 1):
        print(f"\n   [{i}] Q: {item['question']}")
        answer_preview = item['ground_truth'][:100] + "..." if len(item['ground_truth']) > 100 else item['ground_truth']
        print(f"       A ({len(item['ground_truth'])}ì): {answer_preview}")


# ============================================================================
# ë©”ì¸ í•¨ìˆ˜
# ============================================================================
def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(
        description="í†µí•© QA ë°ì´í„°ì…‹ ìƒì„± ìŠ¤í¬ë¦½íŠ¸",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
QA íƒ€ì…:
  simple          : ê°„ë‹¨í•œ ì§ˆë¬¸-ë‹µë³€ (3-5ë¬¸ì¥)
  concise         : ê°„ê²°í•œ ì§ˆë¬¸-ë‹µë³€ (100-300ì)
  standard        : í‘œì¤€ ì§ˆë¬¸-ë‹µë³€ (100-300ë‹¨ì–´)
  hard            : ì–´ë ¤ìš´ ì§ˆë¬¸-ë‹µë³€ (ì—¬ëŸ¬ ë¬¸ì„œ ì°¸ì¡°)
  user_scenario   : ì‚¬ìš©ì ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ ì§ˆë¬¸-ë‹µë³€

ì‚¬ìš© ì˜ˆì‹œ:
  # Simple QA 20ê°œ ìƒì„±
  python scripts/generate_qa_dataset.py --type simple --num-samples 20

  # Concise QA 50ê°œ ìƒì„±
  python scripts/generate_qa_dataset.py --type concise --num-samples 50

  # Hard QA 30ê°œ ìƒì„± (ì»¤ìŠ¤í…€ ì¶œë ¥ ê²½ë¡œ)
  python scripts/generate_qa_dataset.py --type hard --num-samples 30 --output data/custom_hard_qa.json

  # ì—¬ëŸ¬ íƒ€ì… ìˆœì°¨ ìƒì„±
  python scripts/generate_qa_dataset.py --type simple --num-samples 20
  python scripts/generate_qa_dataset.py --type concise --num-samples 30
        """
    )

    parser.add_argument(
        "--type",
        type=str,
        required=True,
        choices=list(QA_TYPE_CONFIG.keys()),
        help="QA íƒ€ì… ì„ íƒ"
    )
    parser.add_argument(
        "--num-samples",
        type=int,
        default=DEFAULT_NUM_SAMPLES,
        help=f"ìƒì„±í•  ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸ê°’: {DEFAULT_NUM_SAMPLES})"
    )
    parser.add_argument(
        "--output",
        type=str,
        help="ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: data/evaluation/{type}_qa_dataset.json)"
    )

    args = parser.parse_args()

    try:
        generate_qa_dataset(
            qa_type=args.type,
            num_samples=args.num_samples,
            output_path=args.output
        )

        print("\nâœ¨ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
