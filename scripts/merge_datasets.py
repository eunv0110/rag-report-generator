#!/usr/bin/env python3
"""ë°ì´í„°ì…‹ ë³‘í•©

ì—¬ëŸ¬ QA ë°ì´í„°ì…‹ íŒŒì¼ì„ í•˜ë‚˜ë¡œ ë³‘í•©í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
ì¤‘ë³µ ì œê±° ì˜µì…˜ ì œê³µ
"""

import json
from pathlib import Path
from typing import List, Dict, Any


def load_dataset(file_path: str) -> List[Dict[str, Any]]:
    """ë°ì´í„°ì…‹ ë¡œë“œ"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def merge_datasets(
    dataset1_path: str,
    dataset2_path: str,
    output_path: str,
    remove_duplicates: bool = True
):
    """
    ë‘ ê°œì˜ QA ë°ì´í„°ì…‹ ë³‘í•©

    Args:
        dataset1_path: ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ ê²½ë¡œ
        dataset2_path: ë‘ ë²ˆì§¸ ë°ì´í„°ì…‹ ê²½ë¡œ
        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        remove_duplicates: ì¤‘ë³µ ì§ˆë¬¸ ì œê±° ì—¬ë¶€
    """
    print("=" * 80)
    print("ğŸ”— QA ë°ì´í„°ì…‹ ë³‘í•©")
    print("=" * 80)

    # ë°ì´í„°ì…‹ ë¡œë“œ
    print(f"\nğŸ“‚ ë°ì´í„°ì…‹ 1 ë¡œë“œ: {dataset1_path}")
    dataset1 = load_dataset(dataset1_path)
    print(f"   âœ… {len(dataset1)}ê°œ í•­ëª©")

    print(f"\nğŸ“‚ ë°ì´í„°ì…‹ 2 ë¡œë“œ: {dataset2_path}")
    dataset2 = load_dataset(dataset2_path)
    print(f"   âœ… {len(dataset2)}ê°œ í•­ëª©")

    # ë³‘í•©
    merged = dataset1 + dataset2
    print(f"\nğŸ”— ë³‘í•© ì™„ë£Œ: {len(merged)}ê°œ í•­ëª©")

    # ì¤‘ë³µ ì œê±° (ì˜µì…˜)
    if remove_duplicates:
        print(f"\nğŸ” ì¤‘ë³µ ì§ˆë¬¸ ì œê±° ì¤‘...")
        seen_questions = set()
        unique_data = []
        duplicates = 0

        for item in merged:
            question = item.get('question', '').strip()
            if question and question not in seen_questions:
                seen_questions.add(question)
                unique_data.append(item)
            else:
                duplicates += 1

        print(f"   âœ… ì¤‘ë³µ ì œê±°: {duplicates}ê°œ í•­ëª© ì œê±°ë¨")
        print(f"   âœ… ìµœì¢…: {len(unique_data)}ê°œ í•­ëª©")
        merged = unique_data

    # í†µê³„
    print(f"\nğŸ“Š ë³‘í•©ëœ ë°ì´í„°ì…‹ í†µê³„:")

    # ì¹´í…Œê³ ë¦¬ ë¶„í¬
    from collections import Counter
    categories = [item.get('metadata', {}).get('category', 'unknown') for item in merged]
    category_counts = Counter(categories)

    print(f"\n   ì¹´í…Œê³ ë¦¬ ë¶„í¬:")
    for cat, count in category_counts.most_common():
        print(f"      {cat}: {count}ê°œ ({count/len(merged)*100:.1f}%)")

    # ì†ŒìŠ¤ ë¶„í¬
    sources = [item.get('metadata', {}).get('source', 'unknown') for item in merged]
    source_counts = Counter(sources)

    print(f"\n   ì†ŒìŠ¤ ë¶„í¬:")
    for src, count in source_counts.most_common():
        print(f"      {src}: {count}ê°œ ({count/len(merged)*100:.1f}%)")

    # ë‹µë³€ ê¸¸ì´ í†µê³„
    answer_lengths = [len(item['ground_truth']) for item in merged]
    avg_len = sum(answer_lengths) / len(answer_lengths)
    min_len = min(answer_lengths)
    max_len = max(answer_lengths)

    print(f"\n   ë‹µë³€ ê¸¸ì´:")
    print(f"      í‰ê· : {avg_len:.0f}ì")
    print(f"      ìµœì†Œ: {min_len}ì")
    print(f"      ìµœëŒ€: {max_len}ì")

    # ì €ì¥
    output_path_obj = Path(output_path)
    output_path_obj.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(merged, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_path}")
    print(f"   ì´ {len(merged)}ê°œ í•­ëª©")

    print("\n" + "=" * 80)
    print("âœ… ë³‘í•© ì™„ë£Œ!")
    print("=" * 80)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(
        description="QA ë°ì´í„°ì…‹ ë³‘í•©",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
  # ê¸°ë³¸ ë³‘í•© (ì¤‘ë³µ ì œê±°)
  python merge_qa_datasets.py \\
    --dataset1 data/evaluation/llm_generated_qa_v2.json \\
    --dataset2 data/evaluation/concise_qa_new.json \\
    --output data/evaluation/merged_qa_v3.json

  # ì¤‘ë³µ ìœ ì§€
  python merge_qa_datasets.py \\
    --dataset1 data/evaluation/llm_generated_qa_v2.json \\
    --dataset2 data/evaluation/concise_qa_new.json \\
    --output data/evaluation/merged_qa_v3.json \\
    --keep-duplicates
        """
    )
    parser.add_argument(
        "--dataset1",
        type=str,
        required=True,
        help="ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ ê²½ë¡œ"
    )
    parser.add_argument(
        "--dataset2",
        type=str,
        required=True,
        help="ë‘ ë²ˆì§¸ ë°ì´í„°ì…‹ ê²½ë¡œ"
    )
    parser.add_argument(
        "--output",
        type=str,
        required=True,
        help="ì¶œë ¥ íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--keep-duplicates",
        action="store_true",
        help="ì¤‘ë³µ ì§ˆë¬¸ì„ ìœ ì§€ (ê¸°ë³¸ê°’: ì œê±°)"
    )

    args = parser.parse_args()

    merge_datasets(
        dataset1_path=args.dataset1,
        dataset2_path=args.dataset2,
        output_path=args.output,
        remove_duplicates=not args.keep_duplicates
    )


if __name__ == "__main__":
    main()
