#!/usr/bin/env python3
"""Langfuse ìë™ í‰ê°€(RAGAS) ê¸°ë°˜ RAG ì„±ëŠ¥ í‰ê°€"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

import json
import time
import os
from typing import List, Dict, Any, Optional
from qdrant_client import QdrantClient
from langchain.chat_models import init_chat_model
from langfuse.types import TraceContext

from config.settings import (
    QDRANT_PATH,
    QDRANT_COLLECTION,
    AZURE_AI_CREDENTIAL,
    AZURE_AI_ENDPOINT
)
from retrievers import RetrieverFactory, BaseRetriever
from utils.langfuse_utils import get_langfuse_client

# ìƒìˆ˜ ì •ì˜
DEFAULT_DATASET_PATH = "/home/work/rag/Project/rag-report-generator/data/evaluation/llm_generated_qa_azure.json"
DEFAULT_TOP_K = 5
DEFAULT_TEMPERATURE = 0.1
DEFAULT_MAX_TOKENS = 500
SYSTEM_PROMPT = "ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."


def load_evaluation_dataset(file_path: str) -> List[Dict[str, Any]]:
    """í‰ê°€ìš© ë°ì´í„°ì…‹ ë¡œë“œ"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def generate_llm_answer(question: str, contexts: List[str]) -> str:
    """
    LLM APIë¥¼ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ ìƒì„±

    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        contexts: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸

    Returns:
        ìƒì„±ëœ ë‹µë³€ ë˜ëŠ” ì—ëŸ¬ ë©”ì‹œì§€
    """
    if not AZURE_AI_CREDENTIAL or not AZURE_AI_ENDPOINT:
        return "Azure OpenAI ì„¤ì •ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."

    # í™˜ê²½ë³€ìˆ˜ ì„¤ì •
    os.environ['AZURE_AI_CREDENTIAL'] = AZURE_AI_CREDENTIAL
    os.environ['AZURE_AI_ENDPOINT'] = AZURE_AI_ENDPOINT

    context_text = "\n\n".join(contexts[:3]) if contexts else "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    prompt = f"""ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ:
{context_text}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

    try:
        # langchainì˜ init_chat_model ì‚¬ìš©
        model = init_chat_model(
            "azure_ai:gpt-5.1",
            temperature=DEFAULT_TEMPERATURE,
            max_completion_tokens=DEFAULT_MAX_TOKENS
        )

        response = model.invoke(prompt)
        return response.content
    except Exception as e:
        error_msg = f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}"
        print(f"  âš ï¸ LLM API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return error_msg


def create_trace_and_generation(
    langfuse,
    retriever_name: str,
    question: str,
    contexts: List[str],
    answer: str,
    ground_truth: str,
    context_metadata: List[Dict],
    item_metadata: Dict,
    total_time: float,
    idx: int,
    context_page_id: Optional[str] = None
) -> str:
    """Langfuse Traceì™€ Generation ìƒì„±
    
    Returns:
        trace_id
    """
    
    # âœ… contexts ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë³€í™˜
    context_text = "\n\n---\n\n".join(contexts) if contexts else ""
    
    # 1. í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ë¡œ Generation ì‹œì‘ (ìµœì†Œí•œì˜ ì •ë³´ë§Œ)
    with langfuse.start_as_current_observation(
        as_type='generation',
        name=f"{retriever_name}_generation",
        model="gpt-5.1"
    ) as generation:
        
        # 2. Generationì—ì„œ trace_id ì¶”ì¶œ
        trace_id = generation.trace_id
        
        # âœ… 3. Generation ëª…ì‹œì  ì—…ë°ì´íŠ¸
        langfuse.update_current_generation(
            input={
                "question": question,
                "context": context_text  # âœ… ë°˜ë“œì‹œ ì—¬ê¸°ì„œ ì¶”ê°€!
            },
            output={
                "answer": answer
            },
            metadata={
                "ground_truth": ground_truth,
                "contexts": contexts,
                "context_metadata": context_metadata
            }
        )
        
        # 4. í˜„ì¬ trace ì—…ë°ì´íŠ¸
        langfuse.update_current_trace(
            name=f"{retriever_name}_evaluation_{idx}",
            input={
                "question": question
            },
            output={
                "answer": answer
            },
            metadata={
                "retriever": retriever_name,
                "total_time_ms": total_time * 1000,
                "num_retrieved_contexts": len(contexts),
                "context_page_id": context_page_id,
                "question_id": idx,
                "category": item_metadata.get("category", "unknown"),
                "difficulty": item_metadata.get("difficulty", "unknown")
            }
        )
    
    # ë””ë²„ê¹…
    print(f"\n[DEBUG] Trace {idx}:")
    print(f"  - ID: {trace_id}")
    print(f"  - Question: {question[:50]}...")
    print(f"  - Context length: {len(context_text)} chars")
    print(f"  - Answer length: {len(answer)} chars")
    
    return trace_id


def add_retrieval_quality_score(
    langfuse,
    trace_id: str,
    context_metadata: List[Dict]
):
    """ê²€ìƒ‰ í’ˆì§ˆ ìŠ¤ì½”ì–´ ì¶”ê°€"""
    if not context_metadata:
        return
    
    avg_score = sum(m["score"] for m in context_metadata) / len(context_metadata)
    langfuse.create_score(
        trace_id=trace_id,
        name="retrieval_quality",
        value=avg_score,
        comment=f"Average retrieval score from {len(context_metadata)} contexts"
    )


def evaluate_single_query(
    retriever: BaseRetriever,
    item: Dict[str, Any],
    langfuse,
    idx: int,
    top_k: int
) -> Dict[str, Any]:
    """ë‹¨ì¼ ì¿¼ë¦¬ í‰ê°€"""
    question = item["question"]
    ground_truth = item["ground_truth"]
    context_page_id = item.get("context_page_id")
    item_metadata = item.get("metadata", {})
    
    start_time = time.time()
    
    # 1. ê²€ìƒ‰ ìˆ˜í–‰
    search_results = retriever.search(question, top_k=top_k)
    contexts = [result.combined_text for result in search_results]
    
    # âœ… contextsê°€ ë¹„ì–´ìˆìœ¼ë©´ ê²½ê³ 
    if not contexts:
        print(f"  âš ï¸ [{idx}] No contexts found for question!")
        contexts = ["ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."]  # RAGASë¥¼ ìœ„í•œ ë”ë¯¸ ì»¨í…ìŠ¤íŠ¸
    
    context_metadata = [
        {
            "score": result.score,
            "page_title": result.page_title,
            "section_title": result.section_title,
            "chunk_id": result.chunk_id
        }
        for result in search_results
    ]
    
    # 2. LLM ë‹µë³€ ìƒì„±
    answer = generate_llm_answer(question, contexts)
    
    # âœ… answerê°€ ë¹„ì–´ìˆê±°ë‚˜ ì—ëŸ¬ ë©”ì‹œì§€ë©´ ê²½ê³ 
    if not answer or answer.startswith("ë‹µë³€ ìƒì„± ì‹¤íŒ¨") or answer.startswith("Azure OpenAI ì„¤ì •"):
        print(f"  âš ï¸ [{idx}] LLM answer generation failed!")
        if not answer:
            answer = "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    total_time = time.time() - start_time
    
    # 3. Langfuse Trace & Generation
    trace_id = create_trace_and_generation(
        langfuse=langfuse,
        retriever_name=retriever.name,
        question=question,
        contexts=contexts,
        answer=answer,
        ground_truth=ground_truth,
        context_metadata=context_metadata,
        item_metadata=item_metadata,
        total_time=total_time,
        idx=idx,
        context_page_id=context_page_id
    )
    
    # 4. ê²€ìƒ‰ í’ˆì§ˆ ìŠ¤ì½”ì–´ ì¶”ê°€
    add_retrieval_quality_score(langfuse, trace_id, context_metadata)
    
    # ì§„í–‰ ìƒí™© ì¶œë ¥
    print(f"  [{idx}] {question[:50]}... ({len(contexts)}ê°œ ë¬¸ì„œ, {total_time*1000:.0f}ms)")
    
    return {
        "question": question,
        "answer": answer,
        "ground_truth": ground_truth,
        "num_contexts": len(contexts),
        "time": total_time,
        "trace_id": trace_id
    }


def evaluate_rag_with_langfuse(
    retriever: BaseRetriever,
    eval_data: List[Dict[str, Any]],
    langfuse,
    qdrant_client: QdrantClient,
    top_k: int = DEFAULT_TOP_K
) -> Dict[str, Any]:
    """Langfuse ìë™ í‰ê°€(RAGAS)ë¡œ RAG ì „ì²´ ì‹œìŠ¤í…œ í‰ê°€"""
    
    print(f"\n{'=' * 60}")
    print(f"ğŸ” {retriever.name} í‰ê°€ ì¤‘...")
    print(f"{'=' * 60}")
    
    stats = {
        "total_queries": len(eval_data),
        "total_time": 0,
        "evaluations": []
    }
    
    for idx, item in enumerate(eval_data, 1):
        eval_result = evaluate_single_query(
            retriever=retriever,
            item=item,
            langfuse=langfuse,
            idx=idx,
            top_k=top_k
        )
        
        stats["evaluations"].append(eval_result)
        stats["total_time"] += eval_result["time"]
    
    stats["avg_time"] = stats["total_time"] / stats["total_queries"]
    stats["avg_contexts"] = sum(e["num_contexts"] for e in stats["evaluations"]) / stats["total_queries"]
    
    return stats


def initialize_retrievers(
    qdrant_client: QdrantClient,
    retriever_types: Optional[List[str]] = None
) -> List[BaseRetriever]:
    """ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™”"""
    if retriever_types is None:
        return RetrieverFactory.get_all_default_retrievers(qdrant_client)
    
    return [
        RetrieverFactory.create(ret_type, qdrant_client)
        for ret_type in retriever_types
    ]


def print_comparison_results(results: List[Dict[str, Any]]):
    """ë¹„êµ ê²°ê³¼ ì¶œë ¥"""
    print("\n" + "=" * 60)
    print("ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")
    print("=" * 60)
    
    print(f"\n{'Retriever':<20} {'Avg Contexts':<15} {'Avg Time (ms)':<15}")
    print("-" * 50)
    
    for result in results:
        print(
            f"{result['retriever']:<20} "
            f"{result['avg_contexts']:<15.2f} "
            f"{result['avg_time']*1000:<15.2f}"
        )


def print_next_steps():
    """ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´ ì¶œë ¥"""
    print("\n" + "=" * 60)
    print("âœ… í‰ê°€ ì™„ë£Œ!")
    print("=" * 60)
    print(f"\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"   1. ğŸŒ Langfuse ëŒ€ì‹œë³´ë“œ: https://cloud.langfuse.com")
    print(f"   2. ğŸ“Š Traces íƒ­ì—ì„œ ìƒì„±ëœ trace í™•ì¸")
    print(f"   3. ğŸ”§ Settings â†’ Evaluations â†’ Context Recall ì„¤ì •")
    print(f"   4. âš™ï¸  Evaluations íƒ­ì—ì„œ ìë™ í‰ê°€ ê²°ê³¼ í™•ì¸")
    print(f"\nâš ï¸  ì£¼ì˜ì‚¬í•­:")
    print(f"   â€¢ generate_llm_answer() í•¨ìˆ˜ë¥¼ ì‹¤ì œ LLM API í˜¸ì¶œë¡œ êµì²´í•˜ì„¸ìš”")
    print(f"   â€¢ context_page_idê°€ ì‹¤ì œ Qdrantì˜ page_id í•„ë“œì™€ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤")


def save_evaluation_results(results: List[Dict[str, Any]], dataset_path: str):
    """í‰ê°€ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    output_file = Path(dataset_path).parent / "langfuse_rag_evaluation_stats.json"
    
    save_results = []
    for result in results:
        save_result = {k: v for k, v in result.items() if k != "evaluations"}
        save_result["num_evaluations"] = len(result.get("evaluations", []))
        save_results.append(save_result)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(save_results, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nğŸ’¾ í†µê³„ ì €ì¥: {output_file}")


def compare_retrievers_with_langfuse(
    dataset_path: str = DEFAULT_DATASET_PATH,
    top_k: int = DEFAULT_TOP_K,
    retriever_types: Optional[List[str]] = None
) -> List[Dict[str, Any]]:
    """Langfuse ìë™ í‰ê°€(RAGAS)ë¡œ ì—¬ëŸ¬ ë¦¬íŠ¸ë¦¬ë²„ ì„±ëŠ¥ ë¹„êµ"""
    
    print("=" * 60)
    print("ğŸ“Š ë¦¬íŠ¸ë¦¬ë²„ ì„±ëŠ¥ ë¹„êµ (Langfuse ìë™ í‰ê°€)")
    print("=" * 60)
    
    # Langfuse í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    langfuse = get_langfuse_client()
    if not langfuse:
        print("âŒ Langfuse í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    # Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    qdrant_client = QdrantClient(path=QDRANT_PATH)
    
    # í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ
    eval_data = load_evaluation_dataset(dataset_path)
    print(f"\nâœ… í‰ê°€ ë°ì´í„°: {len(eval_data)}ê°œ ì§ˆë¬¸")
    
    # ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™”
    print("\nğŸ“¦ ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™” ì¤‘...")
    retrievers = initialize_retrievers(qdrant_client, retriever_types)
    
    print(f"âœ… {len(retrievers)}ê°œ ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™” ì™„ë£Œ")
    for retriever in retrievers:
        print(f"   - {retriever.name}")
    
    # ê° ë¦¬íŠ¸ë¦¬ë²„ í‰ê°€
    results = []
    for retriever in retrievers:
        stats = evaluate_rag_with_langfuse(
            retriever, eval_data, langfuse, qdrant_client, top_k
        )
        results.append({"retriever": retriever.name, **stats})
    
    # Langfuse flush
    print("\nâ³ Langfuseì— ë°ì´í„° ì „ì†¡ ì¤‘...")
    langfuse.flush()
    
    # ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥
    print_comparison_results(results)
    print_next_steps()
    save_evaluation_results(results, dataset_path)
    
    return results


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Langfuse ìë™ í‰ê°€(RAGAS) ê¸°ë°˜ RAG ì„±ëŠ¥ í‰ê°€",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=f"""
ì‚¬ìš© ê°€ëŠ¥í•œ ë¦¬íŠ¸ë¦¬ë²„ íƒ€ì…:
  {', '.join(RetrieverFactory.list_available_types())}

ì˜ˆì œ:
  # ëª¨ë“  ê¸°ë³¸ ë¦¬íŠ¸ë¦¬ë²„ í‰ê°€
  python {__file__}

  # íŠ¹ì • ë¦¬íŠ¸ë¦¬ë²„ë§Œ í‰ê°€
  python {__file__} --retrievers bm25_korean dense

  # ë°ì´í„°ì…‹ ì§€ì •
  python {__file__} --dataset data/evaluation/custom_qa.json
        """
    )
    parser.add_argument(
        "--dataset",
        type=str,
        default=DEFAULT_DATASET_PATH,
        help="í‰ê°€ ë°ì´í„°ì…‹ ê²½ë¡œ"
    )
    parser.add_argument(
        "--top-k",
        type=int,
        default=DEFAULT_TOP_K,
        help="ê²€ìƒ‰í•  ìƒìœ„ kê°œ ë¬¸ì„œ"
    )
    parser.add_argument(
        "--retrievers",
        type=str,
        nargs="+",
        default=None,
        help="í‰ê°€í•  ë¦¬íŠ¸ë¦¬ë²„ íƒ€ì… (ê¸°ë³¸ê°’: ëª¨ë“  ë¦¬íŠ¸ë¦¬ë²„)"
    )
    parser.add_argument(
        "--list-retrievers",
        action="store_true",
        help="ì‚¬ìš© ê°€ëŠ¥í•œ ë¦¬íŠ¸ë¦¬ë²„ íƒ€ì… ëª©ë¡ ì¶œë ¥"
    )
    
    args = parser.parse_args()
    
    if args.list_retrievers:
        print("ì‚¬ìš© ê°€ëŠ¥í•œ ë¦¬íŠ¸ë¦¬ë²„ íƒ€ì…:")
        for ret_type in RetrieverFactory.list_available_types():
            print(f"  - {ret_type}")
        return
    
    compare_retrievers_with_langfuse(
        dataset_path=args.dataset,
        top_k=args.top_k,
        retriever_types=args.retrievers
    )


if __name__ == "__main__":
    main()