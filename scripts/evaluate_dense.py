#!/usr/bin/env python3
"""Dense Retrieval ì„±ëŠ¥ í‰ê°€ with Langfuse + Ragas"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

import json
from typing import List, Dict, Any
from qdrant_client import QdrantClient
from config.settings import QDRANT_PATH
from retrievers.dense_retriever import DenseRetriever
from utils.langfuse_utils import get_langfuse_client
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    context_precision,
    context_recall,
    faithfulness,
    answer_relevancy,
)


def load_evaluation_dataset(file_path: str) -> List[Dict[str, Any]]:
    """í‰ê°€ìš© ë°ì´í„°ì…‹ ë¡œë“œ"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def prepare_ragas_dataset(
    questions: List[str],
    ground_truths: List[str],
    contexts: List[List[str]],
    answers: List[str]
) -> Dataset:
    """
    Ragas í‰ê°€ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹ ì¤€ë¹„

    Args:
        questions: ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        ground_truths: ì •ë‹µ ë¦¬ìŠ¤íŠ¸
        contexts: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        answers: ìƒì„±ëœ ë‹µë³€ ë¦¬ìŠ¤íŠ¸

    Returns:
        Ragas Dataset
    """
    data = {
        "question": questions,
        "ground_truth": ground_truths,
        "contexts": contexts,
        "answer": answers,
    }

    return Dataset.from_dict(data)


def evaluate_dense_retrieval(
    dataset_path: str = "data/evaluation/sample_qa.json",
    top_k: int = 5
):
    """
    Dense ê²€ìƒ‰ ì„±ëŠ¥ì„ Ragasë¡œ í‰ê°€í•˜ê³  Langfuseì— ë¡œê¹…

    Args:
        dataset_path: í‰ê°€ ë°ì´í„°ì…‹ ê²½ë¡œ
        top_k: ê²€ìƒ‰í•  ìƒìœ„ kê°œ ë¬¸ì„œ
    """
    print("=" * 60)
    print("ğŸ” Dense Retrieval í‰ê°€ ì‹œì‘")
    print("=" * 60)

    # Langfuse í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    langfuse = get_langfuse_client()

    # Qdrant í´ë¼ì´ì–¸íŠ¸ ë° Dense Retriever ì´ˆê¸°í™”
    qdrant_client = QdrantClient(path=QDRANT_PATH)
    retriever = DenseRetriever(qdrant_client)

    # í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ
    eval_data = load_evaluation_dataset(dataset_path)
    print(f"\nğŸ“Š í‰ê°€ ë°ì´í„°: {len(eval_data)}ê°œ ì§ˆë¬¸")

    # ê²€ìƒ‰ ìˆ˜í–‰
    questions = []
    ground_truths = []
    contexts_list = []
    answers = []

    for item in eval_data:
        question = item["question"]
        ground_truth = item["ground_truth"]

        # Dense ê²€ìƒ‰
        search_results = retriever.search(question, top_k=top_k)

        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        contexts = [result.combined_text for result in search_results]

        # ê²€ìƒ‰ ê²°ê³¼ì˜ ì²« ë²ˆì§¸ë¥¼ ë‹µë³€ìœ¼ë¡œ ì‚¬ìš©
        answer = contexts[0] if contexts else "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"

        questions.append(question)
        ground_truths.append(ground_truth)
        contexts_list.append(contexts)
        answers.append(answer)

        print(f"\nì§ˆë¬¸: {question}")
        print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(contexts)}")
        print(f"Top 1 Score: {search_results[0].score if search_results else 0:.4f}")

    # Ragas í‰ê°€ ë°ì´í„°ì…‹ ì¤€ë¹„
    print("\n" + "=" * 60)
    print("ğŸ“ˆ Ragas í‰ê°€ ì‹œì‘")
    print("=" * 60)

    ragas_dataset = prepare_ragas_dataset(
        questions=questions,
        ground_truths=ground_truths,
        contexts=contexts_list,
        answers=answers
    )

    # Ragas í‰ê°€ ì‹¤í–‰
    # Context-based metricsë§Œ ì‚¬ìš© (DenseëŠ” ê²€ìƒ‰ë§Œ ìˆ˜í–‰í•˜ë¯€ë¡œ)
    metrics = [
        context_precision,
        context_recall,
    ]

    # LLMì´ í•„ìš”í•œ ë©”íŠ¸ë¦­ì„ ì‚¬ìš©í•˜ë ¤ë©´ OpenAI API í‚¤ ë“±ì´ í•„ìš”
    # faithfulnessì™€ answer_relevancyëŠ” LLMì„ ì‚¬ìš©í•˜ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬
    # metrics.extend([faithfulness, answer_relevancy])

    try:
        results = evaluate(
            ragas_dataset,
            metrics=metrics,
        )

        print("\n" + "=" * 60)
        print("âœ… í‰ê°€ ê²°ê³¼")
        print("=" * 60)
        print(f"Context Precision: {results['context_precision']:.4f}")
        print(f"Context Recall: {results['context_recall']:.4f}")

        # Langfuseì— í‰ê°€ ê²°ê³¼ ë¡œê¹…
        if langfuse:
            print("\nğŸ“Š Langfuseì— ê²°ê³¼ ì—…ë¡œë“œ ì¤‘...")

            # Score ë¡œê¹…
            for metric_name, score in results.items():
                if isinstance(score, (int, float)):
                    langfuse.score(
                        name=f"dense_{metric_name}",
                        value=score,
                        data_type="numeric",
                        comment=f"Dense Retrieval - {metric_name}"
                    )

            langfuse.flush()
            print("âœ… Langfuse ì—…ë¡œë“œ ì™„ë£Œ")

        return results

    except Exception as e:
        print(f"\nâŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("\nğŸ’¡ Tip: LLM ê¸°ë°˜ ë©”íŠ¸ë¦­(faithfulness, answer_relevancy)ì„ ì‚¬ìš©í•˜ë ¤ë©´")
        print("   OPENAI_API_KEY ë“±ì„ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        return None


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description="Dense ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€")
    parser.add_argument(
        "--dataset",
        type=str,
        default="data/evaluation/sample_qa.json",
        help="í‰ê°€ ë°ì´í„°ì…‹ ê²½ë¡œ"
    )
    parser.add_argument(
        "--top-k",
        type=int,
        default=5,
        help="ê²€ìƒ‰í•  ìƒìœ„ kê°œ ë¬¸ì„œ"
    )

    args = parser.parse_args()

    evaluate_dense_retrieval(
        dataset_path=args.dataset,
        top_k=args.top_k
    )


if __name__ == "__main__":
    main()
