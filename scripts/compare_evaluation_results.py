#!/usr/bin/env python3
"""í‰ê°€ ê²°ê³¼ ë¹„êµ (CSV íŒŒì¼ ê¸°ë°˜)

ë‘ ê°œì˜ Langfuse CSV í‰ê°€ ê²°ê³¼ íŒŒì¼ì„ ë¡œë“œí•˜ì—¬
ë©”íŠ¸ë¦­ ë³„ë¡œ ì„±ëŠ¥ì„ ë¹„êµí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

import pandas as pd
import numpy as np
from typing import Dict, Any, List
from collections import defaultdict
import json


def load_csv_data(csv_path: str) -> pd.DataFrame:
    """
    CSV íŒŒì¼ ë¡œë“œ

    Args:
        csv_path: CSV íŒŒì¼ ê²½ë¡œ

    Returns:
        pandas DataFrame
    """
    print(f"ğŸ“‚ íŒŒì¼ ë¡œë”© ì¤‘: {csv_path}")
    df = pd.read_csv(csv_path)
    print(f"   âœ… {len(df)} í–‰ ë¡œë“œë¨")
    return df


def extract_metrics_by_trace(df: pd.DataFrame) -> Dict[str, Dict[str, float]]:
    """
    Traceë³„ë¡œ ë©”íŠ¸ë¦­ ì¶”ì¶œ

    Args:
        df: Langfuse ë°ì´í„°í”„ë ˆì„

    Returns:
        {trace_id: {metric_name: value}} ë”•ì…”ë„ˆë¦¬
    """
    trace_metrics = defaultdict(dict)

    for _, row in df.iterrows():
        trace_id = row['traceId']
        metric_name = row['name']

        # valueê°€ ìˆ«ìí˜•ì¸ ê²½ìš°ë§Œ ì²˜ë¦¬
        if pd.notna(row['value']):
            try:
                metric_value = float(row['value'])
                trace_metrics[trace_id][metric_name] = metric_value
            except (ValueError, TypeError):
                pass

    return dict(trace_metrics)


def calculate_statistics(values: List[float]) -> Dict[str, float]:
    """
    í†µê³„ ê³„ì‚°

    Args:
        values: ê°’ ë¦¬ìŠ¤íŠ¸

    Returns:
        í†µê³„ ë”•ì…”ë„ˆë¦¬ (í‰ê· , ìµœì†Œ, ìµœëŒ€, ì¤‘ì•™ê°’, í‘œì¤€í¸ì°¨)
    """
    if not values:
        return {
            "count": 0,
            "avg": 0.0,
            "min": 0.0,
            "max": 0.0,
            "median": 0.0,
            "std": 0.0
        }

    return {
        "count": len(values),
        "avg": np.mean(values),
        "min": np.min(values),
        "max": np.max(values),
        "median": np.median(values),
        "std": np.std(values)
    }


def analyze_single_file(df: pd.DataFrame, file_name: str) -> Dict[str, Any]:
    """
    ë‹¨ì¼ CSV íŒŒì¼ ë¶„ì„

    Args:
        df: ë°ì´í„°í”„ë ˆì„
        file_name: íŒŒì¼ ì´ë¦„

    Returns:
        ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    print(f"\n{'=' * 60}")
    print(f"ğŸ“Š {file_name} ë¶„ì„ ì¤‘...")
    print(f"{'=' * 60}")

    # Traceë³„ ë©”íŠ¸ë¦­ ì¶”ì¶œ
    trace_metrics = extract_metrics_by_trace(df)

    # ë©”íŠ¸ë¦­ë³„ í†µê³„ ê³„ì‚°
    metrics_summary = defaultdict(list)

    for trace_id, metrics in trace_metrics.items():
        for metric_name, value in metrics.items():
            metrics_summary[metric_name].append(value)

    # í†µê³„ ê³„ì‚°
    stats = {}
    for metric_name, values in metrics_summary.items():
        stats[metric_name] = calculate_statistics(values)

    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“ˆ ì´ Trace ìˆ˜: {len(trace_metrics)}")
    print(f"ğŸ“ˆ ì´ í‰ê°€ í•­ëª© ìˆ˜: {len(df)}")

    if stats:
        print(f"\në©”íŠ¸ë¦­ í†µê³„:")
        for metric_name, metric_stats in sorted(stats.items()):
            print(f"\n   {metric_name}:")
            print(f"      ê°œìˆ˜: {metric_stats['count']}")
            print(f"      í‰ê· : {metric_stats['avg']:.4f}")
            print(f"      ì¤‘ì•™ê°’: {metric_stats['median']:.4f}")
            print(f"      í‘œì¤€í¸ì°¨: {metric_stats['std']:.4f}")
            print(f"      ë²”ìœ„: {metric_stats['min']:.4f} ~ {metric_stats['max']:.4f}")

    return {
        "file_name": file_name,
        "total_traces": len(trace_metrics),
        "total_evaluations": len(df),
        "metrics": stats,
        "trace_metrics": trace_metrics
    }


def compare_two_files(
    result1: Dict[str, Any],
    result2: Dict[str, Any]
) -> None:
    """
    ë‘ íŒŒì¼ì˜ ê²°ê³¼ ë¹„êµ ë° ì¶œë ¥

    Args:
        result1: ì²« ë²ˆì§¸ íŒŒì¼ ë¶„ì„ ê²°ê³¼
        result2: ë‘ ë²ˆì§¸ íŒŒì¼ ë¶„ì„ ê²°ê³¼
    """
    print("\n" + "=" * 80)
    print("ğŸ† ë‘ íŒŒì¼ ë¹„êµ ê²°ê³¼")
    print("=" * 80)

    # ê¸°ë³¸ í†µê³„ ë¹„êµ
    print(f"\n{'í•­ëª©':<30} {result1['file_name']:<25} {result2['file_name']:<25}")
    print("-" * 80)
    print(f"{'ì´ Trace ìˆ˜':<30} {result1['total_traces']:<25} {result2['total_traces']:<25}")
    print(f"{'ì´ í‰ê°€ í•­ëª© ìˆ˜':<30} {result1['total_evaluations']:<25} {result2['total_evaluations']:<25}")

    # ë©”íŠ¸ë¦­ë³„ ë¹„êµ
    all_metrics = set(result1["metrics"].keys()) | set(result2["metrics"].keys())

    if all_metrics:
        print("\n" + "=" * 80)
        print("ğŸ“Š ë©”íŠ¸ë¦­ë³„ ë¹„êµ")
        print("=" * 80)

        for metric_name in sorted(all_metrics):
            print(f"\n[{metric_name}]")
            print(f"{'í†µê³„':<20} {result1['file_name']:<25} {result2['file_name']:<25} {'ì°¨ì´':<15}")
            print("-" * 85)

            stats1 = result1["metrics"].get(metric_name, {})
            stats2 = result2["metrics"].get(metric_name, {})

            if stats1 and stats2:
                # ê°œìˆ˜
                print(f"{'ê°œìˆ˜':<20} {stats1.get('count', 0):<25} {stats2.get('count', 0):<25} {stats2.get('count', 0) - stats1.get('count', 0):<15}")

                # í‰ê· 
                avg1 = stats1.get('avg', 0)
                avg2 = stats2.get('avg', 0)
                diff = avg2 - avg1
                diff_pct = (diff / avg1 * 100) if avg1 != 0 else 0
                print(f"{'í‰ê· ':<20} {avg1:<25.4f} {avg2:<25.4f} {diff:+.4f} ({diff_pct:+.2f}%)")

                # ì¤‘ì•™ê°’
                med1 = stats1.get('median', 0)
                med2 = stats2.get('median', 0)
                diff = med2 - med1
                print(f"{'ì¤‘ì•™ê°’':<20} {med1:<25.4f} {med2:<25.4f} {diff:+.4f}")

                # í‘œì¤€í¸ì°¨
                std1 = stats1.get('std', 0)
                std2 = stats2.get('std', 0)
                diff = std2 - std1
                print(f"{'í‘œì¤€í¸ì°¨':<20} {std1:<25.4f} {std2:<25.4f} {diff:+.4f}")

                # ìµœì†Œ/ìµœëŒ€
                print(f"{'ìµœì†Œê°’':<20} {stats1.get('min', 0):<25.4f} {stats2.get('min', 0):<25.4f}")
                print(f"{'ìµœëŒ€ê°’':<20} {stats1.get('max', 0):<25.4f} {stats2.get('max', 0):<25.4f}")
            else:
                if stats1:
                    print(f"   âš ï¸  {result2['file_name']}ì— '{metric_name}' ë©”íŠ¸ë¦­ ì—†ìŒ")
                else:
                    print(f"   âš ï¸  {result1['file_name']}ì— '{metric_name}' ë©”íŠ¸ë¦­ ì—†ìŒ")

    # ê³µí†µ Trace ë¶„ì„
    traces1 = set(result1["trace_metrics"].keys())
    traces2 = set(result2["trace_metrics"].keys())

    common_traces = traces1 & traces2
    only_in_1 = traces1 - traces2
    only_in_2 = traces2 - traces1

    print("\n" + "=" * 80)
    print("ğŸ” Trace ë¹„êµ")
    print("=" * 80)
    print(f"ê³µí†µ Trace ìˆ˜: {len(common_traces)}")
    print(f"{result1['file_name']}ì—ë§Œ ìˆëŠ” Trace: {len(only_in_1)}")
    print(f"{result2['file_name']}ì—ë§Œ ìˆëŠ” Trace: {len(only_in_2)}")

    # ê³µí†µ Traceì— ëŒ€í•œ ë©”íŠ¸ë¦­ ì°¨ì´ ë¶„ì„
    if common_traces and all_metrics:
        print("\n" + "=" * 80)
        print("ğŸ“ˆ ê³µí†µ Trace ë©”íŠ¸ë¦­ ê°œì„ /ì €í•˜ ë¶„ì„")
        print("=" * 80)

        for metric_name in sorted(all_metrics):
            improvements = 0
            degradations = 0
            unchanged = 0

            for trace_id in common_traces:
                val1 = result1["trace_metrics"][trace_id].get(metric_name)
                val2 = result2["trace_metrics"][trace_id].get(metric_name)

                if val1 is not None and val2 is not None:
                    if val2 > val1:
                        improvements += 1
                    elif val2 < val1:
                        degradations += 1
                    else:
                        unchanged += 1

            if improvements + degradations + unchanged > 0:
                print(f"\n{metric_name}:")
                print(f"   ê°œì„ : {improvements} ({improvements/len(common_traces)*100:.1f}%)")
                print(f"   ì €í•˜: {degradations} ({degradations/len(common_traces)*100:.1f}%)")
                print(f"   ë™ì¼: {unchanged} ({unchanged/len(common_traces)*100:.1f}%)")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(
        description="ë‘ ê°œì˜ Langfuse CSV í‰ê°€ ê²°ê³¼ íŒŒì¼ ë¹„êµ",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument(
        "file1",
        type=str,
        help="ì²« ë²ˆì§¸ CSV íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "file2",
        type=str,
        help="ë‘ ë²ˆì§¸ CSV íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ (JSON í˜•ì‹)"
    )

    args = parser.parse_args()

    # CSV íŒŒì¼ ë¡œë“œ
    df1 = load_csv_data(args.file1)
    df2 = load_csv_data(args.file2)

    # ê° íŒŒì¼ ë¶„ì„
    result1 = analyze_single_file(df1, Path(args.file1).name)
    result2 = analyze_single_file(df2, Path(args.file2).name)

    # ë‘ íŒŒì¼ ë¹„êµ
    compare_two_files(result1, result2)

    # ê²°ê³¼ ì €ì¥ (ì„ íƒì )
    if args.output:
        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        save_data = {
            "file1": {
                "name": result1["file_name"],
                "total_traces": result1["total_traces"],
                "total_evaluations": result1["total_evaluations"],
                "metrics": result1["metrics"]
            },
            "file2": {
                "name": result2["file_name"],
                "total_traces": result2["total_traces"],
                "total_evaluations": result2["total_evaluations"],
                "metrics": result2["metrics"]
            }
        }

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False, default=str)

        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")


if __name__ == "__main__":
    main()
