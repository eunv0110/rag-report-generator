#!/usr/bin/env python3
"""Vector DB êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸ (ì¦ë¶„ ì—…ë°ì´íŠ¸ ì§€ì›)"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from config.settings import *
from models.embeddings.factory import get_embedder
from models.vision.factory import get_vision_model
from core.data_collector import NotionDataSourceCollector
from core.chunker import process_page_data
from core.vector_store import (
    init_qdrant, 
    store_to_qdrant, 
    check_qdrant_data,
    delete_page_from_qdrant
)
from services.incremental_sync import (
    check_existing_data,
    collect_missing_pages,
    update_changed_pages
)
from utils.file_utils import save_json, load_json
from qdrant_client import QdrantClient

def main(force_recreate: bool = False, check_updates: bool = True, limit: int = None):
    """
    Vector DB êµ¬ì¶• ë©”ì¸ í•¨ìˆ˜
    
    Args:
        force_recreate: Trueë©´ ì „ì²´ ì¬ìƒì„±
        check_updates: Trueë©´ ìˆ˜ì •ëœ í˜ì´ì§€ë„ í™•ì¸
        limit: ì²˜ë¦¬í•  í˜ì´ì§€ ìˆ˜ ì œí•œ (Noneì´ë©´ ì „ì²´)
    """
    print("=" * 60)
    print("ğŸš€ Vector DB êµ¬ì¶• ì‹œì‘")
    if limit:
        print(f"ğŸ“Š ì œí•œ: {limit}ê°œ í˜ì´ì§€ë§Œ ì²˜ë¦¬")
    print("=" * 60)
    
    data_file = DATA_DIR / "notion_data.json"
    
    # 1. ëª¨ë¸ ì´ˆê¸°í™”
    print("\nğŸ“¦ ëª¨ë¸ ë¡œë”©...")
    embedder = get_embedder()
    vision_model = get_vision_model()
    qdrant_client = QdrantClient(path=QDRANT_PATH)
    
    # 2. ë°ì´í„° ìˆ˜ì§‘ (ì¦ë¶„)
    if force_recreate:
        print("\nâ™»ï¸ ì „ì²´ ì¬ìˆ˜ì§‘ ëª¨ë“œ")
        if data_file.exists():
            from datetime import datetime
            backup = data_file.with_suffix(f".backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
            data_file.rename(backup)
            print(f"   ë°±ì—…: {backup}")
        
        collector = NotionDataSourceCollector(NOTION_TOKEN, DATA_SOURCE_ID)
        all_data = collector.collect_all(limit=limit)  # âœ… limit ì¶”ê°€
        save_json(all_data, str(data_file))
        pages_to_index = all_data
        
    else:
        # ê¸°ì¡´ ë°ì´í„° í™•ì¸
        existing_info = check_existing_data(str(data_file))
        
        if not existing_info["exists"]:
            print("\nğŸ“¥ ì´ˆê¸° ìˆ˜ì§‘ ì‹œì‘...")
            collector = NotionDataSourceCollector(NOTION_TOKEN, DATA_SOURCE_ID)
            all_data = collector.collect_all(limit=limit)  # âœ… limit ì¶”ê°€
            save_json(all_data, str(data_file))
            pages_to_index = all_data
            
        else:
            # ìƒˆ í˜ì´ì§€ ìˆ˜ì§‘
            collector = NotionDataSourceCollector(NOTION_TOKEN, DATA_SOURCE_ID)
            new_data = collect_missing_pages(
                collector, 
                existing_info["page_ids"], 
                str(data_file),
                limit=limit  # âœ… limit ì¶”ê°€
            )
            
            # ìˆ˜ì •ëœ í˜ì´ì§€ ì—…ë°ì´íŠ¸
            updated_page_ids = set()
            if check_updates:
                all_data = load_json(str(data_file))
                old_data = all_data.copy()
                all_data = update_changed_pages(collector, all_data, str(data_file))
                
                # ì–´ë–¤ í˜ì´ì§€ê°€ ì—…ë°ì´íŠ¸ëëŠ”ì§€ ì¶”ì 
                for old, new in zip(old_data, all_data):
                    if old.get("last_edited_time") != new.get("last_edited_time"):
                        updated_page_ids.add(new["page_id"])
            else:
                all_data = load_json(str(data_file))
            
            # ì¸ë±ì‹±í•  í˜ì´ì§€ ê²°ì •
            pages_to_index = [
                p for p in all_data 
                if p["page_id"] in updated_page_ids or 
                   p in new_data
            ]
    
    if not pages_to_index:
        print("\nâœ… ì¸ë±ì‹±í•  í˜ì´ì§€ ì—†ìŒ (ëª¨ë‘ ìµœì‹  ìƒíƒœ)")
        return
    
    # âœ… limit ì ìš©
    if limit and len(pages_to_index) > limit:
        print(f"\nâš ï¸  {len(pages_to_index)}ê°œ í˜ì´ì§€ ì¤‘ {limit}ê°œë§Œ ì²˜ë¦¬")
        pages_to_index = pages_to_index[:limit]
    
    print(f"\nğŸ“ {len(pages_to_index)}ê°œ í˜ì´ì§€ ì¸ë±ì‹±...")
    
    # 3. Qdrant ì´ˆê¸°í™”
    qdrant_info = check_qdrant_data(qdrant_client)
    
    if force_recreate or not qdrant_info["exists"]:
        # ì „ì²´ ì¬ìƒì„±
        all_chunks = []
        for page in pages_to_index:
            chunks = process_page_data(page, embedder, vision_model)
            all_chunks.extend(chunks)
            print(f"  {page.get('title', 'Untitled')}: {len(chunks)}ê°œ ì²­í¬")
        
        if all_chunks:
            print(f"\nğŸ”¢ ì„ë² ë”© ìƒì„± ì¤‘...")
            texts = [c.combined_text for c in all_chunks]
            embeddings = embedder.embed_texts(texts)
            
            print(f"\nğŸ’¾ Qdrant ì €ì¥ ì¤‘...")
            init_qdrant(qdrant_client, dimension=len(embeddings[0]), recreate=force_recreate)
            store_to_qdrant(all_chunks, embeddings, qdrant_client)
    else:
        # ì¦ë¶„ ì—…ë°ì´íŠ¸: ë³€ê²½ëœ í˜ì´ì§€ë§Œ ì¬ì¸ë±ì‹±
        print("\nğŸ”„ ë³€ê²½ëœ í˜ì´ì§€ ì¬ì¸ë±ì‹±...")
        
        for page in pages_to_index:
            page_id = page["page_id"]
            
            # ê¸°ì¡´ ì²­í¬ ì‚­ì œ
            delete_page_from_qdrant(qdrant_client, page_id)
            
            # ìƒˆ ì²­í¬ ìƒì„±
            chunks = process_page_data(page, embedder, vision_model)
            
            if chunks:
                texts = [c.combined_text for c in chunks]
                embeddings = embedder.embed_texts(texts)
                store_to_qdrant(chunks, embeddings, qdrant_client)
                
                print(f"  âœ… {page.get('title', 'Untitled')}: {len(chunks)}ê°œ ì²­í¬ ì—…ë°ì´íŠ¸")
    
    print("\n" + "=" * 60)
    print("ğŸ‰ Vector DB êµ¬ì¶• ì™„ë£Œ!")
    print("=" * 60)

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--force", action="store_true", help="ì „ì²´ ì¬ìƒì„±")
    parser.add_argument("--no-updates", action="store_true", help="ìˆ˜ì • ì²´í¬ ì•ˆ í•¨")
    parser.add_argument("--limit", type=int, default=None, help="ì²˜ë¦¬í•  í˜ì´ì§€ ìˆ˜ ì œí•œ")  # âœ… ì¶”ê°€
    args = parser.parse_args()
    
    main(force_recreate=args.force, check_updates=not args.no_updates, limit=args.limit)