#!/usr/bin/env python3
"""í‰ê°€ ê²°ê³¼ ë¹„êµ (CSV íŒŒì¼ ê¸°ë°˜)

ë‘ ê°œì˜ Langfuse CSV í‰ê°€ ê²°ê³¼ íŒŒì¼ì„ ë¡œë“œí•˜ì—¬
ë©”íŠ¸ë¦­ ë³„ë¡œ ì„±ëŠ¥ì„ ë¹„êµí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

import pandas as pd
import numpy as np
from typing import Dict, Any, List
from collections import defaultdict
import json

# ====================================================================
# ë°ì´í„° ê²½ë¡œ ì„¤ì • (ì—¬ê¸°ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”)
# ====================================================================
FILE1_PATH = "/home/work/rag/Project/rag-report-generator/data/langfuse/qwen/eval_rrf_multiquery_rrf_multiquery_20251226_qwen_v3.csv"
FILE2_PATH = "/home/work/rag/Project/rag-report-generator/data/langfuse/qwen/stage4.csv"
FILE3_PATH = "/home/work/rag/Project/rag-report-generator/data/langfuse/qwen/eval_rrf_multiquery_longcontext_rrf_multiquery_longcontext_20251226_qwen_v3.csv"
FILE4_PATH = "/home/work/rag/Project/rag-report-generator/data/langfuse/qwen/eval_ensemble_rrf_ensemble_rrf_20251226_qwen_v3.csv"

OUTPUT_PATH = None  # ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥í•˜ë ¤ë©´ ê²½ë¡œ ì§€ì • (ì˜ˆ: "results/comparison.json")


def load_csv_data(csv_path: str) -> pd.DataFrame:
    """
    CSV íŒŒì¼ ë¡œë“œ

    Args:
        csv_path: CSV íŒŒì¼ ê²½ë¡œ

    Returns:
        pandas DataFrame
    """
    print(f"ğŸ“‚ íŒŒì¼ ë¡œë”© ì¤‘: {csv_path}")

    # ì—¬ëŸ¬ ì¸ì½”ë”©ì„ ì‹œë„
    encodings = ['utf-8', 'cp949', 'euc-kr', 'latin1']

    for encoding in encodings:
        try:
            df = pd.read_csv(csv_path, encoding=encoding)
            print(f"   âœ… {len(df)} í–‰ ë¡œë“œë¨ (ì¸ì½”ë”©: {encoding})")
            return df
        except UnicodeDecodeError:
            continue

    # ëª¨ë“  ì¸ì½”ë”© ì‹¤íŒ¨ì‹œ ì—ëŸ¬
    raise ValueError(f"íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œë„í•œ ì¸ì½”ë”©: {encodings}")



def extract_metrics_by_trace(df: pd.DataFrame) -> Dict[str, Dict[str, float]]:
    """
    Traceë³„ë¡œ ë©”íŠ¸ë¦­ ì¶”ì¶œ

    Args:
        df: Langfuse ë°ì´í„°í”„ë ˆì„

    Returns:
        {trace_id: {metric_name: value}} ë”•ì…”ë„ˆë¦¬
    """
    trace_metrics = defaultdict(dict)

    for _, row in df.iterrows():
        trace_id = row['traceId']
        metric_name = row['name']

        # valueê°€ ìˆ«ìí˜•ì¸ ê²½ìš°ë§Œ ì²˜ë¦¬
        if pd.notna(row['value']):
            try:
                metric_value = float(row['value'])
                trace_metrics[trace_id][metric_name] = metric_value
            except (ValueError, TypeError):
                pass

    return dict(trace_metrics)


def calculate_statistics(values: List[float]) -> Dict[str, float]:
    """
    í†µê³„ ê³„ì‚°

    Args:
        values: ê°’ ë¦¬ìŠ¤íŠ¸

    Returns:
        í†µê³„ ë”•ì…”ë„ˆë¦¬ (í‰ê· , ìµœì†Œ, ìµœëŒ€, ì¤‘ì•™ê°’, í‘œì¤€í¸ì°¨)
    """
    if not values:
        return {
            "count": 0,
            "avg": 0.0,
            "min": 0.0,
            "max": 0.0,
            "median": 0.0,
            "std": 0.0
        }

    return {
        "count": len(values),
        "avg": np.mean(values),
        "min": np.min(values),
        "max": np.max(values),
        "median": np.median(values),
        "std": np.std(values)
    }


def analyze_single_file(df: pd.DataFrame, file_name: str) -> Dict[str, Any]:
    """
    ë‹¨ì¼ CSV íŒŒì¼ ë¶„ì„

    Args:
        df: ë°ì´í„°í”„ë ˆì„
        file_name: íŒŒì¼ ì´ë¦„

    Returns:
        ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    print(f"\n{'=' * 60}")
    print(f"ğŸ“Š {file_name} ë¶„ì„ ì¤‘...")
    print(f"{'=' * 60}")

    # Traceë³„ ë©”íŠ¸ë¦­ ì¶”ì¶œ
    trace_metrics = extract_metrics_by_trace(df)

    # ë©”íŠ¸ë¦­ë³„ í†µê³„ ê³„ì‚°
    metrics_summary = defaultdict(list)

    for trace_id, metrics in trace_metrics.items():
        for metric_name, value in metrics.items():
            metrics_summary[metric_name].append(value)

    # í†µê³„ ê³„ì‚°
    stats = {}
    for metric_name, values in metrics_summary.items():
        stats[metric_name] = calculate_statistics(values)

    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“ˆ ì´ Trace ìˆ˜: {len(trace_metrics)}")
    print(f"ğŸ“ˆ ì´ í‰ê°€ í•­ëª© ìˆ˜: {len(df)}")

    if stats:
        print(f"\në©”íŠ¸ë¦­ í†µê³„:")
        for metric_name, metric_stats in sorted(stats.items()):
            print(f"\n   {metric_name}:")
            print(f"      ê°œìˆ˜: {metric_stats['count']}")
            print(f"      í‰ê· : {metric_stats['avg']:.4f}")
            print(f"      ì¤‘ì•™ê°’: {metric_stats['median']:.4f}")
            print(f"      í‘œì¤€í¸ì°¨: {metric_stats['std']:.4f}")
            print(f"      ë²”ìœ„: {metric_stats['min']:.4f} ~ {metric_stats['max']:.4f}")

    return {
        "file_name": file_name,
        "total_traces": len(trace_metrics),
        "total_evaluations": len(df),
        "metrics": stats,
        "trace_metrics": trace_metrics
    }


def compare_four_files(
    result1: Dict[str, Any],
    result2: Dict[str, Any],
    result3: Dict[str, Any],
    result4: Dict[str, Any]
) -> None:
    """
    4ê°œ íŒŒì¼ì˜ ê²°ê³¼ë¥¼ í•œ ë²ˆì— ë¹„êµ ë° ì¶œë ¥

    Args:
        result1: ì²« ë²ˆì§¸ íŒŒì¼ ë¶„ì„ ê²°ê³¼
        result2: ë‘ ë²ˆì§¸ íŒŒì¼ ë¶„ì„ ê²°ê³¼
        result3: ì„¸ ë²ˆì§¸ íŒŒì¼ ë¶„ì„ ê²°ê³¼
        result4: ë„¤ ë²ˆì§¸ íŒŒì¼ ë¶„ì„ ê²°ê³¼
    """
    results = [result1, result2, result3, result4]

    print("\n" + "=" * 150)
    print("ğŸ† 4ê°œ íŒŒì¼ ë¹„êµ ê²°ê³¼")
    print("=" * 150)

    # ê¸°ë³¸ í†µê³„ ë¹„êµ
    print(f"\n{'í•­ëª©':<30} {result1['file_name']:<28} {result2['file_name']:<28} {result3['file_name']:<28} {result4['file_name']:<28}")
    print("-" * 150)
    print(f"{'ì´ Trace ìˆ˜':<30} {result1['total_traces']:<28} {result2['total_traces']:<28} {result3['total_traces']:<28} {result4['total_traces']:<28}")
    print(f"{'ì´ í‰ê°€ í•­ëª© ìˆ˜':<30} {result1['total_evaluations']:<28} {result2['total_evaluations']:<28} {result3['total_evaluations']:<28} {result4['total_evaluations']:<28}")

    # ë©”íŠ¸ë¦­ë³„ ë¹„êµ
    all_metrics = set()
    for result in results:
        all_metrics.update(result["metrics"].keys())

    if all_metrics:
        print("\n" + "=" * 150)
        print("ğŸ“Š ë©”íŠ¸ë¦­ë³„ ë¹„êµ")
        print("=" * 150)

        for metric_name in sorted(all_metrics):
            print(f"\n[{metric_name}]")

            # í—¤ë” ì¶œë ¥
            print(f"{'í†µê³„':<20} {result1['file_name']:<28} {result2['file_name']:<28} {result3['file_name']:<28} {result4['file_name']:<28}")
            print("-" * 150)

            # ê° í†µê³„ í•­ëª©ë³„ë¡œ 4ê°œ íŒŒì¼ ë¹„êµ
            stats_list = [result["metrics"].get(metric_name, {}) for result in results]

            # ê°œìˆ˜
            counts = [stats.get('count', 0) for stats in stats_list]
            print(f"{'ê°œìˆ˜':<20} {counts[0]:<28} {counts[1]:<28} {counts[2]:<28} {counts[3]:<28}")

            # í‰ê· 
            avgs = [stats.get('avg', 0) for stats in stats_list]
            avg_str = [f"{avg:.4f}" for avg in avgs]
            print(f"{'í‰ê· ':<20} {avg_str[0]:<28} {avg_str[1]:<28} {avg_str[2]:<28} {avg_str[3]:<28}")

            # ìµœê³  í‰ê·  ì°¾ê¸°
            if any(stats_list):
                max_avg = max(avgs)
                max_idx = avgs.index(max_avg)
                print(f"{'  â†’ ìµœê³  í‰ê· ':<20} {results[max_idx]['file_name']} ({max_avg:.4f})")

            # ì¤‘ì•™ê°’
            medians = [stats.get('median', 0) for stats in stats_list]
            med_str = [f"{med:.4f}" for med in medians]
            print(f"{'ì¤‘ì•™ê°’':<20} {med_str[0]:<28} {med_str[1]:<28} {med_str[2]:<28} {med_str[3]:<28}")

            # í‘œì¤€í¸ì°¨
            stds = [stats.get('std', 0) for stats in stats_list]
            std_str = [f"{std:.4f}" for std in stds]
            print(f"{'í‘œì¤€í¸ì°¨':<20} {std_str[0]:<28} {std_str[1]:<28} {std_str[2]:<28} {std_str[3]:<28}")

            # ìµœì†Œê°’
            mins = [stats.get('min', 0) for stats in stats_list]
            min_str = [f"{m:.4f}" for m in mins]
            print(f"{'ìµœì†Œê°’':<20} {min_str[0]:<28} {min_str[1]:<28} {min_str[2]:<28} {min_str[3]:<28}")

            # ìµœëŒ€ê°’
            maxs = [stats.get('max', 0) for stats in stats_list]
            max_str = [f"{m:.4f}" for m in maxs]
            print(f"{'ìµœëŒ€ê°’':<20} {max_str[0]:<28} {max_str[1]:<28} {max_str[2]:<28} {max_str[3]:<28}")

    # Trace ë¹„êµ
    all_traces = [set(result["trace_metrics"].keys()) for result in results]
    common_traces = all_traces[0].intersection(*all_traces[1:])

    print("\n" + "=" * 150)
    print("ğŸ” Trace ë¹„êµ")
    print("=" * 150)
    print(f"ì „ì²´ ê³µí†µ Trace ìˆ˜: {len(common_traces)}")
    for i, result in enumerate(results, 1):
        print(f"{result['file_name']}: {len(all_traces[i-1])} Traces")

    # ê³µí†µ Traceì— ëŒ€í•œ ë©”íŠ¸ë¦­ ìˆœìœ„ ë¶„ì„
    if common_traces and all_metrics:
        print("\n" + "=" * 150)
        print("ğŸ“ˆ ê³µí†µ Trace ë©”íŠ¸ë¦­ ìˆœìœ„ (í‰ê·  ê¸°ì¤€)")
        print("=" * 150)

        ranking = {}
        for metric_name in sorted(all_metrics):
            metric_avgs = []
            for result in results:
                if metric_name in result["metrics"]:
                    metric_avgs.append({
                        "file_name": result["file_name"],
                        "avg": result["metrics"][metric_name].get("avg", 0)
                    })

            # í‰ê· ê°’ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
            metric_avgs.sort(key=lambda x: x["avg"], reverse=True)
            ranking[metric_name] = metric_avgs

        for metric_name, ranks in ranking.items():
            print(f"\n{metric_name}:")
            for i, rank_data in enumerate(ranks, 1):
                print(f"   {i}ìœ„: {rank_data['file_name']:<30} (í‰ê· : {rank_data['avg']:.4f})")


def compare_two_files(
    result1: Dict[str, Any],
    result2: Dict[str, Any]
) -> None:
    """
    ë‘ íŒŒì¼ì˜ ê²°ê³¼ ë¹„êµ ë° ì¶œë ¥

    Args:
        result1: ì²« ë²ˆì§¸ íŒŒì¼ ë¶„ì„ ê²°ê³¼
        result2: ë‘ ë²ˆì§¸ íŒŒì¼ ë¶„ì„ ê²°ê³¼
    """
    print("\n" + "=" * 80)
    print("ğŸ† ë‘ íŒŒì¼ ë¹„êµ ê²°ê³¼")
    print("=" * 80)

    # ê¸°ë³¸ í†µê³„ ë¹„êµ
    print(f"\n{'í•­ëª©':<30} {result1['file_name']:<25} {result2['file_name']:<25}")
    print("-" * 80)
    print(f"{'ì´ Trace ìˆ˜':<30} {result1['total_traces']:<25} {result2['total_traces']:<25}")
    print(f"{'ì´ í‰ê°€ í•­ëª© ìˆ˜':<30} {result1['total_evaluations']:<25} {result2['total_evaluations']:<25}")

    # ë©”íŠ¸ë¦­ë³„ ë¹„êµ
    all_metrics = set(result1["metrics"].keys()) | set(result2["metrics"].keys())

    if all_metrics:
        print("\n" + "=" * 80)
        print("ğŸ“Š ë©”íŠ¸ë¦­ë³„ ë¹„êµ")
        print("=" * 80)

        for metric_name in sorted(all_metrics):
            print(f"\n[{metric_name}]")
            print(f"{'í†µê³„':<20} {result1['file_name']:<25} {result2['file_name']:<25} {'ì°¨ì´':<15}")
            print("-" * 85)

            stats1 = result1["metrics"].get(metric_name, {})
            stats2 = result2["metrics"].get(metric_name, {})

            if stats1 and stats2:
                # ê°œìˆ˜
                print(f"{'ê°œìˆ˜':<20} {stats1.get('count', 0):<25} {stats2.get('count', 0):<25} {stats2.get('count', 0) - stats1.get('count', 0):<15}")

                # í‰ê· 
                avg1 = stats1.get('avg', 0)
                avg2 = stats2.get('avg', 0)
                diff = avg2 - avg1
                diff_pct = (diff / avg1 * 100) if avg1 != 0 else 0
                print(f"{'í‰ê· ':<20} {avg1:<25.4f} {avg2:<25.4f} {diff:+.4f} ({diff_pct:+.2f}%)")

                # ì¤‘ì•™ê°’
                med1 = stats1.get('median', 0)
                med2 = stats2.get('median', 0)
                diff = med2 - med1
                print(f"{'ì¤‘ì•™ê°’':<20} {med1:<25.4f} {med2:<25.4f} {diff:+.4f}")

                # í‘œì¤€í¸ì°¨
                std1 = stats1.get('std', 0)
                std2 = stats2.get('std', 0)
                diff = std2 - std1
                print(f"{'í‘œì¤€í¸ì°¨':<20} {std1:<25.4f} {std2:<25.4f} {diff:+.4f}")

                # ìµœì†Œ/ìµœëŒ€
                print(f"{'ìµœì†Œê°’':<20} {stats1.get('min', 0):<25.4f} {stats2.get('min', 0):<25.4f}")
                print(f"{'ìµœëŒ€ê°’':<20} {stats1.get('max', 0):<25.4f} {stats2.get('max', 0):<25.4f}")
            else:
                if stats1:
                    print(f"   âš ï¸  {result2['file_name']}ì— '{metric_name}' ë©”íŠ¸ë¦­ ì—†ìŒ")
                else:
                    print(f"   âš ï¸  {result1['file_name']}ì— '{metric_name}' ë©”íŠ¸ë¦­ ì—†ìŒ")

    # ê³µí†µ Trace ë¶„ì„
    traces1 = set(result1["trace_metrics"].keys())
    traces2 = set(result2["trace_metrics"].keys())

    common_traces = traces1 & traces2
    only_in_1 = traces1 - traces2
    only_in_2 = traces2 - traces1

    print("\n" + "=" * 80)
    print("ğŸ” Trace ë¹„êµ")
    print("=" * 80)
    print(f"ê³µí†µ Trace ìˆ˜: {len(common_traces)}")
    print(f"{result1['file_name']}ì—ë§Œ ìˆëŠ” Trace: {len(only_in_1)}")
    print(f"{result2['file_name']}ì—ë§Œ ìˆëŠ” Trace: {len(only_in_2)}")

    # ê³µí†µ Traceì— ëŒ€í•œ ë©”íŠ¸ë¦­ ì°¨ì´ ë¶„ì„
    if common_traces and all_metrics:
        print("\n" + "=" * 80)
        print("ğŸ“ˆ ê³µí†µ Trace ë©”íŠ¸ë¦­ ê°œì„ /ì €í•˜ ë¶„ì„")
        print("=" * 80)

        for metric_name in sorted(all_metrics):
            improvements = 0
            degradations = 0
            unchanged = 0

            for trace_id in common_traces:
                val1 = result1["trace_metrics"][trace_id].get(metric_name)
                val2 = result2["trace_metrics"][trace_id].get(metric_name)

                if val1 is not None and val2 is not None:
                    if val2 > val1:
                        improvements += 1
                    elif val2 < val1:
                        degradations += 1
                    else:
                        unchanged += 1

            if improvements + degradations + unchanged > 0:
                print(f"\n{metric_name}:")
                print(f"   ê°œì„ : {improvements} ({improvements/len(common_traces)*100:.1f}%)")
                print(f"   ì €í•˜: {degradations} ({degradations/len(common_traces)*100:.1f}%)")
                print(f"   ë™ì¼: {unchanged} ({unchanged/len(common_traces)*100:.1f}%)")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ìŠ¤í¬ë¦½íŠ¸ ìƒë‹¨ì˜ ì„¤ì • ì‚¬ìš©
    file1_path = FILE1_PATH
    file2_path = FILE2_PATH
    file3_path = FILE3_PATH
    file4_path = FILE4_PATH
    output_path = OUTPUT_PATH

    # CSV íŒŒì¼ ë¡œë“œ
    df1 = load_csv_data(file1_path)
    df2 = load_csv_data(file2_path)
    df3 = load_csv_data(file3_path)
    df4 = load_csv_data(file4_path)

    # ê° íŒŒì¼ ë¶„ì„
    result1 = analyze_single_file(df1, Path(file1_path).name)
    result2 = analyze_single_file(df2, Path(file2_path).name)
    result3 = analyze_single_file(df3, Path(file3_path).name)
    result4 = analyze_single_file(df4, Path(file4_path).name)

    # 4ê°œ íŒŒì¼ ë¹„êµ
    compare_four_files(result1, result2, result3, result4)

    # ê²°ê³¼ ì €ì¥ (ì„ íƒì )
    if output_path:
        output_path_obj = Path(output_path)
        output_path_obj.parent.mkdir(parents=True, exist_ok=True)

        save_data = {
            "file1": {
                "name": result1["file_name"],
                "total_traces": result1["total_traces"],
                "total_evaluations": result1["total_evaluations"],
                "metrics": result1["metrics"]
            },
            "file2": {
                "name": result2["file_name"],
                "total_traces": result2["total_traces"],
                "total_evaluations": result2["total_evaluations"],
                "metrics": result2["metrics"]
            },
            "file3": {
                "name": result3["file_name"],
                "total_traces": result3["total_traces"],
                "total_evaluations": result3["total_evaluations"],
                "metrics": result3["metrics"]
            },
            "file4": {
                "name": result4["file_name"],
                "total_traces": result4["total_traces"],
                "total_evaluations": result4["total_evaluations"],
                "metrics": result4["metrics"]
            }
        }

        with open(output_path_obj, "w", encoding="utf-8") as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False, default=str)

        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path_obj}")


if __name__ == "__main__":
    main()
