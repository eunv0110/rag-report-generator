#!/usr/bin/env python3
"""4ê°€ì§€ Retrieval ì „ëµ ì„±ëŠ¥ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

í‰ê°€ ì „ëµ:
  1. RRF (Baseline)
  2. RRF + MultiQuery
  3. RRF + MultiQuery + LongContext
  4. RRF + LongContext + TimeWeighted

ì‚¬ìš©ë²•:
    python evaluators/evaluate_4_strategies_new.py

    # íŠ¹ì • ì „ëµë§Œ í‰ê°€
    python evaluators/evaluate_4_strategies_new.py --strategies 1 2

    # Top-K ì„¤ì •
    python evaluators/evaluate_4_strategies_new.py --top-k 10
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

import json
import time
import os
from datetime import datetime
from typing import List, Dict, Any, Optional
from qdrant_client import QdrantClient
from langchain.chat_models import init_chat_model

from config.settings import (
    QDRANT_PATH,
    AZURE_AI_CREDENTIAL,
    AZURE_AI_ENDPOINT,
)
from retrievers.ensemble_retriever import get_ensemble_retriever
from retrievers.multiquery_retriever import get_multiquery_retriever
from retrievers.longcontext_retriever import get_longcontext_retriever
from retrievers.timeweighted_retriever import get_time_weighted_retriever
from utils.langfuse_utils import get_langfuse_client
from models.embeddings.factory import get_embedder

# ìƒìˆ˜ ì •ì˜
DEFAULT_DATASET_PATH = "/home/work/rag/Project/rag-report-generator/data/evaluation/merged_qa_dataset.json"
DEFAULT_NUM_CONTEXTS_FOR_ANSWER = 5
DEFAULT_TEMPERATURE = 0.1
DEFAULT_MAX_TOKENS = 500
DEFAULT_TOP_K = 10

SYSTEM_PROMPT_FILE = "prompts/templates/evaluation/system_prompt.txt"
ANSWER_PROMPT_FILE = "prompts/templates/evaluation/answer_generation_prompt.txt"


def load_prompt(prompt_file: str) -> str:
    """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ"""
    prompt_path = Path(__file__).parent.parent / prompt_file
    if not prompt_path.exists():
        raise FileNotFoundError(f"í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {prompt_path}")

    with open(prompt_path, "r", encoding="utf-8") as f:
        return f.read().strip()


def load_evaluation_dataset(file_path: str) -> List[Dict[str, Any]]:
    """í‰ê°€ìš© ë°ì´í„°ì…‹ ë¡œë“œ"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def generate_llm_answer(question: str, contexts: List[str]) -> str:
    """LLM APIë¥¼ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ ìƒì„±"""
    if not AZURE_AI_CREDENTIAL or not AZURE_AI_ENDPOINT:
        return "Azure OpenAI ì„¤ì •ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."

    os.environ['AZURE_AI_CREDENTIAL'] = AZURE_AI_CREDENTIAL
    os.environ['AZURE_AI_ENDPOINT'] = AZURE_AI_ENDPOINT

    answer_prompt_template = load_prompt(ANSWER_PROMPT_FILE)
    context_text = "\n\n".join(contexts[:DEFAULT_NUM_CONTEXTS_FOR_ANSWER]) if contexts else "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    prompt = answer_prompt_template.replace("{{context}}", context_text).replace("{{question}}", question)

    try:
        model = init_chat_model(
            "azure_ai:gpt-4.1",
            temperature=DEFAULT_TEMPERATURE,
            max_completion_tokens=DEFAULT_MAX_TOKENS
        )
        response = model.invoke(prompt)
        return response.content
    except Exception as e:
        error_msg = f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}"
        print(f"  âš ï¸ LLM API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return error_msg


def generate_version_tag(retriever_name: str, version: str = "v1") -> str:
    """ë²„ì „ íƒœê·¸ ìƒì„±"""
    date_str = datetime.now().strftime("%Y%m%d")
    return f"{retriever_name}_{date_str}_{version}"


def create_trace_and_generation(
    langfuse,
    retriever_name: str,
    question: str,
    contexts: List[str],
    answer: str,
    ground_truth: str,
    context_metadata: List[Dict],
    item_metadata: Dict,
    total_time: float,
    idx: int,
    context_page_id: Optional[str] = None,
    version_tag: str = "v1",
    retriever_tags: List[str] = None
) -> str:
    """Langfuse Traceì™€ Generation ìƒì„±"""
    context_text = "\n\n---\n\n".join(contexts) if contexts else ""

    if retriever_tags is None:
        retriever_tags = []

    all_tags = [
        f"{retriever_name}_{version_tag}",
        version_tag,
        "evaluation"
    ] + retriever_tags

    with langfuse.start_as_current_observation(
        as_type='generation',
        name=f"generation_{retriever_name}_{version_tag}",
        model="gpt-4.1",
        input={
            "question": question,
            "context": context_text
        },
        output={
            "answer": answer
        },
        metadata={
            "ground_truth": ground_truth,
            "contexts": contexts,
            "context_metadata": context_metadata,
            "retriever_type": retriever_name,
            "version": version_tag,
            "retriever_tags": retriever_tags
        }
    ) as generation:
        trace_id = generation.trace_id

        langfuse.update_current_trace(
            name=f"eval_{retriever_name}_{version_tag}_q{idx}",
            tags=all_tags,
            input={
                "question": question,
                "context": context_text
            },
            output={
                "answer": answer
            },
            metadata={
                "retriever": retriever_name,
                "version": version_tag,
                "total_time_ms": total_time * 1000,
                "num_retrieved_contexts": len(contexts),
                "context_page_id": context_page_id,
                "question_id": idx,
                "category": item_metadata.get("category", "unknown"),
                "difficulty": item_metadata.get("difficulty", "unknown"),
                "retriever_components": retriever_tags
            }
        )

    print(f"\n[DEBUG] Trace {idx}:")
    print(f"  - ID: {trace_id}")
    print(f"  - Question: {question[:50]}...")
    print(f"  - Context length: {len(context_text)} chars")
    print(f"  - Answer length: {len(answer)} chars")

    return trace_id


def add_retrieval_quality_score(langfuse, trace_id: str, context_metadata: List[Dict]):
    """ê²€ìƒ‰ í’ˆì§ˆ ìŠ¤ì½”ì–´ ì¶”ê°€"""
    if not context_metadata:
        return

    # metadataì— scoreê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ê³„ì‚°
    scores = [m.get("score") for m in context_metadata if m.get("score") is not None]
    if scores:
        avg_score = sum(scores) / len(scores)
        langfuse.create_score(
            trace_id=trace_id,
            name="retrieval_quality",
            value=avg_score,
            comment=f"Average retrieval score from {len(scores)} contexts"
        )


def evaluate_single_query(
    retriever,
    retriever_name: str,
    item: Dict[str, Any],
    langfuse,
    idx: int,
    top_k: int,
    base_version: str = "v1",
    retriever_tags: List[str] = None
) -> Dict[str, Any]:
    """ë‹¨ì¼ ì¿¼ë¦¬ í‰ê°€"""
    question = item["question"]
    ground_truth = item["ground_truth"]
    context_page_id = item.get("context_page_id")
    item_metadata = item.get("metadata", {})

    version_tag = generate_version_tag(retriever_name, base_version)
    start_time = time.time()

    # ê²€ìƒ‰ ìˆ˜í–‰ (invoke ë©”ì„œë“œ ì‚¬ìš©)
    search_results = retriever.invoke(question)

    # LangChain Documentë¥¼ contextsë¡œ ë³€í™˜
    contexts = []
    context_metadata = []

    for result in search_results[:top_k]:
        contexts.append(result.page_content)
        context_metadata.append({
            "page_title": result.metadata.get('page_title', 'Unknown'),
            "section_title": result.metadata.get('section_title', 'N/A'),
            "chunk_id": result.metadata.get('chunk_id', 'unknown'),
            "score": result.metadata.get('_combined_score') or result.metadata.get('_similarity_score')
        })

    if not contexts:
        print(f"  âš ï¸ [{idx}] No contexts found for question!")
        contexts = ["ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."]

    # LLM ë‹µë³€ ìƒì„±
    answer = generate_llm_answer(question, contexts)

    if not answer or answer.startswith("ë‹µë³€ ìƒì„± ì‹¤íŒ¨") or answer.startswith("Azure OpenAI ì„¤ì •"):
        print(f"  âš ï¸ [{idx}] LLM answer generation failed!")
        if not answer:
            answer = "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    total_time = time.time() - start_time

    # Langfuse Trace & Generation
    trace_id = create_trace_and_generation(
        langfuse=langfuse,
        retriever_name=retriever_name,
        question=question,
        contexts=contexts,
        answer=answer,
        ground_truth=ground_truth,
        context_metadata=context_metadata,
        item_metadata=item_metadata,
        total_time=total_time,
        idx=idx,
        context_page_id=context_page_id,
        version_tag=version_tag,
        retriever_tags=retriever_tags
    )

    # ê²€ìƒ‰ í’ˆì§ˆ ìŠ¤ì½”ì–´ ì¶”ê°€
    add_retrieval_quality_score(langfuse, trace_id, context_metadata)

    print(f"  [{idx}] {question[:50]}... ({len(contexts)}ê°œ ë¬¸ì„œ, {total_time*1000:.0f}ms)")

    return {
        "question": question,
        "answer": answer,
        "ground_truth": ground_truth,
        "num_contexts": len(contexts),
        "time": total_time,
        "trace_id": trace_id
    }


def evaluate_retriever(
    retriever,
    retriever_name: str,
    eval_data: List[Dict[str, Any]],
    langfuse,
    top_k: int = DEFAULT_TOP_K,
    base_version: str = "v1",
    retriever_tags: List[str] = None
) -> Dict[str, Any]:
    """ë¦¬íŠ¸ë¦¬ë²„ í‰ê°€"""
    print(f"\n{'=' * 60}")
    print(f"ğŸ” {retriever_name} í‰ê°€ ì¤‘...")
    print(f"{'=' * 60}")

    stats = {
        "total_queries": len(eval_data),
        "total_time": 0,
        "evaluations": []
    }

    for idx, item in enumerate(eval_data, 1):
        eval_result = evaluate_single_query(
            retriever=retriever,
            retriever_name=retriever_name,
            item=item,
            langfuse=langfuse,
            idx=idx,
            top_k=top_k,
            base_version=base_version,
            retriever_tags=retriever_tags
        )

        stats["evaluations"].append(eval_result)
        stats["total_time"] += eval_result["time"]

        # ê° ì§ˆë¬¸ í‰ê°€ í›„ ìºì‹œ ì €ì¥ (ì¤‘ë‹¨ë˜ì–´ë„ ìºì‹œ ë³´ì¡´)
        try:
            embedder = get_embedder()
            if hasattr(embedder, 'use_cache') and embedder.use_cache and embedder.cache:
                embedder.cache.save()
        except:
            pass  # ìºì‹œ ì €ì¥ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ (í‰ê°€ ì§„í–‰ì— ì˜í–¥ ì—†ìŒ)

    stats["avg_time"] = stats["total_time"] / stats["total_queries"]
    stats["avg_contexts"] = sum(e["num_contexts"] for e in stats["evaluations"]) / stats["total_queries"]

    return stats


def create_strategy_retriever(strategy_num: int, top_k: int = 10):
    """
    ì „ëµë³„ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±

    Returns:
        (retriever, retriever_name, retriever_tags)
    """
    if strategy_num == 1:
        # Strategy 1: RRF (Baseline)
        retriever = get_ensemble_retriever(k=top_k)
        retriever_name = "ensemble_rrf"
        retriever_tags = ["ensemble", "rrf", "bm25", "dense"]

    elif strategy_num == 2:
        # Strategy 2: RRF + MultiQuery
        base_retriever = get_ensemble_retriever(k=top_k)
        retriever = get_multiquery_retriever(
            base_retriever=base_retriever,
            num_queries=3,
            k=top_k
        )
        retriever_name = "rrf_multiquery"
        retriever_tags = ["multiquery", "ensemble", "rrf", "bm25", "dense"]

    elif strategy_num == 3:
        # Strategy 3: RRF + MultiQuery + LongContext
        base_retriever = get_ensemble_retriever(k=top_k)
        multiquery_retriever = get_multiquery_retriever(
            base_retriever=base_retriever,
            num_queries=3,
            k=top_k
        )
        retriever = get_longcontext_retriever(
            base_retriever=multiquery_retriever,
            k=top_k
        )
        retriever_name = "rrf_multiquery_longcontext"
        retriever_tags = ["longcontext", "multiquery", "ensemble", "rrf", "bm25", "dense"]

    elif strategy_num == 4:
        # Strategy 4: RRF + LongContext + TimeWeighted
        # TimeWeightedë¥¼ ë¨¼ì € ì ìš©
        time_weighted = get_time_weighted_retriever(
            decay_rate=0.01,
            k=top_k
        )
        retriever = get_longcontext_retriever(
            base_retriever=time_weighted,
            k=top_k
        )
        retriever_name = "rrf_longcontext_timeweighted"
        retriever_tags = ["longcontext", "time_weighted", "decay_0.01"]

    else:
        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ì „ëµ ë²ˆí˜¸: {strategy_num}")

    return retriever, retriever_name, retriever_tags


def run_evaluation(
    strategy_num: int,
    dataset: str,
    top_k: int,
    version: str,
    langfuse
) -> bool:
    """ë‹¨ì¼ í‰ê°€ ì „ëµ ì‹¤í–‰

    Args:
        strategy_num: ì „ëµ ë²ˆí˜¸
        dataset: í‰ê°€ ë°ì´í„°ì…‹ ê²½ë¡œ
        top_k: Top-K ê°’
        version: ë²„ì „ íƒœê·¸
        langfuse: Langfuse í´ë¼ì´ì–¸íŠ¸

    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    STRATEGIES = {
        1: "RRF (Baseline)",
        2: "RRF + MultiQuery",
        3: "RRF + MultiQuery + LongContext",
        4: "RRF + LongContext + TimeWeighted"
    }

    strategy_name = STRATEGIES[strategy_num]

    print("\n" + "=" * 70)
    print(f"{strategy_num}ï¸âƒ£  {strategy_name} í‰ê°€ ì¤‘...")
    print("=" * 70)

    try:
        # í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ
        eval_data = load_evaluation_dataset(dataset)

        # ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
        retriever, retriever_name, retriever_tags = create_strategy_retriever(strategy_num, top_k)

        print(f"âœ… ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™” ì™„ë£Œ: {retriever_name}")
        print(f"   - íƒœê·¸: {', '.join(retriever_tags)}")

        # í‰ê°€ ìˆ˜í–‰
        stats = evaluate_retriever(
            retriever=retriever,
            retriever_name=retriever_name,
            eval_data=eval_data,
            langfuse=langfuse,
            top_k=top_k,
            base_version=version,
            retriever_tags=retriever_tags
        )

        # ê²°ê³¼ ì €ì¥
        output_file = Path(dataset).parent / f"{retriever_name}_evaluation_stats.json"
        save_result = {k: v for k, v in stats.items() if k != "evaluations"}
        save_result["num_evaluations"] = len(stats.get("evaluations", []))
        save_result["config"] = {
            "strategy_num": strategy_num,
            "strategy_name": strategy_name,
            "retriever_name": retriever_name,
            "retriever_tags": retriever_tags,
            "top_k": top_k,
            "version": version,
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(save_result, f, indent=2, ensure_ascii=False, default=str)

        print(f"\nâœ… {strategy_name} í‰ê°€ ì™„ë£Œ")
        print(f"   - í‰ê·  ì‹œê°„: {stats['avg_time']*1000:.2f}ms")
        print(f"   - í‰ê·  ì»¨í…ìŠ¤íŠ¸ ìˆ˜: {stats['avg_contexts']:.2f}")
        print(f"   - ê²°ê³¼ ì €ì¥: {output_file}")

        return True

    except Exception as e:
        print(f"âŒ {strategy_name} í‰ê°€ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(
        description="4ê°€ì§€ Retrieval ì „ëµ ì„±ëŠ¥ í‰ê°€",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )

    parser.add_argument(
        "--strategies",
        type=int,
        nargs="+",
        choices=[1, 2, 3, 4],
        default=[1, 2, 3, 4],
        help="í‰ê°€í•  ì „ëµ ë²ˆí˜¸ (ê¸°ë³¸ê°’: ëª¨ë‘)"
    )

    parser.add_argument(
        "--dataset",
        type=str,
        default=DEFAULT_DATASET_PATH,
        help="í‰ê°€ ë°ì´í„°ì…‹ ê²½ë¡œ"
    )

    parser.add_argument(
        "--top-k",
        type=int,
        default=DEFAULT_TOP_K,
        help="Top-K ê°’ (ê¸°ë³¸ê°’: 10)"
    )

    parser.add_argument(
        "--version",
        type=str,
        default="v1",
        help="ë²„ì „ íƒœê·¸ (ê¸°ë³¸ê°’: v1)"
    )

    args = parser.parse_args()

    # ì‹œì‘ ë©”ì‹œì§€
    print("=" * 70)
    print("ğŸš€ 4ê°€ì§€ Retrieval ì „ëµ ì„±ëŠ¥ í‰ê°€ ì‹œì‘")
    print("=" * 70)
    print(f"\nğŸ“Š í‰ê°€ ì„¤ì •:")
    print(f"   - Dataset: {args.dataset}")
    print(f"   - Top-K: {args.top_k}")
    print(f"   - Version: {args.version}")
    print(f"   - í‰ê°€í•  ì „ëµ: {args.strategies}")

    # Langfuse í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    langfuse = get_langfuse_client()
    if not langfuse:
        print("âŒ Langfuse í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ê° ì „ëµ í‰ê°€
    results = {}
    for strategy_num in sorted(args.strategies):
        success = run_evaluation(
            strategy_num=strategy_num,
            dataset=args.dataset,
            top_k=args.top_k,
            version=args.version,
            langfuse=langfuse
        )
        results[strategy_num] = success

    # Langfuse flush
    print("\nâ³ Langfuseì— ë°ì´í„° ì „ì†¡ ì¤‘...")
    langfuse.flush()

    # ì„ë² ë”© ìºì‹œ ì €ì¥
    print("\nğŸ’¾ ì„ë² ë”© ìºì‹œ ì €ì¥ ì¤‘...")
    try:
        embedder = get_embedder()
        if hasattr(embedder, 'save_cache'):
            embedder.save_cache()
        else:
            print("  â„¹ï¸  ìºì‹œ ì €ì¥ ë©”ì„œë“œ ì—†ìŒ (ìºì‹œ ë¹„í™œì„±í™” ìƒíƒœ)")
    except Exception as e:
        print(f"  âš ï¸  ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    # ê²°ê³¼ ìš”ì•½
    STRATEGY_NAMES = {
        1: "RRF (Baseline)",
        2: "RRF + MultiQuery",
        3: "RRF + MultiQuery + LongContext",
        4: "RRF + LongContext + TimeWeighted"
    }

    print("\n" + "=" * 70)
    print("ğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print("=" * 70)

    for strategy_num in sorted(args.strategies):
        strategy_name = STRATEGY_NAMES[strategy_num]
        status = "âœ… ì„±ê³µ" if results[strategy_num] else "âŒ ì‹¤íŒ¨"
        print(f"{strategy_num}. {strategy_name:<40} {status}")

    # ì™„ë£Œ ë©”ì‹œì§€
    success_count = sum(results.values())
    total_count = len(results)

    print("\n" + "=" * 70)
    if success_count == total_count:
        print("âœ… ëª¨ë“  í‰ê°€ ì™„ë£Œ!")
    else:
        print(f"âš ï¸  ì¼ë¶€ í‰ê°€ ì‹¤íŒ¨ ({success_count}/{total_count} ì„±ê³µ)")
    print("=" * 70)

    print("\nğŸ“Š ë‹¤ìŒ ë‹¨ê³„:")
    print("   1. Langfuse ëŒ€ì‹œë³´ë“œì—ì„œ ê²°ê³¼ í™•ì¸: https://cloud.langfuse.com")
    print("   2. ê° ì „ëµë³„ stats íŒŒì¼ í™•ì¸:")
    print("      - data/evaluation/ensemble_rrf_evaluation_stats.json")
    print("      - data/evaluation/rrf_multiquery_evaluation_stats.json")
    print("      - data/evaluation/rrf_multiquery_longcontext_evaluation_stats.json")
    print("      - data/evaluation/rrf_longcontext_timeweighted_evaluation_stats.json")
    print()

    # ì‹¤íŒ¨í•œ ê²½ìš° ì¢…ë£Œ ì½”ë“œ 1 ë°˜í™˜
    if success_count < total_count:
        sys.exit(1)


if __name__ == "__main__":
    main()
